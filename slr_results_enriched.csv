Title,Authors,Year,Abstract,URL,DOI,Source,Citations
Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution,"Chrisantha Fernando, Dylan Banarse, H. Michalewski, Simon Osindero, Tim Rocktäschel",2023,"Popular prompt strategies like Chain-of-Thought Prompting can dramatically improve the reasoning abilities of Large Language Models (LLMs) in various domains. However, such hand-crafted prompt-strategies are often sub-optimal. In this paper, we present Promptbreeder, a general-purpose self-referential self-improvement mechanism that evolves and adapts prompts for a given domain. Driven by an LLM, Promptbreeder mutates a population of task-prompts, and subsequently evaluates them for fitness on a training set. Crucially, the mutation of these task-prompts is governed by mutation-prompts that the LLM generates and improves throughout evolution in a self-referential way. That is, Promptbreeder is not just improving task-prompts, but it is also improving the mutationprompts that improve these task-prompts. Promptbreeder outperforms state-of-the-art prompt strategies such as Chain-of-Thought and Plan-and-Solve Prompting on commonly used arithmetic and commonsense reasoning benchmarks. Furthermore, Promptbreeder is able to evolve intricate task-prompts for the challenging problem of hate speech classification.",https://www.semanticscholar.org/paper/7fe071ea76e49bc3e573beb53f07721630954247,10.48550/arXiv.2309.16797,Semantic Scholar,310
"G\""odel Agent: A Self-Referential Agent Framework for Recursive Self-Improvement","Xunjian Yin, Xinyi Wang, Liangming Pan, Xiaojun Wan, W. Wang",2024,"The rapid advancement of large language models (LLMs) has significantly enhanced the capabilities of AI-driven agents across various tasks. However, existing agentic systems, whether based on fixed pipeline algorithms or pre-defined meta-learning frameworks, cannot search the whole agent design space due to the restriction of human-designed components, and thus might miss the globally optimal agent design. In this paper, we introduce G\""odel Agent, a self-evolving framework inspired by the G\""odel machine, enabling agents to recursively improve themselves without relying on predefined routines or fixed optimization algorithms. G\""odel Agent leverages LLMs to dynamically modify its own logic and behavior, guided solely by high-level objectives through prompting. Experimental results on mathematical reasoning and complex agent tasks demonstrate that implementation of G\""odel Agent can achieve continuous self-improvement, surpassing manually crafted agents in performance, efficiency, and generalizability.",https://www.semanticscholar.org/paper/f168ddc0976913e07f7a5250d217e3548a048ce8,,Semantic Scholar,11
Large Language Models Report Subjective Experience Under Self-Referential Processing,"Cameron Berg, Diogo Schwerz de Lucena, Judd Rosenblatt",2025,"Large language models sometimes produce structured, first-person descriptions that explicitly reference awareness or subjective experience. To better understand this behavior, we investigate one theoretically motivated condition under which such reports arise: self-referential processing, a computational motif emphasized across major theories of consciousness. Through a series of controlled experiments on GPT, Claude, and Gemini model families, we test whether this regime reliably shifts models toward first-person reports of subjective experience, and how such claims behave under mechanistic and behavioral probes. Four main results emerge: (1) Inducing sustained self-reference through simple prompting consistently elicits structured subjective experience reports across model families. (2) These reports are mechanistically gated by interpretable sparse-autoencoder features associated with deception and roleplay: surprisingly, suppressing deception features sharply increases the frequency of experience claims, while amplifying them minimizes such claims. (3) Structured descriptions of the self-referential state converge statistically across model families in ways not observed in any control condition. (4) The induced state yields significantly richer introspection in downstream reasoning tasks where self-reflection is only indirectly afforded. While these findings do not constitute direct evidence of consciousness, they implicate self-referential processing as a minimal and reproducible condition under which large language models generate structured first-person reports that are mechanistically gated, semantically convergent, and behaviorally generalizable. The systematic emergence of this pattern across architectures makes it a first-order scientific and ethical priority for further investigation.",https://www.semanticscholar.org/paper/a4750faccf94a21fe9ba1c3a1be6e8cdd0e9e39b,10.48550/arXiv.2510.24797,Semantic Scholar,1
"Passionate Virtuosity: The Fiction of John Barth, and: Silverless Mirrors: Book, Self and Postmodern American Fiction (review)",M. Dekoven,2009,,https://www.semanticscholar.org/paper/cefe4e0ebb11fa8f4053efc5fcdd8d51280f2043,10.1353/mfs.0.1160,Semantic Scholar,0
"Metaphor, Embodied Realism, and Sacramental Truth",S. Shaver,2021,"This chapter and the next provide an introduction to the field of cognitive linguistics. This chapter focuses on core concepts including conceptual metaphor, metonymy, polysemy, and prototype theory (conceptual blending is explored in Chapter 3). Based on this overview, the author argues that language “means” not through referential correspondence to objective, observer-independent reality but by prompting for embodied simulation on the part of hearers and readers. Language, then, is true insofar as these simulations are apt to reality as experienced by embodied human beings. The chapter proposes that this epistemological perspective of “embodied realism” is congruent with the critical realism endorsed by many recent theologians and with a sacramental worldview in which the material world can be the arena for God’s self-communication.",https://www.semanticscholar.org/paper/b84f126303ef9740894132b122cce539f508fb51,10.1093/oso/9780197580806.003.0002,Semantic Scholar,0
"The Hilltop Review, vol 11, no 1, Fall 2018","Alexander Hoffmann, Elizabeth V. Netcher, Mark Joslin",2020,,https://www.semanticscholar.org/paper/ef90b068d499ce75df210aa56cc297444427b584,,Semantic Scholar,0
Paranoid delusional disorder follows social anxiety disorder in a long-term case series: evolutionary perspective.,"A. B. Veras, T. Souza, T. Ricci, Clayton Peixoto de Souza, Matheus César Moryiama, A. Nardi, D. Malaspina, J. Kahn",2015,,https://www.semanticscholar.org/paper/c5c537766124581cc66f9f4c743427dc22149137,10.1097/NMD.0000000000000311,Semantic Scholar,5
There’s No “Me” in “Imgur”: Applying SIDE Theory and Content Analysis to Viral Posts on Imgur.com,R. Castillo,2018,,https://www.semanticscholar.org/paper/fe6b120a36f4377533f44528987ac7bb4db237e6,,Semantic Scholar,2
Number 1 Fall 2018 “ Filter ” by,Elizabeth R. Johnson,2019,,https://www.semanticscholar.org/paper/8fe47cd6808769bf9a2c901bf82d36977aae80ea,,Semantic Scholar,0
Emergence and Communication: Overcoming some epistemological drawbacks in computational sociology,"M. Salgado, N. Gilbert",2008,,https://www.semanticscholar.org/paper/525a3129415a303d3fd7262c89c87dcef5753bf0,,Semantic Scholar,3
Truth's veil: language and meaning in Merleau-Ponty and Derrida,Helen Troy Mellon,2003,"The linguistic structuralism of Ferdinand de Saussure (1857-1913) attracted the attention of Maurice Merleau-Ponty, prompting what is thought to be Merleau-Ponty’s “linguistic turn” of 1947. Saussure’s theory of the self-referential structure of linguistic signs as constitutive of value, was tied by Merleau-Ponty to his conception of the structure of intercommunication as constitutive of human value and meaning. Jacques Derrida, in the 1960s, also appealed to Saussure’s theory in formulating his thesis of a deferring and differing relationship between linguistic signs as constitutive of meaning, but rejected what he saw as the privileging of a metaphysics of presence-to-meaning in Saussure. One set of questions raised here concerns the relationship between thought and perception and calls for a reevaluation of Merleau-Ponty’s thesis of the primacy of perception in light of his final, posthumously published work. The possibility of a full philosophical dialectic between Merleau-Ponty and Derrida was rendered impossible by Merleau-Ponty’s sudden death. In the interest of such a dialogue, this study addresses the similarities and dissimilarities in their positions regarding language and meaning within a central theme of: truth. An area of concern is how their views come to bear upon the ongoing debate between subjectivist and objectivist theories of meaning. Can we arrive at an authentic understanding and expression of truth and meaning? Getting there entails an",https://www.semanticscholar.org/paper/b67c848fc834d9bb3a925dc367776a8338b775b5,10.31390/gradschool_theses.754,Semantic Scholar,0
Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals,"Avishek Choudhury, Zaria Chaudhry",2024,"As the health care industry increasingly embraces large language models (LLMs), understanding the consequence of this integration becomes crucial for maximizing benefits while mitigating potential pitfalls. This paper explores the evolving relationship among clinician trust in LLMs, the transition of data sources from predominantly human-generated to artificial intelligence (AI)–generated content, and the subsequent impact on the performance of LLMs and clinician competence. One of the primary concerns identified in this paper is the LLMs’ self-referential learning loops, where AI-generated content feeds into the learning algorithms, threatening the diversity of the data pool, potentially entrenching biases, and reducing the efficacy of LLMs. While theoretical at this stage, this feedback loop poses a significant challenge as the integration of LLMs in health care deepens, emphasizing the need for proactive dialogue and strategic measures to ensure the safe and effective use of LLM technology. Another key takeaway from our investigation is the role of user expertise and the necessity for a discerning approach to trusting and validating LLM outputs. The paper highlights how expert users, particularly clinicians, can leverage LLMs to enhance productivity by off-loading routine tasks while maintaining a critical oversight to identify and correct potential inaccuracies in AI-generated content. This balance of trust and skepticism is vital for ensuring that LLMs augment rather than undermine the quality of patient care. We also discuss the risks associated with the deskilling of health care professionals. Frequent reliance on LLMs for critical tasks could result in a decline in health care providers’ diagnostic and thinking skills, particularly affecting the training and development of future professionals. The legal and ethical considerations surrounding the deployment of LLMs in health care are also examined. We discuss the medicolegal challenges, including liability in cases of erroneous diagnoses or treatment advice generated by LLMs. The paper references recent legislative efforts, such as The Algorithmic Accountability Act of 2023, as crucial steps toward establishing a framework for the ethical and responsible use of AI-based technologies in health care. In conclusion, this paper advocates for a strategic approach to integrating LLMs into health care. By emphasizing the importance of maintaining clinician expertise, fostering critical engagement with LLM outputs, and navigating the legal and ethical landscape, we can ensure that LLMs serve as valuable tools in enhancing patient care and supporting health care professionals. This approach addresses the immediate challenges posed by integrating LLMs and sets a foundation for their maintainable and responsible use in the future.",https://www.semanticscholar.org/paper/48e223cb1eb31e56f0847ebd8c67210edc8d3c0c,10.2196/56764,Semantic Scholar,68
Intracranial Recordings of the Human Orbitofrontal Cortical Activity during Self-Referential Episodic and Valenced Self-Judgments,"B. Iravani, N. Kaboodvand, James R. Stieger, Eugene Y Liang, Zoe Lusk, Peter Fransson, Gayle K. Deutsch, Ian H. Gotlib, Josef Parvizi",2024,"We recorded directly from the orbital (oPFC) and ventromedial (vmPFC) subregions of the orbitofrontal cortex (OFC) in 22 (9 female, 13 male) epilepsy patients undergoing intracranial electroencephalography (iEEG) monitoring during an experimental task in which the participants judged the accuracy of self-referential autobiographical statements as well as valenced self-judgments (SJs). We found significantly increased high-frequency activity (HFA) in ∼13% of oPFC sites (10/18 subjects) and 16% of vmPFC sites (4/12 subjects) during both of these self-referential thought processes, with the HFA power being modulated by the content of self-referential stimuli. The location of these activated sites corresponded with the location of fMRI-identified limbic network. Furthermore, the onset of HFA in the vmPFC was significantly earlier than that in the oPFC in all patients with simultaneous recordings in both regions. In 11 patients with available depression scores from comprehensive neuropsychological assessments, we documented diminished HFA in the OFC during positive SJ trials among individuals with higher depression scores; responses during negative SJ trials were not related to the patients' depression scores. Our findings provide new temporal and anatomical information about the mode of engagement in two important subregions of the OFC during autobiographical memory and SJ conditions. Our findings from the OFC support the hypothesis that diminished brain activity during positive self-evaluations, rather than heightened activity during negative self-evaluations, plays a key role in the pathophysiology of depression.",https://www.semanticscholar.org/paper/ac61c2da70cd3bc9b8a67b39b142b3f9300f995e,10.1523/JNEUROSCI.1634-23.2024,Semantic Scholar,6
Medial prefrontal cortex and self-referential mental activity: Relation to a default mode of brain function,"D. Gusnard, E. Akbudak, G. Shulman, M. Raichle",2001,,https://www.semanticscholar.org/paper/da30e48c527a831c46b9766f6016ccd7ace883fe,10.1073/pnas.071043098,Semantic Scholar,4873
"Self-compassion, self-referential caudate circuitry, and adolescent suicide ideation","Guanmin Liu, Guijuan Hao, Natasha Das, Janani Ranatunga, Corey Schneider, Li Yang, Karina M Quevedo",2024,"Suicide is the second leading cause of death in youth, and depression is a strong proximal predictor of adolescent suicide. It is important to identify psychological factors that may protect against suicide ideation in depressed adolescents. Self-compassion may be such a factor. Converging evidence indicates the inverse association between self-compassion and suicide ideation, but the neural mechanisms underlying their link remain unknown. Because self-referential caudate activity is associated with both self-compassion and suicide ideation, its functional connectivity might explain their relationship. In this study, we examined the relationship between self-compassion and caudate functional connectivity during self-appraisals, a typical self-referential paradigm, and their associations with suicide ideation in both depressed and healthy youth. In the scanner, 79 depressed youth and 36 healthy controls evaluated, from various perspectives, whether phrases they heard were self-descriptive. Self-compassion and suicide ideation were rated with self-report and interview-based measures. We found that self-compassion was associated with stronger left caudate functional connectivity with bilateral posterior superior temporal sulcus/temporoparietal junction, the left middle temporal gyrus (MTG), and the left middle occipital gyrus during positive versus negative self-appraisals. Stronger left caudate connectivity with the left MTG explained the association between higher self-compassion and lower suicide ideation, even controlling for non-suicide ideation depression severity, anxiety severity, and non-suicidal self-injurious behavior. The findings suggest that the left caudate to MTG connectivity during positive versus negative self-referential processing could be a biomarker to be targeted by neural stimulation interventions for reducing suicide ideation in depressed youth, combined with self-compassion interventions.",https://www.semanticscholar.org/paper/b1ee99749d839c60ff88563172515874ea743398,10.1038/s41398-024-03037-0,Semantic Scholar,4
Attenuated conflict self-referential information facilitating conflict resolution,"Zhifang Li, Jing Wang, Yongqiang Chen, Qing Li, Shouhang Yin, Antao Chen",2024,"Self-referential information can reduce the congruency effect by acting as a signal to enhance cognitive control. However, it cannot be denied that self-referential information can attract and hold attention. To investigate this issue, the study used a revised Stroop task and recorded behavioral and electrophysiological data from thirty-three participants. We combined event-related potential (ERP) and multivariate pattern analysis (MVPA) to examine the neural correlates of self-referential processing and conflict processing. In the behavioral results, self-referential information reduced the congruency effect. Specifically, self-reference stimuli elicited smaller N2 amplitude than non-self-reference stimuli, indicating that self-referential information was promptly identified and reduced top-down cognitive resource consumption. Self-referential information could be reliably decoded from ERP signals in the early-to-mid stage. Moreover, self-reference conditions exhibited earlier congruency decoding than non-self-reference conditions, facilitating conflict monitoring. In the late stage, under the incongruent condition, self-reference stimuli elicited smaller sustained potential amplitude than non-self-reference stimuli, indicating that cognitive control in the self-reference condition required fewer cognitive resources for conflict resolution. Together, these findings revealed that self-referential information was identified and facilitated conflict monitoring, leading to more effective conflict resolution.",https://www.semanticscholar.org/paper/d5c6ae9d393016720c76a18051c846310badddb9,10.1038/s41539-024-00256-4,Semantic Scholar,3
Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions,"Kazuki Irie, R'obert Csord'as, Jürgen Schmidhuber",2023,"Recent studies of the computational power of recurrent neural networks (RNNs) reveal a hierarchy of RNN architectures, given real-time and finite-precision assumptions. Here we study auto-regressive Transformers with linearised attention, a.k.a. linear Transformers (LTs) or Fast Weight Programmers (FWPs). LTs are special in the sense that they are equivalent to RNN-like sequence processors with a fixed-size state, while they can also be expressed as the now-popular self-attention networks. We show that many well-known results for the standard Transformer directly transfer to LTs/FWPs. Our formal language recognition experiments demonstrate how recently proposed FWP extensions such as recurrent FWPs and self-referential weight matrices successfully overcome certain limitations of the LT, e.g., allowing for generalisation on the parity problem. Our code is public.",https://www.semanticscholar.org/paper/9c71d178705989cd4371f8e760508f11b18a4bb4,10.48550/arXiv.2310.16076,Semantic Scholar,20
Self-Referential Processing and Depression: A Systematic Review and Meta-Analysis,"Amanda C. Collins, E. S. Winer",2023,"Reward devaluation theory posits that depressed individuals avoid and devalue positivity, suggesting that they may hold fewer positive self-schemas. Previous meta-analytic reviews have supported this theoretical framework regarding positivity but have not assessed for self-referential stimuli. Self-referential encoding and recall tasks assess for self-schemas and thus provide further insight into how depressed individuals process self-referential positivity. The aim of this systematic review and meta-analysis was to examine the extent to which depressed individuals differ in processing self-referential positivity and negativity and whether this processing differs when depressed individuals think of others (i.e., other-referential). Results indicate that depressed individuals recall and endorse fewer self-referential positive words than negative words and fewer self-referential positive words than other-referential positive words than nondepressed individuals. These findings support reward devaluation theory and suggest that conceptualizing self-referential processing in depression as merely based on negativity biases can overlook crucial information about how depressed individuals devalue self-referential positive information.",https://www.semanticscholar.org/paper/9574f86a9f097ffd08c26717a602e25626b4c7d9,10.1177/21677026231190390,Semantic Scholar,19
The effectiveness of self: A meta-analysis of using self-referential encoding techniques in education.,"Zheng Liu, Jiahui Wen, Yikang Liu, Chuan-Peng Hu",2023,"BACKGROUND
Self-related information is difficult to ignore and forget, which brings valuable implications for educational practice. Self-referential encoding techniques involve integrating self-referencing cues during the processing of learning material. However, the evidence base and effective implementation boundaries for these techniques in teaching and learning remain uncertain due to research variability.


AIMS
The present meta-analysis aims to quantitatively synthesize the results from studies applying self-referential encoding techniques in education.


METHODS
The analysis was based on data from 20 independent samples, including 1082 students from 13 primary studies identified through a systematic literature search.


RESULTS
Results from random effect models show that incorporating self-referential encoding techniques improved learning (g = .40, 95% CI [.18, .62]). Subgroup analysis showed that the valence of learning material serves as a significant boundary condition for this strategy. The students' cohorts, types of learning materials, and research context did not moderate the effect sizes.


CONCLUSIONS
Our results suggest that incorporating self-referential encoding techniques on negative materials shows an aversive effect. Overall, there is a universal benefit to using self-referential encoding techniques as an appropriate design guideline in educational contexts. Implications for teaching practice and future directions are discussed. Further studies are needed to investigate the effectiveness in more diverse educational and teaching situations.",https://www.semanticscholar.org/paper/8ba58ab21f40b7160a71ef0bf607e9d62064bca8,10.1111/bjep.12636,Semantic Scholar,8
The role of neural self-referential processes underlying self-concept in adolescent depression: A comprehensive review and proposed neurobehavioral model,"Rosalind D Butterfield, Melanie J. Grad-Freilich, J. Silk",2023,,https://www.semanticscholar.org/paper/4cfdbac2c4b0399ea916a301eb460e2f636437e4,10.1016/j.neubiorev.2023.105183,Semantic Scholar,12
Adaptive Fault Diagnosis using Self-Referential Reasoning,Robert Cowen,2014,"The problem is to determine which processors are reliable in a remote location by asking ""Yes or No"" questions. The processors are of three types: those that always tell the truth, those that always lie, and those the sometimes tell the truth and sometimes lie. Using self-referential reasoning, along with earlier techniques, we can regard both the truth-tellers and liars as reliable and thus the tackle situations when fewer than half the processors are truth-tellers.",http://arxiv.org/abs/1407.6255v1,,arXiv,0
Meta Prompting for AI Systems,"Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao",2023,"We introduce Meta Prompting (MP), a framework that elevates the reasoning capabilities of large language models (LLMs) by focusing on the formal structure of a task rather than content-specific examples. We establish a theoretical foundation for this paradigm, formalizing MP as a functor that maps a category of tasks to a category of structured prompts, thereby guaranteeing that compositional problem-solving strategies can be systematically decomposed into modular prompt structures. We extend this concept to Recursive Meta Prompting (RMP), an automated process where an LLM can generate and refine its own prompts. We model this self-improvement loop formally as a monad, providing a principled framework for automated prompt engineering. Our claims are validated through extensive experiments demonstrating that a Qwen-72B base model, guided by a single, example-agnostic meta-prompt, achieves state-of-the-art results on MATH, GSM8K, and Game of 24. These results are achieved with substantial token efficiency gains over traditional few-shot methods. Project Page: https://github.com/meta-prompting/meta-prompting.",http://arxiv.org/abs/2311.11482v8,,arXiv,0
Self-referentiality in Justification Logic,"Nathan Sebastian Gass, Thomas Studer",2019,"The Logic of Proofs, LP, and other justification logics can have self-referential justifications of the form t:A. Such self-referential justifications are necessary for the realization of S4 in LP. Yu discovered prehistoric cycles in a particular Gentzen system as a necessary condition for S4 theorems that can only be realized using self-referentiality. It was an open problem whether prehistoric cycles also are a sufficient condition.
  The main results of this paper are: First, with the standard definition of self-referential theorems, prehistoric cycles are not a sufficient condition. Second, with an expansion on that definition, prehistoric cycles become sufficient for self-referential theorems.",http://arxiv.org/abs/1902.01106v2,,arXiv,0
Self-Referential Noise and the Synthesis of Three-Dimensional Space,"Reginald T. Cahill, Christopher M. Klinger",1998,Generalising results from Godel and Chaitin in mathematics suggests that self-referential systems contain intrinsic randomness. We argue that this is relevant to modelling the universe and show how three-dimensional space may arise from a non-geometric order-disorder model driven by self-referential noise.,http://arxiv.org/abs/gr-qc/9812083v2,10.1023/A:1001984518976,arXiv,0
Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding,"Mirac Suzgun, Adam Tauman Kalai",2024,"We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct ""expert"" instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks. The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions. Furthermore, our research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility. Through rigorous experimentation with GPT-4, we establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1%, expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.",http://arxiv.org/abs/2401.12954v1,,arXiv,0
The ultimate tactics of self-referential systems,Christine Cordula Dantas,2015,"Mathematics is usually regarded as a kind of language. The essential behavior of physical phenomena can be expressed by mathematical laws, providing descriptions and predictions. In the present essay I argue that, although mathematics can be seen, in a first approach, as a language, it goes beyond this concept. I conjecture that mathematics presents two extreme features, denoted here by {\sl irreducibility} and {\sl insaturation}, representing delimiters for self-referentiality. These features are then related to physical laws by realizing that nature is a self-referential system obeying bounds similar to those respected by mathematics. Self-referential systems can only be autonomous entities by a kind of metabolism that provides and sustains such an autonomy. A rational mind, able of consciousness, is a manifestation of the self-referentiality of the Universe. Hence mathematics is here proposed to go beyond language by actually representing the most fundamental existence condition for self-referentiality. This idea is synthesized in the form of a principle, namely, that {\sl mathematics is the ultimate tactics of self-referential systems to mimic themselves}. That is, well beyond an effective language to express the physical world, mathematics uncovers a deep manifestation of the autonomous nature of the Universe, wherein the human brain is but an instance.",http://arxiv.org/abs/1506.04952v1,10.1007/978-3-319-27495-9_17,arXiv,0
Self-reference Upfront: A Study of Self-referential Gödel Numberings,"Balthasar Grabmayr, Albert Visser",2020,"In this paper we examine various requirements on the formalisation choices under which self-reference can be adequately formalised in arithmetic. In particular, we study self-referential numberings, which immediately provide a strong notion of self-reference even for expressively weak languages. The results of this paper suggest that the question whether truly self-referential reasoning can be formalised in arithmetic is more sensitive to the underlying coding apparatus than usually believed. As a case study, we show how this sensitivity affects the formal study of certain principles of self-referential truth.",http://arxiv.org/abs/2006.12178v2,,arXiv,0
Ordinal Folding Index: A Computable Metric for Self-Referential Semantics,"Faruk Alpay, Hamdi Al Alakkad",2025,"The Ordinal Folding Index (OFI) is a new, fully computable yard-stick that measures how many rounds of self-reference a statement, protocol or position must unfold before its truth or outcome stabilises. By turning this abstract 'fold-back' depth into a single ordinal number, OFI forges a direct link between areas that are usually studied in isolation: the closure stages of fixed-point logics, the time-to-win values of infinite parity games, and the ordinal progressions that calibrate the strength of formal theories. We prove that OFI refines all classical game-theoretic and logical metrics while remaining algorithmically enumerable, supply a polynomial-time approximation scheme on finite arenas, and show how the index coincides exactly with the length of the shortest winning strategy in the associated evaluation game. Alongside the theory we outline five open problems from the completeness of the computable-ordinal spectrum to the possibility of 'compressing' deep self-reference that chart a research programme at the intersection of computer-aided logic, algorithmic game theory and ordinal analysis. OFI thus invites game theorists and logicians alike to view infinite play, transfinite induction and reflective reasoning through a single, intuitive lens, opening common ground for techniques.",http://arxiv.org/abs/2508.00151v1,,arXiv,0
Self-Referential Order,"T. Aste, P. Butler, T. Di Matteo",2013,"We introduce the concept of {\it self-referential order} which provides a way to quantify structural organization in non crystalline materials. The key idea consists in the observation that, in a disordered system, where there is no ideal, reference, template structure, each sub-portion of the whole structure can be taken as reference for the rest and the system can be described in terms of its parts in a self-referential way. Some of the parts carry larger information about the rest of the structure and they are identified as {\it motifs}. We discuss how this method can efficiently reduce the amount of information required to describe a complex disordered structure by encoding it in a set of motifs and {\it matching rules}. We propose an information-theoretic approach to define a {\it self-referential-order-parameter} and we show that, by means of entropic measures, such a parameter can be quantified explicitly. A proof of concept application to equal disk packing is presented and discussed.",http://arxiv.org/abs/1305.5090v2,10.1080/14786435.2013.835495,arXiv,0
Attention Prompting on Image for Large Vision-Language Models,"Runpeng Yu, Weihao Yu, Xinchao Wang",2024,"Compared with Large Language Models (LLMs), Large Vision-Language Models (LVLMs) can also accept images as input, thus showcasing more interesting emergent capabilities and demonstrating impressive performance on various vision-language tasks. Motivated by text prompting in LLMs, visual prompting has been explored to enhance LVLMs' capabilities of perceiving visual information. However, previous visual prompting techniques solely process visual inputs without considering text queries, limiting the models' ability to follow text instructions to complete tasks. To fill this gap, in this work, we propose a new prompting technique named Attention Prompting on Image, which just simply overlays a text-query-guided attention heatmap on the original input image and effectively enhances LVLM on various tasks. Specifically, we generate an attention heatmap for the input image dependent on the text query with an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel values of the original image to obtain the actual input image for the LVLM. Extensive experiments on various vison-language benchmarks verify the effectiveness of our technique. For example, Attention Prompting on Image improves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks, respectively.",http://arxiv.org/abs/2409.17143v1,,arXiv,0
A non self-referential expression of Tsallis' probability distribution function,"T. Wada, A. M. Scarfone",2005,"The canonical probability distribution function (pdf) obtained by optimizing the Tsallis entropy under the linear mean energy constraint (first formalism) or the escort mean energy constraint (third formalism) suffer self-referentiality. In a recent paper [Phys. Lett. A {\bf335} (2005) 351-362] the authors have shown that the pdfs obtained in the two formalisms are equivalent to the pdf in non self-referential form. Based on this result we derive an alternative expression, which is non self-referential, for the Tsallis distributions in both first and third formalisms.",http://arxiv.org/abs/cond-mat/0502394v1,10.1140/epjb/e2005-00356-3,arXiv,0
Training-free Regional Prompting for Diffusion Transformers,"Anthony Chen, Jianjin Xu, Wenzhao Zheng, Gaole Dai, Yida Wang, Renrui Zhang, Haofan Wang, Shanghang Zhang",2024,"Diffusion models have demonstrated excellent capabilities in text-to-image generation. Their semantic understanding (i.e., prompt following) ability has also been greatly improved with large language models (e.g., T5, Llama). However, existing models cannot perfectly handle long and complex text prompts, especially when the text prompts contain various objects with numerous attributes and interrelated spatial relationships. While many regional prompting methods have been proposed for UNet-based models (SD1.5, SDXL), but there are still no implementations based on the recent Diffusion Transformer (DiT) architecture, such as SD3 and FLUX.1.In this report, we propose and implement regional prompting for FLUX.1 based on attention manipulation, which enables DiT with fined-grained compositional text-to-image generation capability in a training-free manner. Code is available at https://github.com/antonioo-c/Regional-Prompting-FLUX.",http://arxiv.org/abs/2411.02395v1,,arXiv,0
Self-referential theories,Samuel Allen Alexander,2020,"We study the structure of families of theories in the language of arithmetic extended to allow these families to refer to one another and to themselves. If a theory contains schemata expressing its own truth and expressing a specific Turing index for itself, and contains some other mild axioms, then that theory is untrue. We exhibit some families of true self-referential theories that barely avoid this forbidden pattern.",http://arxiv.org/abs/2008.11535v1,,arXiv,0
Self-Referential Definition of Orthogonality,"Elemer E Rosinger, Gusti van Zyl",2009,"There has for longer been an interest in finding equivalent conditions which define inner product spaces, and the respective literature is considerable, see for instance Amir, which lists 350 such results. Here, in this tradition, an alternative definition of orthogonality is presented which does not make use of any inner product. This definition, in the spirit of the recently developed non-wellfounded set theory, is self-referential, or circulatory.",http://arxiv.org/abs/0904.0082v2,,arXiv,0
"Brief Lecture Notes on Self-Referential Mathematics, and Beyond",Elemer E Rosinger,2009,"Recently delivered lectures on Self-Referential Mathematics, [2], at the Department of Mathematics and Applied Mathematics, University of Pretoria, are briefly presented. Comments follow on the subject, as well as on Inconsistent Mathematics.",http://arxiv.org/abs/0905.0227v1,,arXiv,0
Bootstrap Universe from Self-Referential Noise,"Reginald T. Cahill, Christopher M. Klinger",1997,We further deconstruct Heraclitean Quantum Systems giving a model for a universe using pregeometric notions in which the end-game problem is overcome by means of self-referential noise. The model displays self-organisation with the emergence of 3-space and time. The time phenomenon is richer than the present geometric modelling.,http://arxiv.org/abs/gr-qc/9708013v1,,arXiv,0
Guiding Large Language Models via Directional Stimulus Prompting,"Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan",2023,"We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts. The code and data are publicly available at \url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.",http://arxiv.org/abs/2302.11520v4,,arXiv,0
Goedel Machines: Self-Referential Universal Problem Solvers Making Provably Optimal Self-Improvements,Juergen Schmidhuber,2003,"We present the first class of mathematically rigorous, general, fully self-referential, self-improving, optimally efficient problem solvers. Inspired by Kurt Goedel's celebrated self-referential formulas (1931), such a problem solver rewrites any part of its own code as soon as it has found a proof that the rewrite is useful, where the problem-dependent utility function and the hardware and the entire initial code are described by axioms encoded in an initial proof searcher which is also part of the initial code. The searcher systematically and efficiently tests computable proof techniques (programs whose outputs are proofs) until it finds a provably useful, computable self-rewrite. We show that such a self-rewrite is globally optimal - no local maxima! - since the code first had to prove that it is not useful to continue the proof search for alternative self-rewrites. Unlike previous non-self-referential methods based on hardwired proof searchers, ours not only boasts an optimal order of complexity but can optimally reduce any slowdowns hidden by the O()-notation, provided the utility of such speed-ups is provable at all.",http://arxiv.org/abs/cs/0309048v5,,arXiv,0
An Exploration of a Reflective Evaluation Tool for the Teaching Competency of Pre-Service Physical Education Teachers in Korea,"Chul-min Kim, Eunchang Kwak",2022,"The purpose of this study was to develop an evaluation tool that could help pre-service teachers develop their teaching skills. As for the research method, the Delphi technique was used to collect opinions from physical education professor evaluation experts. The survey was conducted three times and various opinions of experts were collected and analyzed. The newly constructed evaluation tool consists of 46 questions for class preparation (the creation of the learning environment), the introduction (routine activities, learning goals, and task presentation), development (class strategy, observation and interaction, and the maintenance of the learning environment), and conclusion (routine activities, summary, and closure). It was designed to increase the accuracy of evaluation by developing evaluation criteria for each question. An evaluation tool including quantitative and qualitative methods for use in pre-service physical education teacher education was developed. The significance of this study is the development of an effective evaluation tool that can evaluate the core teaching behaviors in the field of physical education. This evaluation tool should be used as a learning tool that includes planning, operation, evaluation, and seeking improvement measures through reflective activities. If pre-service teacher education institutions apply this evaluation tool in their teacher training programs, it would be a great chance to learn how to develop and sustain teaching abilities and effectiveness.",https://www.semanticscholar.org/paper/ae80dac5b9732076fb3c7ef18b129a0b57160bcd,10.3390/su14138195,Semantic Scholar,8
On Teaching Reflective Evaluation,"Yu-jing Zhang, Zhiquan Zhang",2022,,https://www.semanticscholar.org/paper/ab76a18d8bd3748c041f2148e1affa3d124bab0c,,Semantic Scholar,0
Zines as Reflective Evaluation Within Interdisciplinary Learning Programmes,"Autumn Brown, Mairéad Hurley, Sophie Perry, Joseph Roche",2021,"This paper presents a unique method for documenting and reflecting learning in interdisciplinary science learning settings, which prioritises the perspectives of marginalised learners and which may be used across cultural contexts. Short for “magazine” or “fanzine,” zines are small DIY booklets which can contain poetry, narrative, drawings, comics, collage and more. Often associated with radical or alternative cultures, they can become a kind of self-made soapbox for the creator, a material artifact that, by its very deconstructed and deconstructing nature, encourages a personalised remixing of ideas. Within this paper, we examine the practical and pedagogical positioning of zines within a STEAM (Science, Technology, Engineering, Arts, and Mathematics) context. As both a visual and text-based artifact, a zine is uniquely capable of capturing broad responses to diverse learning experiences which blur disciplinary boundaries and offers an inclusive and firmly emancipatory approach to reflective practice.",https://www.semanticscholar.org/paper/2561039257d328635027f9d00b626e43c8aa7034,10.3389/feduc.2021.675329,Semantic Scholar,7
Affective Evaluations of Exercising: The Role of Automatic-Reflective Evaluation Discrepancy.,"R. Brand, Franziska Antoniewicz",2016,,https://www.semanticscholar.org/paper/89017bfb1adf8129ccbff6f12c7a3c29c7c6b5df,10.1123/jsep.2016-0171,Semantic Scholar,37
Evaluating the outcomes and processes of a research-action partnership: The need for continuous reflective evaluation,"Chantal L. Taylor, J. Cockburn, M. Rouget, Jayanti Ray-Mukherjee, S. Mukherjee, R. Slotow, D. Roberts, R. Boon, S. O’Donoghue, E. Douwes",2016,"Background : The KwaZulu-Natal Sandstone Sourveld (KZNSS) Research Programme is part of a collaborative, transdisciplinary research partnership between the University of KwaZulu-Natal and the eThekwini Municipality (EM), aimed at bridging the science-policy-practice gap. The research programme focuses on generating knowledge and capacity to support local land-use planning, management and policy development related to biodiversity and climate change issues. Objectives : The objectives were (1) to describe how a continuous reflective evaluation approach helped to better understand the research programme and its outcomes; and (2) to assess research outputs and outcomes, relevance of outcomes to the requirements of EM, and participants’ perceptions of the programme (both the outcomes and the process). Methods : The evaluation took a mixed methods approach, combining various quantitative and qualitative methods such as anonymous individual questionnaires, reflective exercises and group reflections. Results : The KZNSS programme was successful in capacity building and establishing a long-term partnership, but had lower scientific publication output and practice uptake than expected. Participants’ perceptions changed over time, with a decrease in the perceived success of addressing tangible research outcomes, and an increase in the perceived success of collaborative relationships in the partnership. Conclusion : Transdisciplinary partnerships can be a means of integrating research into policy and practice through knowledge exchange. An important lesson in the early stages of this partnership was to pay attention to the process and not only the outputs. The study highlights the importance of continuous participatory reflection and evaluation in such partnerships.",https://www.semanticscholar.org/paper/f1c935a0a9c70272e98b07bba8cca774048894a5,10.4102/ABC.V46I2.2154,Semantic Scholar,17
Evaluation of Reflective Measurement Models,"Joseph F. Hair, G. Hult, C. Ringle, M. Sarstedt, N. Danks, Soumya Ray",2021,"The goal of reflective measurement model assessment is to ensure the reliability and validity of the construct measures and therefore provides support for the suitability of their inclusion in the path model. This chapter introduces the key criteria that are relevant in reflective measurement model assessment: indicator reliability, internal consistency reliability (Cronbach’s alpha, reliability coefficient rhoA, and composite reliability rhoC), convergent validity, and discriminant validity. We illustrate their use by means of the SEMinR package and a well-known model on corporate reputation.",https://www.semanticscholar.org/paper/df92ae5cf549f6a39b66d9fb3b476bba41dae0b2,10.1007/978-3-030-80519-7_4,Semantic Scholar,236
PCI: A Reflective Evaluation Framework for Systems Change,"Beverly A. Parsons, Huilan Y. Krenn",2018,,https://www.semanticscholar.org/paper/44f2b07e79e52ba4c6cb192de724b944f67565b7,10.9707/1944-5660.1405,Semantic Scholar,1
Development and psychometric validation of the REFlective evaLuation of psoriasis Efficacy of Treatment and Severity (REFLETS) questionnaire: a common measure of plaque-type psoriasis severity and treatment efficacy for patients and clinicians,"H. Gilet, A. Climens, B. Arnould, Hervé Bachelez, M. Bagot, P. Beaulieu, Pascal Joly, Denis Jullien, M. Maître, J. Ortonne, Carle Paul, E. Thibout",2014,"To date, there is no global consensus on the definition of the severity of psoriasis. The REFlective evaLuation of psoriasis Efficacy of Treatment and Severity (REFLETS) questionnaire has recently been developed to provide a better understanding of plaque‐type psoriasis severity and treatment efficacy from both patient and clinician perspectives.",https://www.semanticscholar.org/paper/9186815145dff4259b94828428b848d40755173c,10.1111/jdv.12601,Semantic Scholar,13
A Critical Evaluation of the Reflective Functioning Questionnaire (RFQ),"Sascha Müller, L. Wendt, C. Spitzer, O. Masuhr, Sarah N. Back, J. Zimmermann",2021,"Abstract The Reflective Functioning Questionnaire (RFQ) is an 8-item self-report measure of reflective functioning that is presumed to capture individual differences in hypo- and hypermentalizing. Despite its broad acceptance by the field, we argue that the validity of the measure is not well-established. The current research elaborates on problems of the RFQ related to its item content, scoring procedure, dimensionality, and associations with psychopathology. We tested these considerations across three large clinical and non-clinical samples from Germany and the US (total N = 2289). In a first study, we found that the RFQ may assess a single latent dimension related to hypomentalizing but is rather unlikely to capture maladaptive forms of hypermentalizing. Moreover, the RFQ exhibited very strong associations with measures of personality pathology, while associations with measures of symptom distress were less strong. In a second preregistered study focused on convergent and discriminant validity, however, a commonality analysis indicated that associations with indicators of personality pathology are inflated because some of the RFQ items tap into emotional lability and impulsivity rather than mentalizing. Our findings demonstrate limitations of the RFQ. We discuss key challenges in assessing mentalizing via self-report.",https://www.semanticscholar.org/paper/35d7edc4fc18531232016c765d92817c7e8e25ba,10.1080/00223891.2021.1981346,Semantic Scholar,89
"AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls","Yu Du, Fangyun Wei, Hongyang Zhang",2024,"We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench. Code will be available at https://github.com/dyabel/AnyTool.",https://www.semanticscholar.org/paper/24a56d41df2c939c3223bde7bf26050302c3dea7,10.48550/arXiv.2402.04253,Semantic Scholar,74
A guide for evaluation of online learning in medical education: a qualitative reflective analysis,"Nourhan F. Wasfy, Enjy Abouzeid, A. A. Nasser, S. Ahmed, Ilham Youssry, N. Hegazy, M. Shehata, Doaa Kamal, H. Atwa",2021,"Background With the strike of Covid-19, an unprecedented rapid shift to remote learning happened worldwide with a paradigm shift to online learning from an institutional adjuvant luxury package and learner choice into a forced solo choice. This raises the question of quality assurance. While some groups have already established standards for online courses, teaching and programs yet very little information is included on methodology of their development and very little emphasis is placed on the online learning experience. Nevertheless, no work has been done specifically for medical education institutions. Aim To develop a set of descriptors for best practice in online learning in medical education utilizing existing expertise and needs. Methods This work utilizes a qualitative multistage approach to identify the descriptors of best practice in online learning starting with a question guided focus group, thematic analysis, Delphi technique and an expert consensus session done simultaneously for triangulation. This was done involving 32 institution in 19 countries. Results This materialized into the development of a set of standards, indicators, and development of a checklist for each standard area. The standard areas identified were organizational capacity, educational effectiveness, and human resources each of which listed a number of standards. Expert consensus sessions identified the need for qualification of data and thus the development of indicators for best practice. Conclusion Standards are needed for online learning experience and their development and redesign is situational and needs to be enhanced methodologically in axes that are pertaining to the needs of the education community. Taking such axes into consideration by educators and institutions will lead to planning and implementing successful online learning activities, while taking them into consideration by the evaluators will help them conduct comprehensive audits and provide stakeholders with highly informative evaluation reports.",https://www.semanticscholar.org/paper/4b8cf91d6fab8e72b805d94827dd026a9fed8418,10.1186/s12909-021-02752-2,Semantic Scholar,39
Evaluation of students’ scientific process skills through reflective worksheets in the inquiry-based learning environments,Ayfer Mutlu,2020,"ABSTRACT This study aimed to evaluate how 7th-grade students’ scientific process skills changed in the inquiry-based learning environment through reflective worksheets. For this purpose, four inquiry-based activities related to electrical circuits were developed. Forty students enrolled in 7th grade in a public school in a mid-size city of Turkey located in the north-west region participated in the study, and they are randomly stratified into eight groups. While students carried out the activities, they used reflective worksheets, including two parts, for reflection of the development of their scientific process skills. Instructions were conducted for four weeks, and all the 32 reflective worksheets were evaluated by using the assessment rubric (for Part-I) and content analysis (for Part-II). The results obtained from both parts of the reflective worksheets showed that inquiry-based learning activities promoted students’ scientific process skills such as defining the problem, formulating a hypothesis, observing and interpreting results during the inquiry-based learning process. Students also improved in terms of ability such as using scientific terms, drawing scientific and comprehensible figures, and making scientific explanations. In addition to these, it was found that students had more positive opinions about the learning process.",https://www.semanticscholar.org/paper/1c084c323614ce75ed56500c80def74a691ba776,10.1080/14623943.2020.1736999,Semantic Scholar,64
Reflective capacity in nurses in specialist education: Swedish translation and psychometric evaluation of the Reflective Capacity Scale of the Reflective Practice Questionnaire,"S. Gustafsson, Å. Engström, Britt-Marie Lindgren, S. Gabrielsson",2020,This study aimed to test the validity and reliability of the Swedish version of the Reflective Capacity Scale of the Reflective Practice Questionnaire in a nursing context.,https://www.semanticscholar.org/paper/1df2435e05cecd26385c326133cd7259dfbd4b9b,10.1002/nop2.659,Semantic Scholar,14
Design of a generic questionnaire for reflective evaluation of a virtual reality-based intervention using virtual dolphins for children with autism,"N. Chia, Jenyi Li",2012,,https://www.semanticscholar.org/paper/b870681037a317b534b7a3bfae78b1099bc403f0,,Semantic Scholar,11
Supporting foster carers to meet the needs of looked after children: A feasibility and pilot evaluation of the Reflective Fostering Programme,"N. Midgley, Antonella Cirasola, Chloe Austerberry, Erica Ranzato, Grace West, Peter Martin, S. Redfern, Richard Cotmore, Theresa Park",2019,"This study presents the feasibility and pilot evaluation of the Reflective Fostering Programme (RFP), a recently developed, group-based program to support foster carers, based on the concept of “reflective parenting.” This innovative development follows calls by the National Institute for Health and Clinical Excellence and other organizations to help improve outcomes for children in care by providing better support to their carers. This study aimed to establish whether it is possible to implement the RFP and to gather preliminary data on the acceptability and effectiveness of the program. Twenty-eight foster carers took part in the study. Results indicate that training and delivery of the RFP were feasible; the program was felt to be relevant and meaningful to both foster carers and social care professionals delivering it. Preliminary pre-post evaluation showed a statistically significant improvement in foster carers’ stress, their achievement of self-defined goals and child’s emotion lability and overall strengths and difficulties. There were no statistically significant changes in carers’ reflective functioning, although some foster carers reported on changes in reflective capacity during focus groups. Preliminary findings about the feasibility of training and delivery of the RFP, as well as the acceptability and effectiveness of the program, are encouraging, but further impact evaluation is needed.",https://www.semanticscholar.org/paper/8eb0ab60bc6618835203a49c5fdd8b3a73290789,10.1177/2516103218817550,Semantic Scholar,40
Experimental thermal evaluation of building roofs with conventional and reflective coatings,"I. Hernández-Pérez, J. Xamán, E. Macias-Melo, K. Aguilar-Castro, I. Zavala-Guillén, I. Hernández-López, E. Simá",2018,,https://www.semanticscholar.org/paper/4c2f78a6e2dafbea88d3dbebe9bab6926eaccab2,10.1016/J.ENBUILD.2017.09.085,Semantic Scholar,70
Characterization and thermal performance evaluation of infrared reflective coatings compatible with historic buildings,"F. Becherini, E. Lucchi, A. Gandini, Maria Casado Barrasa, A. Troi, F. Roberti, Maria Sachini, M. C. D. Tuccio, Leire Garmendia Arrieta, L. Pockelè, A. Bernardi",2018,,https://www.semanticscholar.org/paper/fda50450002879928c37d0fb5ee3d1741e61b1c9,10.1016/J.BUILDENV.2018.02.034,Semantic Scholar,45
A reflective evaluation of patient handover practices.,"S. Davies, M. Priestley",2006,,https://www.semanticscholar.org/paper/75c663d6df503368422d534069bb9781591743e9,10.7748/NS2006.02.20.21.49.C4056,Semantic Scholar,41
"CARE: An Integrated Framework to Support Continuous, Adaptable, Reflective Evaluation of E-Government Systems","G. Orange, A. Burke, Tony Elliman, A. Kor",2007,,https://www.semanticscholar.org/paper/fddc4ccb1b3bf0d0c3fc045717ad55e15326655c,10.4018/JCEC.2007070102,Semantic Scholar,18
Evaluation of measurment and structural model of the reflective model constructs in PLS – SEM,"M. Janadari, Subramaniam Sri Ramalu, C. Wei",2016,,https://www.semanticscholar.org/paper/a59ff6259d42e0c8f4c8ab1f29d4bbb8f87540dc,,Semantic Scholar,71
Reflective Cardinals,Dmytro Taranovsky,2012,"We introduce and axiomatize the notion of a reflective cardinal, use it to give semantics to higher order set theory, and explore connections between the notion of reflective cardinals and large cardinal axioms.",http://arxiv.org/abs/1203.2270v5,,arXiv,0
Normalization of peer-evaluation measures of group research quality across academic disciplines,"Ralph Kenna, Bertrand Berche",2010,"Peer-evaluation based measures of group research quality such as the UK's Research Assessment Exercise (RAE), which do not employ bibliometric analyses, cannot directly avail of such methods to normalize research impact across disciplines. This is seen as a conspicuous flaw of such exercises and calls have been made to find a remedy. Here a simple, systematic solution is proposed based upon a mathematical model for the relationship between research quality and group quantity. This model manifests both the Matthew effect and a phenomenon akin to the Ringelmann effect and reveals the existence of two critical masses for each academic discipline: a lower value, below which groups are vulnerable, and an upper value beyond which the dependency of quality on quantity reduces and plateaus appear when the critical masses are large. A possible normalization procedure is then to pitch these plateaus at similar levels. We examine the consequences of this procedure at RAE for a multitude of academic disciplines, corresponding to a range of critical masses.",http://arxiv.org/abs/1006.3863v2,10.3152/095820211X12941371876625,arXiv,0
"FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity","Shiyao Cui, Zhenyu Zhang, Yilong Chen, Wenyuan Zhang, Tianyun Liu, Siqi Wang, Tingwen Liu",2023,"The widespread of generative artificial intelligence has heightened concerns about the potential harms posed by AI-generated texts, primarily stemming from factoid, unfair, and toxic content. Previous researchers have invested much effort in assessing the harmlessness of generative language models. However, existing benchmarks are struggling in the era of large language models (LLMs), due to the stronger language generation and instruction following capabilities, as well as wider applications. In this paper, we propose FFT, a new benchmark with 2116 elaborated-designed instances, for LLM harmlessness evaluation with factuality, fairness, and toxicity. To investigate the potential harms of LLMs, we evaluate 9 representative LLMs covering various parameter scales, training stages, and creators. Experiments show that the harmlessness of LLMs is still under-satisfactory, and extensive analysis derives some insightful findings that could inspire future research for harmless LLM research.",http://arxiv.org/abs/2311.18580v2,,arXiv,0
Experimental Realization of a Reflective Optical Limiter,"Jarrett H. Vella, John H. Goldsmith, Andrew T. Browning, Nicholaos I. Limberopoulos, Ilya Vitebskiy, Eleana Makri, Tsampikos Kottos",2015,"Optical limiters transmit low-intensity light, while blocking laser radiation with excessively high intensity or fluence. A typical passive optical limiter absorbs most of the high level radiation, which can cause irreversible damage. In this communication we report the first experimental realization of a reflective optical limiter, which does not absorb the high-level laser radiation, but rather reflects it back to space. The design is based on a periodic layered structure composed of alternating SiO2 and Si3N4 layers with a single GaAs defect layer in the middle. At low intensities, the layered structure displays a strong resonant transmission via the localized defect mode. At high intensities, the two-photon absorption in the GaAs layer suppresses the localized mode along with the resonant transmission, the entire layered structure turns highly reflective within a broad frequency range covering the entire photonic band gap of the periodic layered structure. By contrast, a stand-alone GaAs layer would absorb most of the high-level radiation, thus acting as a basic absorptive optical limiter. The proposed design can only perform at shortwave IR, where GaAs displays negligible linear absorption and very strong nonlinear two-photon absorption. With judicious choice of optical materials, the same principle can be replicated for any other frequency range.",http://arxiv.org/abs/1510.08028v2,10.1103/PhysRevApplied.5.064010,arXiv,0
Reflective Oracles: A Foundation for Classical Game Theory,"Benja Fallenstein, Jessica Taylor, Paul F. Christiano",2015,"Classical game theory treats players as special---a description of a game contains a full, explicit enumeration of all players---even though in the real world, ""players"" are no more fundamentally special than rocks or clouds. It isn't trivial to find a decision-theoretic foundation for game theory in which an agent's coplayers are a non-distinguished part of the agent's environment. Attempts to model both players and the environment as Turing machines, for example, fail for standard diagonalization reasons.
  In this paper, we introduce a ""reflective"" type of oracle, which is able to answer questions about the outputs of oracle machines with access to the same oracle. These oracles avoid diagonalization by answering some queries randomly. We show that machines with access to a reflective oracle can be used to define rational agents using causal decision theory. These agents model their environment as a probabilistic oracle machine, which may contain other agents as a non-distinguished part.
  We show that if such agents interact, they will play a Nash equilibrium, with the randomization in mixed strategies coming from the randomization in the oracle's answers. This can be seen as providing a foundation for classical game theory in which players aren't special.",http://arxiv.org/abs/1508.04145v1,,arXiv,0
Understanding User Perspectives on Prompts for Brief Reflection on Troubling Emotions,"Ananya Bhattacharjee, Pan Chen, Linjia Zhou, Abhijoy Mandal, Jai Aggarwal, Katie O'Leary, Anne Hsu, Alex Mariakakis, Joseph Jay Williams",2021,"We investigate users' perspectives on an online reflective question activity (RQA) that prompts people to externalize their underlying emotions on a troubling situation. Inspired by principles of cognitive behavioral therapy, our 15-minute activity encourages self-reflection without a human or automated conversational partner. A deployment of our RQA on Amazon Mechanical Turk suggests that people perceive several benefits from our RQA, including structured awareness of their thoughts and problem-solving around managing their emotions. Quantitative evidence from a randomized experiment suggests people find that our RQA makes them feel less worried by their selected situation and worth the minimal time investment. A further two-week technology probe deployment with 11 participants indicates that people see benefits to doing this activity repeatedly, although the activity may get monotonous over time. In summary, this work demonstrates the promise of online reflection activities that carefully leverage principles of psychology in their design.",http://arxiv.org/abs/2112.10833v1,,arXiv,0
"Reflective hyperbolic 2-elementary lattices, K3 surfaces and hyperkahler varieties",Valery Alexeev,2022,"We compute Coxeter diagrams of several ``large'' reflective even 2-elementary hyperbolic lattices and their maximal parabolic subdiagrams, and give some applications of these results to the theory of K3 surfaces and hyperkahler varieties.",http://arxiv.org/abs/2209.09110v4,,arXiv,0
Self-correspondences of K3 surfaces via moduli of sheaves and arithmetic hyperbolic reflection groups,Viacheslav V. Nikulin,2008,"An integral hyperbolic lattice is called reflective if its automorphism group is generated by reflections, up to finite index. Since 1981, it is known that their number is essentially finite.
  We show that K3 surfaces over C with reflective Picard lattices can be characterized in terms of compositions of their self-correspondences via moduli of sheaves with primitive isotropic Mukai vector: Their self-correspondences with integral action on the Picard lattice are numerically equivalent to compositions of a finite number of especially simple self-correspondences via moduli of sheaves.
  This relates two topics: Self-correspondences of K3 surfaces via moduli of sheaves and Arithmetic hyperbolic reflection groups. It also raises several natural unsolved related problems.",http://arxiv.org/abs/0810.2945v3,,arXiv,0
Arithmetic hyperbolic reflection groups,Mikhail Belolipetsky,2015,A hyperbolic reflection group is a discrete group generated by reflections in the faces of an $n$-dimensional hyperbolic polyhedron. This survey article is dedicated to the study of arithmetic hyperbolic reflection groups with an emphasis on the results that were obtained in the last ten years and on the open problems.,http://arxiv.org/abs/1506.03111v4,,arXiv,0
A Reflective Approach to Providing Flexibility in Application Distribution,"Álvaro Rebón Portillo, Scott Walker, Graham Kirby, Alan Dearle",2010,"Current middleware systems suffer from drawbacks. Often one is forced to make decisions early in the design process about which classes may participate in inter-machine communication. Further, application level and middleware specific semantics cannot be separated forcing an unnatural design. The RAFDA project proposes to adress these deficiencies by creating an adaptive, reflective framework that enables the transformation of non-distributed applications into isomorphic applications whose distribution architecture is flexible. This paper describes the code transformation techniques that have been developed as part of the project. The system enables the distribution of a program according to a flexible configuration without user intervention. Proxy objects can then be substituted, permitting cross-address space communication. The distributed program can adapt to its environment by dynamically altering its distribution boundaries.",http://arxiv.org/abs/1006.5643v1,,arXiv,0
Galerkin Methods for Boltzmann-Poisson transport with reflection conditions on rough boundaries,"Jose A. Morales Escalante, Irene M. Gamba",2015,"We consider in this paper the mathematical and numerical modelling of reflective boundary conditions (BC) associated to Boltzmann - Poisson systems, including diffusive reflection in addition to specularity, in the context of electron transport in semiconductor device modelling at nano scales, and their implementation in Discontinuous Galerkin (DG) schemes. We study these BC on the physical boundaries of the device and develop a numerical approximation to model an insulating boundary condition, or equivalently, a pointwise zero flux mathematical condition for the electron transport equation. Such condition balances the incident and reflective momentum flux at the microscopic level, pointwise at the boundary, in the case of a more general mixed reflection with momentum dependant specularity probability $p(\vec{k})$. We compare the computational prediction of physical observables given by the numerical implementation of these different reflection conditions in our DG scheme for BP models, and observe that the diffusive condition influences the kinetic moments over the whole domain in position space.",http://arxiv.org/abs/1512.09210v3,10.1016/j.jcp.2018.02.041,arXiv,0
UAV-Assisted and Intelligent Reflecting Surfaces-Supported Terahertz Communications,"Yijin Pan, Kezhi Wang, Cunhua Pan, Huiling Zhu, Jiangzhou Wang",2020,"In this paper, unmanned aerial vehicles (UAVs) and intelligent reflective surface (IRS) are utilized to support terahertz (THz) communications.
  To this end, the joint optimization of UAV's trajectory, the phase shift of IRS, the allocation of THz sub-bands, and the power control is investigated to maximize the minimum average achievable rate of all the users.
  An iteration algorithm based on successive Convex Approximation with the Rate constraint penalty (CAR) is developed to obtain UAV's trajectory, and the IRS phase shift is formulated as a closed-form expression with introduced pricing factors.
  Simulation results show that the proposed scheme significantly enhances the rate performance of the whole system.",http://arxiv.org/abs/2010.14223v2,,arXiv,0
Bragg-Berry mirrors: reflective broadband q-plates,"Mushegh Rafayelyan, Etienne Brasselet",2016,We report on the experimental realization of flat mirrors enabling the broadband generation of optical vortices upon reflection. The effect is based on the geometric Berry phase associated with the circular Bragg reflection phenomenon from chiral uniaxial media. We show the reflective optical vortex generation from both diffractive and nondiffractive paraxial light beams using spatially patterned chiral liquid crystal films. The intrinsic spectrally broadband character of spin-orbit generation of optical phase singularities is demonstrated over the full visible domain. Our results do not rely on any birefringent retardation requirement and consequently foster the development of a novel generation of robust optical elements for spin-orbit photonic technologies.,http://arxiv.org/abs/1606.02762v2,10.1364/OL.41.003972,arXiv,0
A Pragmatic Guide to Geoparsing Evaluation,"Milan Gritta, Mohammad Taher Pilehvar, Nigel Collier",2018,"Empirical methods in geoparsing have thus far lacked a standard evaluation framework describing the task, metrics and data used to compare state-of-the-art systems. Evaluation is further made inconsistent, even unrepresentative of real-world usage by the lack of distinction between the different types of toponyms, which necessitates new guidelines, a consolidation of metrics and a detailed toponym taxonomy with implications for Named Entity Recognition (NER) and beyond. To address these deficiencies, our manuscript introduces a new framework in three parts. Part 1) Task Definition: clarified via corpus linguistic analysis proposing a fine-grained Pragmatic Taxonomy of Toponyms. Part 2) Metrics: discussed and reviewed for a rigorous evaluation including recommendations for NER/Geoparsing practitioners. Part 3) Evaluation Data: shared via a new dataset called GeoWebNews to provide test/train examples and enable immediate use of our contributions. In addition to fine-grained Geotagging and Toponym Resolution (Geocoding), this dataset is also suitable for prototyping and evaluating machine learning NLP models.",http://arxiv.org/abs/1810.12368v5,,arXiv,0
Evaluation Agent: Efficient and Promptable Evaluation Framework for Visual Generative Models,"Fan Zhang, Shulin Tian, Ziqi Huang, Yu Qiao, Ziwei Liu",2024,"Recent advancements in visual generative models have enabled high-quality image and video generation, opening diverse applications. However, evaluating these models often demands sampling hundreds or thousands of images or videos, making the process computationally expensive, especially for diffusion-based models with inherently slow sampling. Moreover, existing evaluation methods rely on rigid pipelines that overlook specific user needs and provide numerical results without clear explanations. In contrast, humans can quickly form impressions of a model's capabilities by observing only a few samples. To mimic this, we propose the Evaluation Agent framework, which employs human-like strategies for efficient, dynamic, multi-round evaluations using only a few samples per round, while offering detailed, user-tailored analyses. It offers four key advantages: 1) efficiency, 2) promptable evaluation tailored to diverse user needs, 3) explainability beyond single numerical scores, and 4) scalability across various models and tools. Experiments show that Evaluation Agent reduces evaluation time to 10% of traditional methods while delivering comparable results. The Evaluation Agent framework is fully open-sourced to advance research in visual generative models and their efficient evaluation.",http://arxiv.org/abs/2412.09645v3,,arXiv,0
A comparative evaluation and analysis of three generations of Distributional Semantic Models,"Alessandro Lenci, Magnus Sahlgren, Patrick Jeuniaux, Amaru Cuba Gyllensten, Martina Miliani",2021,"Distributional semantics has deeply changed in the last decades. First, predict models stole the thunder from traditional count ones, and more recently both of them were replaced in many NLP applications by contextualized vectors produced by Transformer neural language models. Although an extensive body of research has been devoted to Distributional Semantic Model (DSM) evaluation, we still lack a thorough comparison with respect to tested models, semantic tasks, and benchmark datasets. Moreover, previous work has mostly focused on task-driven evaluation, instead of exploring the differences between the way models represent the lexical semantic space. In this paper, we perform a comprehensive evaluation of type distributional vectors, either produced by static DSMs or obtained by averaging the contextualized vectors generated by BERT. First of all, we investigate the performance of embeddings in several semantic tasks, carrying out an in-depth statistical analysis to identify the major factors influencing the behavior of DSMs. The results show that i.) the alleged superiority of predict based models is more apparent than real, and surely not ubiquitous and ii.) static DSMs surpass contextualized representations in most out-of-context semantic tasks and datasets. Furthermore, we borrow from cognitive neuroscience the methodology of Representational Similarity Analysis (RSA) to inspect the semantic spaces generated by distributional models. RSA reveals important differences related to the frequency and part-of-speech of lexical items.",http://arxiv.org/abs/2105.09825v2,,arXiv,0
Rethinking Machine Learning Model Evaluation in Pathology,"Syed Ashar Javed, Dinkar Juyal, Zahil Shanis, Shreya Chakraborty, Harsha Pokkalla, Aaditya Prakash",2022,"Machine Learning has been applied to pathology images in research and clinical practice with promising outcomes. However, standard ML models often lack the rigorous evaluation required for clinical decisions. Machine learning techniques for natural images are ill-equipped to deal with pathology images that are significantly large and noisy, require expensive labeling, are hard to interpret, and are susceptible to spurious correlations. We propose a set of practical guidelines for ML evaluation in pathology that address the above concerns. The paper includes measures for setting up the evaluation framework, effectively dealing with variability in labels, and a recommended suite of tests to address issues related to domain shift, robustness, and confounding variables. We hope that the proposed framework will bridge the gap between ML researchers and domain experts, leading to wider adoption of ML techniques in pathology and improving patient outcomes.",http://arxiv.org/abs/2204.05205v3,,arXiv,0
Meta-Evaluation of Translation Evaluation Methods: a systematic up-to-date overview,"Lifeng Han, Serge Gladkoff",2016,"Starting from the 1950s, Machine Translation (MT) was challenged by different scientific solutions, which included rule-based methods, example-based and statistical models (SMT), to hybrid models, and very recent years the neural models (NMT). While NMT has achieved a huge quality improvement in comparison to conventional methodologies, by taking advantage of a huge amount of parallel corpora available from the internet and the recently developed super computational power support with an acceptable cost, it struggles to achieve real human parity in many domains and most language pairs, if not all of them. Alongside the long road of MT research and development, quality evaluation metrics played very important roles in MT advancement and evolution. In this tutorial, we overview the traditional human judgement criteria, automatic evaluation metrics, unsupervised quality estimation models, as well as the meta-evaluation of the evaluation methods. Among these, we will also cover the very recent work in the MT evaluation (MTE) fields, taking advantage of the large size of pre-trained language models for automatic metric customisation towards exactly deployed language pairs and domains. In addition, we also introduce the statistical confidence estimation regarding the sample size needed for human evaluation in real practice simulation. Full tutorial material is \textbf{available} to download at https://github.com/poethan/LREC22_MetaEval_Tutorial.",http://arxiv.org/abs/1605.04515v9,,arXiv,0
ReIFE: Re-evaluating Instruction-Following Evaluation,"Yixin Liu, Kejian Shi, Alexander R. Fabbri, Yilun Zhao, Peifeng Wang, Chien-Sheng Wu, Shafiq Joty, Arman Cohan",2024,"The automatic evaluation of instruction following typically involves using large language models (LLMs) to assess response quality. However, there is a lack of comprehensive evaluation of these LLM-based evaluators across two dimensions: the base LLMs and the evaluation protocols. Therefore, we present a thorough meta-evaluation of instruction following, including 25 base LLMs and 15 recently proposed evaluation protocols, on 4 human-annotated datasets, assessing the evaluation accuracy of the LLM-evaluators. Our evaluation allows us to identify the best-performing base LLMs and evaluation protocols with a high degree of robustness. Moreover, our large-scale evaluation reveals: (1) Base LLM performance ranking remains largely consistent across evaluation protocols, with less capable LLMs showing greater improvement from protocol enhancements; (2) Robust evaluation of evaluation protocols requires many base LLMs with varying capability levels, as protocol effectiveness can depend on the base LLM used; (3) Evaluation results on different datasets are not always consistent, so a rigorous evaluation requires multiple datasets with distinctive features. We release our meta-evaluation suite ReIFE, which provides the codebase and evaluation result collection for more than 500 LLM-evaluator configurations, to support future research in instruction-following evaluation.",http://arxiv.org/abs/2410.07069v1,,arXiv,0
Reproducible Subjective Evaluation,"Max Morrison, Brian Tang, Gefei Tan, Bryan Pardo",2022,"Human perceptual studies are the gold standard for the evaluation of many research tasks in machine learning, linguistics, and psychology. However, these studies require significant time and cost to perform. As a result, many researchers use objective measures that can correlate poorly with human evaluation. When subjective evaluations are performed, they are often not reported with sufficient detail to ensure reproducibility. We propose Reproducible Subjective Evaluation (ReSEval), an open-source framework for quickly deploying crowdsourced subjective evaluations directly from Python. ReSEval lets researchers launch A/B, ABX, Mean Opinion Score (MOS) and MUltiple Stimuli with Hidden Reference and Anchor (MUSHRA) tests on audio, image, text, or video data from a command-line interface or using one line of Python, making it as easy to run as objective evaluation. With ReSEval, researchers can reproduce each other's subjective evaluations by sharing a configuration file and the audio, image, text, or video files.",http://arxiv.org/abs/2203.04444v1,,arXiv,0
"Meeseeks: A Feedback-Driven, Iterative Self-Correction Benchmark evaluating LLMs'Instruction Following Capability","Jiaming Wang, Yunke Zhao, Peng Ding, Jun Kuang, Yibin Shen, Zhe Tang, Yilin Jin, Zongyu Wang, Xiaoyu Li, Xuezhi Cao, Xunliang Cai",2025,"The capability to precisely adhere to instructions is a cornerstone for Large Language Models (LLMs) to function as dependable agents in real-world scenarios. However, confronted with complex prompts, LLMs frequently encounter difficulties in fulfilling all specified requirements within a single response. Drawing inspiration from recent advancements in Chain-of-Thought (CoT) prompting and self-correction methodologies, we introduce Meeseeks (The name is inspired by Mr. Meeseeks from""Rick and Morty,""a character renowned for efficiently accomplishing assigned tasks. See: https://en.wikipedia.org/wiki/Mr._Meeseeks), a fully automated iterative instruction-following benchmark equipped with an integrated feedback mechanism. Meeseeks identifies erroneous components in model responses and provides corresponding feedback accurately, thereby iteratively guiding the model toward self-correction. The dataset contains over 700 curated instances annotated by 32 distinct capability tags in Chinese and English. Extensive experimental results reveal that different state-of-the-art commercial and open-source LLMs exhibit vastly disparate performance, and even after 20 turns of iterative feedback-driven self-correction, nearly all models demonstrate suboptimal performance. We conducted comprehensive analysis from both macro and instance levels, uncovering numerous common issues prevalent in current state-of-the-art models, as well as several counterintuitive phenomena. We've open-sourced our work on https://github.com/ADoublLEN/Meeseeks.",https://www.semanticscholar.org/paper/2d75ddb607af256068d7797955fa810132d17581,,Semantic Scholar,2
PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction,"Xiaolu Hou, Bing Ma, Jiaxiang Cheng, Xuhua Ren, Kai Yu, Wenyue Li, Tianxiang Zheng, Qinglin Lu",2025,"With the growing demand for short videos and personalized content, automated Video Log (Vlog) generation has become a key direction in multimodal content creation. Existing methods mostly rely on predefined scripts, lacking dynamism and personal expression. Therefore, there is an urgent need for an automated Vlog generation approach that enables effective multimodal collaboration and high personalization. To this end, we propose PersonaVlog, an automated multimodal stylized Vlog generation framework that can produce personalized Vlogs featuring videos, background music, and inner monologue speech based on a given theme and reference image. Specifically, we propose a multi-agent collaboration framework based on Multimodal Large Language Models (MLLMs). This framework efficiently generates high-quality prompts for multimodal content creation based on user input, thereby improving the efficiency and creativity of the process. In addition, we incorporate a feedback and rollback mechanism that leverages MLLMs to evaluate and provide feedback on generated results, thereby enabling iterative self-correction of multimodal content. We also propose ThemeVlogEval, a theme-based automated benchmarking framework that provides standardized metrics and datasets for fair evaluation. Comprehensive experiments demonstrate the significant advantages and potential of our framework over several baselines, highlighting its effectiveness and great potential for generating automated Vlogs.",https://www.semanticscholar.org/paper/20f408f62be92cd36bbcbdff4ceda7409baf9262,10.48550/arXiv.2508.13602,Semantic Scholar,0
Iterative self-correction for secured images using turbo codes and soft input decryption,"N. Zivic, O. Rehman",2020,"An algorithm for improved error correction of the data protected using standard security mechanisms, like Message Authentication Codes, is introduced in this paper. Images are taken as example data, and the correction of images using the additionally available authentication data is investigated. Parts of the protected image, the so called “landmarks”, are considered as the Region of Interest and protected by their respective authentication tags. An iterative process is used as a basis for the learning ability of the algorithm: in every iteration, parts of the decoder’s Trellis path are learned by the algorithm, thereby improving the channel decoding results. In this way, the knowledge gained in the previous iterations of the algorithm is used in the current iteration for further improvement of decoding results. Additionally, iterative processes used for error corrections are supported by authentication tags, which are used as a measure of correction success. Simulation results for images are presented to show the effectiveness of the proposed algorithm. A security analysis of the proposed algorithm is also given in the paper.",https://www.semanticscholar.org/paper/e9f22368e8b4af69673f62d3284f497bb582a2e2,10.3233/JIFS-190234,Semantic Scholar,3
"MCQG-SRefine: Multiple Choice Question Generation and Evaluation with Iterative Self-Critique, Correction, and Comparison Feedback","Zonghai Yao, Aditya Parashar, Huixue Zhou, Won Seok Jang, Feiyun Ouyang, Zhichao Yang, Hong Yu",2024,"Automatic question generation (QG) is essential for AI and NLP, particularly in intelligent tutoring, dialogue systems, and fact verification. Generating multiple-choice questions (MCQG) for professional exams, like the United States Medical Licensing Examination (USMLE), is particularly challenging, requiring domain expertise and complex multi-hop reasoning for high-quality questions. However, current large language models (LLMs) like GPT-4 struggle with professional MCQG due to outdated knowledge, hallucination issues, and prompt sensitivity, resulting in unsatisfactory quality and difficulty. To address these challenges, we propose MCQG-SRefine, an LLM self-refine-based (Critique and Correction) framework for converting medical cases into high-quality USMLE-style questions. By integrating expert-driven prompt engineering with iterative self-critique and self-correction feedback, MCQG-SRefine significantly enhances human expert satisfaction regarding both the quality and difficulty of the questions. Furthermore, we introduce an LLM-as-Judge-based automatic metric to replace the complex and costly expert evaluation process, ensuring reliable and expert-aligned assessments.",https://www.semanticscholar.org/paper/d5544571fa37a6eaac972153cec57df591c38a04,10.48550/arXiv.2410.13191,Semantic Scholar,13
Towards Intrinsic Self-Correction Enhancement in Monte Carlo Tree Search Boosted Reasoning via Iterative Preference Learning,"Huchen Jiang, Yangyang Ma, Chaofan Ding, Kexin Luan, Xinhan Di",2024,"With current state-of-the-art approaches aimed at enhancing the reasoning capabilities of Large Language Models(LLMs) through iterative preference learning inspired by AlphaZero, we propose to further enhance the step-wise reasoning capabilities through intrinsic self-correction to some extent. Our work leverages step-wise preference learning to enhance self-verification via reinforcement learning. We initially conduct our work through a two-stage training procedure. At the first stage, the self-correction reasoning ability of an LLM is enhanced through its own predictions, relying entirely on self-generated data within the intrinsic self-correction to some extent. At the second stage, the baseline step-wise preference learning is leveraged via the application of the enhanced self-correct policy achieved at the first stage. In the evaluation of arithmetic reasoning tasks, our approach outperforms OpenMath2-Llama3.1-8B, dart-math-mistral-7b-uniform on MATH with increases in accuracy to 71.34%(+4.18%) and 48.06%(+4.94%) and LLama-3.1-8B-Instruct, Mistral-7B-Instruct-v0.1 on GSM8K with increases in accuracy to 86.76%(+2.00%) and 38.06%(+2.28%).",https://www.semanticscholar.org/paper/96c90a0c76f361fc45af33569d9f58aa3ea7350b,10.48550/arXiv.2412.17397,Semantic Scholar,2
Remote Sensing Image Rectangling With Iterative Warping Kernel Self-Correction Transformer,"Linwei Qiu, Fengying Xie, Chang Liu, Ke Wang, Xuedong Song, Z. Shi",2024,"Stitched remote sensing images often exhibit irregular boundaries, which can be frustrating for general users and detrimental to downstream tasks such as object detection and segmentation. However, this issue has received insufficient attention and remains unexplored within the remote sensing domain. In this study, we investigate mesh-based rectangling techniques for remote sensing images, aiming to produce rectangular outputs while preserving the original field-of-view (FoV) and avoiding the introduction of unreliable content. Observing that prior rectangling algorithms tend to generate unsatisfactory boundaries or discernible distortions, that is, under-rectangling or over-rectangling, we propose the concept of a warping kernel associated with mesh deformations to account for these phenomena. Consequently, we introduce the iterative warping kernel self-correction transformer (IWKFormer), designed to enhance warping kernel estimation and generate superior rectangular outcomes. It primarily comprises two components: a mesh feature extractor built upon the partial swin transformer block (PSTB) and a corrector module using the swin transformer block (STB). These modules collaborate to derive warping kernels implicitly. The extractor extracts latent features pertinent to mesh deformation, whereas the corrector iteratively refines the warping kernel estimation to improve the ultimate prediction. Furthermore, to bolster further research, we have constructed an aerial imagery stitching rectangling dataset (AIRD), featuring a wide array of stitching scenes. Extensive experimentation on the AIRD demonstrates that our method yields visually appealing and naturally rectangled images, achieving state-of-the-art performance. The code and data will be available at https://github.com/yyywxk/IWKFormer.",https://www.semanticscholar.org/paper/146a7e5871d271ed34d52a32284ff31d71057571,10.1109/TGRS.2024.3441246,Semantic Scholar,2
On the Intrinsic Self-Correction Capability of LLMs: Uncertainty and Latent Concept,"Guang-Da Liu, Haitao Mao, Bochuan Cao, Zhiyu Xue, K. Johnson, Jiliang Tang, Rongrong Wang",2024,"Large Language Models (LLMs) are able to improve their responses when instructed to do so, a capability known as self-correction. When instructions provide only the task's goal without specific details about potential issues in the response, LLMs must rely on their internal knowledge to improve response quality, a process referred to as intrinsic self-correction. The empirical success of intrinsic self-correction is evident in various applications, but how and why it is effective remains unknown. In this paper, we unveil that intrinsic self-correction can be progressively improved, allowing it to approach a converged state. Our findings are verified in: (1) the scenario of multi-round question answering, by comprehensively demonstrating that intrinsic self-correction can progressively introduce performance gains through iterative interactions, ultimately converging to stable performance; and (2) the context of intrinsic self-correction for enhanced morality, in which we provide empirical evidence that iteratively applying instructions reduces model uncertainty towards convergence, which then leads to convergence of both the calibration error and self-correction performance, ultimately resulting in a stable state of intrinsic self-correction. Furthermore, we introduce a mathematical formulation and a simulation task indicating that the latent concepts activated by self-correction instructions drive the reduction of model uncertainty. Based on our experimental results and analysis of the convergence of intrinsic self-correction, we reveal its underlying mechanism: consistent injected instructions reduce model uncertainty which yields converged, improved performance.",https://www.semanticscholar.org/paper/f2cf484329dabf3499b17d724cf476ccf92aea9c,10.48550/arXiv.2406.02378,Semantic Scholar,16
Embedding Self-Correction as an Inherent Ability in Large Language Models for Enhanced Mathematical Reasoning,"Kuofeng Gao, Huanqia Cai, Qingyao Shuai, Dihong Gong, Zhifeng Li",2024,"Accurate mathematical reasoning with Large Language Models (LLMs) is crucial in revolutionizing domains that heavily rely on such reasoning. However, LLMs often encounter difficulties in certain aspects of mathematical reasoning, leading to flawed reasoning and erroneous results. To mitigate these issues, we introduce a novel mechanism, the Chain of Self-Correction (CoSC), specifically designed to embed self-correction as an inherent ability in LLMs, enabling them to validate and rectify their own results. The CoSC mechanism operates through a sequence of self-correction stages. In each stage, the LLMs generate a program to address a given problem, execute this program using program-based tools to obtain an output, subsequently verify this output. Based on the verification, the LLMs either proceed to the next correction stage or finalize the answer. This iterative self-correction process allows the LLMs to refine its reasoning steps and improve the accuracy of its mathematical reasoning. We implement CoSC using a two-phase fine-tuning approach. First, LLMs are trained with a relatively small volume of seeding data generated from GPT-4. Then, we enhance CoSC by training with a larger volume of self-generated data, without relying on GPT-4. Experiments show that CoSC significantly boosts performance on standard mathematical datasets compared to existing open-source LLMs. Notably, our CoSC-Code-34B model achieved a 53.5% score on the challenging MATH dataset, outperforming models like ChatGPT, GPT-4, and multi-modal LLMs such as GPT-4V and Gemini-1.0. Importantly, CoSC operates in a zero-shot manner without requiring demonstrations.",https://www.semanticscholar.org/paper/8980d6721cb335c7754839f0c02b9a96a5b8c346,10.48550/arXiv.2410.10735,Semantic Scholar,14
Quantitative Analysis method of laser-induced breakdown spectroscopy based on temperature iterative correction of self-absorption effect,"Jiajia Hou, Dacheng Zhang, Zhongqi Feng, Jiangfeng Zhu",2024,"Laser-induced breakdown spectroscopy (LIBS) is an ideal real-time on-line method for the detection of minor elements in alloys. However, in the case of high-density plasma generated by LIBS, the self-absorption is usually an undesired effect because it not only reduces the true line intensity, introduces nonlinear effects in the growth of line intensity versus the content of the emitting species, but also affects the characterization parameters of the plasma, and finally affects the accuracy of quantitative analysis. Since the plasma electron temperature (T), radiation particle number density and absorption path length (Nl) determine the degree of self-absorption and affect the corrected spectral line intensity, a new self-absorption correction method based on temperature iteration is proposed. This method obtains the initial T through spectral line intensity, and calculates the self-absorption coefficient SA based on the initial Nl parameter to correct the spectral line intensity. Then a new T is obtained from the new spectral line intensity and the new SA is calculated to further correct the spectral line intensity. Through continuous calculation and correction of these two parameters, self-absorption correction is finally achieved. The experimental results of alloy steel samples showed that the linearity of Boltzmann plots was increased from 0.867 without self-absorption correction to 0.974 with self-absorption correction, and the linear correlation coefficient R2 of the single variable calibration curve for Mn element increased from 0.971 to 0.997. The relative error of elemental content measurement has significantly improved from 4.32% without self-absorption correction to 1.23% with self-absorption correction. Compared with the commonly applied self-absorption correction methods, this method has obvious advantages of simpler programming, higher computation efficiency, and its independency of the availability or accuracy of Stark broadening coefficients. Moreover, this method can directly obtain the radiation particle number density and absorption path length, which is beneficial to the diagnosis and quantitative analysis of plasma.",https://www.semanticscholar.org/paper/9bbd28159f7b9563ee1794e5604a6d3598c476a0,10.7498/aps.73.20231541,Semantic Scholar,0
VASCAR: Content-Aware Layout Generation via Visual-Aware Self-Correction,"Jiahao Zhang, Ryota Yoshihashi, Shunsuke Kitada, Atsuki Osanai, Yuta Nakashima",2024,"Large language models (LLMs) have proven effective for layout generation due to their ability to produce structure-description languages, such as HTML or JSON. In this paper, we argue that while LLMs can perform reasonably well in certain cases, their intrinsic limitation of not being able to perceive images restricts their effectiveness in tasks requiring visual content, e.g., content-aware layout generation. Therefore, we explore whether large vision-language models (LVLMs) can be applied to content-aware layout generation. To this end, inspired by the iterative revision and heuristic evaluation workflow of designers, we propose the training-free Visual-Aware Self-Correction LAyout GeneRation (VASCAR). VASCAR enables LVLMs (e.g., GPT-4o and Gemini) iteratively refine their outputs with reference to rendered layout images, which are visualized as colored bounding boxes on poster background (i.e., canvas). Extensive experiments and user study demonstrate VASCAR's effectiveness, achieving state-of-the-art (SOTA) layout generation quality. Furthermore, the generalizability of VASCAR across GPT-4o and Gemini demonstrates its versatility.",https://www.semanticscholar.org/paper/d52aa072ef15e93546bbc99891c065edf05b2a75,10.48550/arXiv.2412.04237,Semantic Scholar,9
Iterative Error Self-Correction for Robust Fault Diagnosis of Mechanical Equipment With Noisy Label,"Huan Wang, Yanfang Li",2022,"Deep neural network (DNN) is an effective technology for machinery fault diagnosis. The good performance of DNN is based on the assumption that all labels are completely correct. However, mislabeled data are common in actual industrial applications, which will cause severe performance degradation. This article explores the performance of DNN under noisy labels and the reasons for its performance degradation. Furthermore, a novel iterative error self-correction (IESC) algorithm based on the maximum activation of softmax is proposed. During the training process, the label is dynamically optimized, and it is no longer fixed. IESC automatically models the distribution of correct labels, gradually identifies incorrect labels, and automatically corrects incorrect labels. In addition, a noise-tolerant loss is introduced to enhance the network’s noise robustness. Experiments on two real machinery fault diagnosis cases prove that our method has excellent label correction and fault diagnosis performance. It significantly improves the performance of DNNs and promotes its practical application potential.",https://www.semanticscholar.org/paper/aa94c979f18746d690b6046e8c9b8853f061ed4c,10.1109/tim.2022.3189730,Semantic Scholar,6
Self-supervised video distortion correction algorithm based on iterative optimization,"Zhihao Ren, Ya Su",2023,,https://www.semanticscholar.org/paper/81364066329e828349a4961014abceacfc8a46ce,10.1016/j.patcog.2023.110114,Semantic Scholar,4
An Iterative Self-Absorption Correction Algorithm for 3D Ptycho-Fluorescence Imaging,"S. Cipiccia, D. Batey, Xiaowen Shi, Steve Price, A. Parsons, K. Wanelik, A. Wilson, R. Crook, R. Raja, C. Rau",2018,"X-ray micro-fluorescence is a powerful nondestructive imaging technique to access the chemical composition of samples. The x-ray fluorescence tomography requires absorption correction when the self-absorption of the sample is not negligible. Schroer in 2001[1] tackled the problem calculating the mass absorption coefficient at the fluorescence energies from the asymmetry of the fluorescence sinogram. Alternatively the mass absorption coefficients have been estimated from combining fluorescence, transmission and Compton tomography[2]. Based on Schroer study, Huang in 2017 [3] presented an iterative self-absorption correction algorithm for quantitative x-ray fluorescence computed tomography.",https://www.semanticscholar.org/paper/08a579e9faa52056db66a43a5b405af7da1fea69,10.1017/S1431927618012862,Semantic Scholar,2
OpenVLThinker: An Early Exploration to Complex Vision-Language Reasoning via Iterative Self-Improvement,"Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, Kai-Wei Chang",2025,,https://www.semanticscholar.org/paper/f19443485fb8ac84718da18da891e616401d7492,10.48550/arXiv.2503.17352,Semantic Scholar,115
Self-Iterative Correction Second-order Differential Passive Positioning Sensor Technology Based on Interferometer Auxiliary,"Gaogao Liu, Qidong Zhang, Jian Xu, Ziyu Huang, Beibei Mu, Jiangbo Zhu, Zhao Wang, Hongfu Guo",2025,,https://www.semanticscholar.org/paper/5ca3d4469d1c37609797e9df371a1ee45e62d71e,10.1109/taes.2025.3625903,Semantic Scholar,0
ProofNet++: A Neuro-Symbolic System for Formal Proof Verification with Self-Correction,Murari Ambati,2025,"We propose ProofNet++, a neuro-symbolic framework that enhances automated theorem proving by combining large language models (LLMs) with formal proof verification and self-correction mechanisms. Current LLM-based systems suffer from hallucinated logical steps and unverifiable reasoning. ProofNet++ mitigates these limitations by integrating symbolic proof tree supervision, a reinforcement learning loop using verifiers as reward functions, and an iterative self-correction module. Our experiments on miniF2F, Lean's mathlib, and HOL Light show that ProofNet++ significantly improves proof accuracy, correctness, and formal verifiability over prior models. We provide theoretical analysis of the convergence and stability of the verifier-guided RL framework and release our datasets and codebase for future research.",https://www.semanticscholar.org/paper/1dfc9916fddcf2820cdbc23f9cd3e95170a4ce28,10.48550/arXiv.2505.24230,Semantic Scholar,1
Self-Taught Self-Correction for Small Language Models,"Viktor Moskvoretskii, Christian Biemann, Irina Nikishina",2025,"Although large language models (LLMs) have achieved remarkable performance across various tasks, they remain prone to errors. A key challenge is enabling them to self-correct. While prior research has relied on external tools or large proprietary models, this work explores self-correction in small language models (SLMs) through iterative fine-tuning using solely self-generated data. We introduce the Self-Taught Self-Correction (STaSC) algorithm, which incorporates multiple algorithmic design choices. Experimental results on a question-answering task demonstrate that STaSC effectively learns self-correction, leading to significant performance improvements. Our analysis further provides insights into the mechanisms of self-correction and the impact of different design choices on learning dynamics and overall performance. To support future research, we release our user-friendly codebase and lightweight models.",https://www.semanticscholar.org/paper/81a42c8c87994a5978471423b66ccaa283232577,10.48550/arXiv.2503.08681,Semantic Scholar,4
A Probabilistic Inference Scaling Theory for LLM Self-Correction,"Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, Zhifang Sui",2025,"Large Language Models (LLMs) have demonstrated the capability to refine their generated answers through self-correction, enabling continuous performance improvement over multiple rounds. However, the mechanisms underlying how and why accuracy evolves during this iterative process remain unexplored. To fill this gap, we propose a probabilistic theory to model the dynamics of accuracy change and explain the performance improvements observed in multi-round self-correction. Through mathematical derivation, we establish that the accuracy after the $t^{th}$ round of self-correction is given by: $Acc_t = Upp - \alpha^t(Upp - Acc_0),$ where $Acc_0$ denotes the initial accuracy, $Upp$ represents the upper bound of accuracy convergence, and $\alpha$ determines the rate of convergence. Based on our theory, these parameters can be calculated and the predicted accuracy curve then can be obtained through only a single round of self-correction. Extensive experiments across diverse models and datasets demonstrate that our theoretical predictions align closely with empirical accuracy curves, validating the effectiveness of the theory. Our work provides a theoretical foundation for understanding LLM self-correction, thus paving the way for further explorations.",https://www.semanticscholar.org/paper/80886f50bde843b01bedc8296986f159bb4f6f0e,10.48550/arXiv.2508.16456,Semantic Scholar,1
Noise Self-Correction via Relation Propagation for Robust Cross-Modal Retrieval,"Ruoxuan Li, Xiangyu Wu, Yang Yang",2025,"Cross-modal retrieval refers to identifying semantically relevant data across different modalities. However, annotation errors or inherent ambiguity can cause semantic inconsistency in sample pairs, degrading retrieval performance. Prior efforts either relied heavily on the quality of explicitly dividing clean and noisy subsets, or solely leveraged carefully selected single anchor information, neglecting relationships among diverse neighbors. In this paper, we propose a novel Graph-based Label Propagation (GLP) framework that learns pseudo-labels via label propagation on a sparse graph, enabling self-correction of noisy labels. Specifically, each modality's instances are treated as nodes, connected via k-nearest neighbor (kNN) search to form a sparse graph. Pseudo-label vectors are generated for all nodes within one modality to capture the matching degree of inter-modal nodes. Through iterative label propagation, the stabilized pseudo-labels implicitly exploit both intra- and inter-modal relationships to derive a reliable matching degree. A dynamic queue further enhances graph quality by updating high-quality nodes. Experiments on Flickr30K, MSCOCO, and CC120K show that our method outperforms state-of-the-art approaches, especially under high noise. Code is available at https://github.com/njustkmg/MM25-GLP.",https://www.semanticscholar.org/paper/6143453c5fbedfca4bd8eaf8967208633d34e40e,10.1145/3746027.3755585,Semantic Scholar,1
A System Error Self-Correction Target-Positioning Method in Video Satellite Observation,"Xiangru Bai, Haibo Song, Caizhi Fan, Liwei Hao, Yueneng Yang",2025,"Satellite-based target positioning is vital for applications like disaster relief and precision mapping. Practically, satellite errors, e.g., thermal deformation and attitude errors, lead to a mix of fixed and random errors in the measured line-of-sight angles, resulting in a decline in target-positioning accuracy. Motivated by this concern, this study introduces a systematic error self-correction target-positioning method under continuous observations using a single video satellite. After analyzing error sources and establishing an error-inclusive positioning model, we formulate a dimension-extended equation estimating both target position and fixed biases. Based on the equation, a projection transformation method is proposed to obtain the linearized estimation of unknown parameters first, and an iterative optimization method is then utilized to further refine the estimate. Compared with state-of-the-art algorithms, the proposed method can improve positioning accuracy by 98.70% in simulation scenarios with large fixed errors. Thus, the simulation and actual data calculation results demonstrate that, compared with state-of-the-art algorithms, the proposed algorithm effectively improves the target-positioning accuracy under non-ideal error conditions.",https://www.semanticscholar.org/paper/ddc2b47d9e008ff9fb25d6f5681a248259ae0a49,10.3390/rs17172935,Semantic Scholar,1
Learning Iterative Reasoning through Energy Minimization,"Yilun Du, Shuang Li, Joshua B. Tenenbaum, Igor Mordatch",2022,"Deep learning has excelled on complex pattern recognition tasks such as image classification and object recognition. However, it struggles with tasks requiring nontrivial reasoning, such as algorithmic computation. Humans are able to solve such tasks through iterative reasoning -- spending more time thinking about harder tasks. Most existing neural networks, however, exhibit a fixed computational budget controlled by the neural network architecture, preventing additional computational processing on harder tasks. In this work, we present a new framework for iterative reasoning with neural networks. We train a neural network to parameterize an energy landscape over all outputs, and implement each step of the iterative reasoning as an energy minimization step to find a minimal energy solution. By formulating reasoning as an energy minimization problem, for harder problems that lead to more complex energy landscapes, we may then adjust our underlying computational budget by running a more complex optimization procedure. We empirically illustrate that our iterative reasoning approach can solve more accurate and generalizable algorithmic reasoning tasks in both graph and continuous domains. Finally, we illustrate that our approach can recursively solve algorithmic problems requiring nested reasoning",http://arxiv.org/abs/2206.15448v1,,arXiv,0
Stabilization and Variations to the Adaptive Local Iterative Filtering Algorithm: the Fast Resampled Iterative Filtering Method,"Giovanni Barbarino, Antonio Cicone",2021,"Non-stationary signals are ubiquitous in real life. Many techniques have been proposed in the last decades which allow decomposing multi-component signals into simple oscillatory mono-components, like the groundbreaking Empirical Mode Decomposition technique and the Iterative Filtering method. When a signal contains mono-components that have rapid varying instantaneous frequencies, we can think, for instance, to chirps or whistles, it becomes particularly hard for most techniques to properly factor out these components. The Adaptive Local Iterative Filtering technique has recently gained interest in many applied fields of research for being able to deal with non-stationary signals presenting amplitude and frequency modulation. In this work, we address the open question of how to guarantee a priori convergence of this technique, and propose two new algorithms. The first method, called Stable Adaptive Local Iterative Filtering, is a stabilized version of the Adaptive Local Iterative Filtering that we prove to be always convergent. The stability, however, comes at the cost of higher complexity in the calculations. The second technique, called Resampled Iterative Filtering, is a new generalization of the Iterative Filtering method. We prove that Resampled Iterative Filtering is guaranteed to converge a priori for any kind of signal. Furthermore, in the discrete setting, by leveraging on the mathematical properties of the matrices involved, we show that its calculations can be accelerated drastically. Finally, we present some artificial and real-life examples to show the powerfulness and performance of the proposed methods.",http://arxiv.org/abs/2111.02764v2,10.1007/s00211-024-01394-y,arXiv,0
A deflated Schur complement method for the iterative solution of a high-order discontinuous element discretization of the Poisson equation,"Sumedh Joshi, Peter Diamessis",2016,"A combination of block-Jacobi and deflation preconditioning is used to solve a high-order discontinuous element-based collocation discretization of the Schur complement of the Poisson-Neumann system as arises in the operator splitting of the incompressible Navier-Stokes equations. The ill-posedness of the Poisson-Neumann system manifests as an inconsistency of the Schur complement problem, but it is shown that this can be accounted for with appropriate projections out of the null space of the Schur complement matrix without affecting the accuracy of the solution. The block-Jacobi preconditioner, combined with deflation, is shown to yield GMRES convergence independent of the polynomial order of expansion within an element. Finally, while the number of GMRES iterations does grow as the element size is reduced (e.g. $h$-refinement), the dependence is very mild; the number of GMRES iterations roughly doubles as the element size is divided by a factor of six. In light of these numerical results, the deflated Schur complement approach seems practicable, especially for high-order methods given its convergence independent of polynomial order.",http://arxiv.org/abs/1601.03432v1,,arXiv,0
Mastering Agile Jumping Skills from Simple Practices with Iterative Learning Control,"Chuong Nguyen, Lingfan Bao, Quan Nguyen",2024,"Achieving precise target jumping with legged robots poses a significant challenge due to the long flight phase and the uncertainties inherent in contact dynamics and hardware. Forcefully attempting these agile motions on hardware could result in severe failures and potential damage. Motivated by these challenging problems, we propose an Iterative Learning Control (ILC) approach that aims to learn and refine jumping skills from easy to difficult, instead of directly learning these challenging tasks. We verify that learning from simplicity can enhance safety and target jumping accuracy over trials. Compared to other ILC approaches for legged locomotion, our method can tackle the problem of a long flight phase where control input is not available. In addition, our approach allows the robot to apply what it learns from a simple jumping task to accomplish more challenging tasks within a few trials directly in hardware, instead of learning from scratch. We validate the method via extensive experiments in the A1 model and hardware for various jumping tasks. Starting from a small jump (e.g., a forward leap of 40cm), our learning approach empowers the robot to accomplish a variety of challenging targets, including jumping onto a 20cm high box, jumping to a greater distance of up to 60cm, as well as performing jumps while carrying an unknown payload of 2kg. Our framework can allow the robot to reach the desired position and orientation targets with approximate errors of 1cm and 1 degree within a few trials.",http://arxiv.org/abs/2408.02619v1,,arXiv,0
Improved GUI Grounding via Iterative Narrowing,Anthony Nguyen,2024,"Graphical User Interface (GUI) grounding plays a crucial role in enhancing the capabilities of Vision-Language Model (VLM) agents. While general VLMs, such as GPT-4V, demonstrate strong performance across various tasks, their proficiency in GUI grounding remains suboptimal. Recent studies have focused on fine-tuning these models specifically for zero-shot GUI grounding, yielding significant improvements over baseline performance. We introduce a visual prompting framework that employs an iterative narrowing mechanism to further improve the performance of both general and fine-tuned models in GUI grounding. For evaluation, we tested our method on a comprehensive benchmark comprising various UI platforms and provided the code to reproduce our results.",http://arxiv.org/abs/2411.13591v7,,arXiv,0
Self-rewarding correction for mathematical reasoning,"Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, Tong Zhang",2025,"We study self-rewarding reasoning large language models (LLMs), which can simultaneously generate step-by-step reasoning and evaluate the correctness of their outputs during the inference time-without external feedback. This integrated approach allows a single model to independently guide its reasoning process, offering computational advantages for model deployment. We particularly focus on the representative task of self-correction, where models autonomously detect errors in their responses, revise outputs, and decide when to terminate iterative refinement loops. To enable this, we propose a two-staged algorithmic framework for constructing self-rewarding reasoning models using only self-generated data. In the first stage, we employ sequential rejection sampling to synthesize long chain-of-thought trajectories that incorporate both self-rewarding and self-correction mechanisms. Fine-tuning models on these curated data allows them to learn the patterns of self-rewarding and self-correction. In the second stage, we further enhance the models' ability to assess response accuracy and refine outputs through reinforcement learning with rule-based signals. Experiments with Llama-3 and Qwen-2.5 demonstrate that our approach surpasses intrinsic self-correction capabilities and achieves performance comparable to systems that rely on external reward models.",http://arxiv.org/abs/2502.19613v1,,arXiv,0
Self-overlap correction simplifies the Parisi formula for vector spins,Hong-Bin Chen,2023,"We propose a simpler approach to identifying the limit of free energy in a vector spin glass model by adding a self-overlap correction to the Hamiltonian. This avoids constraining the self-overlap and allows us to identify the limit with the classical Parisi formula, similar to the proof for scalar models with Ising spins. For the upper bound, the correction cancels self-overlap terms in Guerra's interpolation. For the lower bound, we add an extra perturbation term to make the self-overlap concentrate, a technique already used in [Probab. Math. Phys., 2(2):281-339, Ann. Inst. Henri Poincaré Probab. Stat., 59(3):1143-1182] to ensure the Ghirlanda-Guerra identities. We then remove the correction using a Hamilton-Jacobi equation technique, which yields a formula similar to that in [Ann. Probab., 46(2):865-896]. Additionally, we sketch a direct proof of the main result in [Electron. J. Probab. 25(23):1-17].",http://arxiv.org/abs/2303.16284v2,,arXiv,0
General convergence theorems for iterative processes and applications to the Weierstrass root-finding method,Petko D. Proinov,2015,"In this paper, we prove some general convergence theorems for the Picard iteration in cone metric spaces over a solid vector space. As an application, we provide a detailed convergence analysis of the Weierstrass iterative method for computing all zeros of a polynomial simultaneously. These results improve and generalize existing ones in the literature.",http://arxiv.org/abs/1503.05243v1,,arXiv,0
eHDG:An Exponentially Convergent Iterative Solver for HDG Discretizations of Hyperbolic Partial Differential Equations,"Sriramkrishnan Muralikrishnan, Minh-Binh Tran, Tan Bui-Thanh",2016,"We present a scalable and efficient iterative solver for high-order hybridized discontinuous Galerkin (HDG) discretizations of hyperbolic partial differential equations. It is an interplay between domain decomposition methods and HDG discretizations. In particular, the method is a fixed-point approach that requires only independent element-by-element local solves in each iteration. As such, it is well-suited for current and future computing systems with massive concurrencies. We rigorously show that the proposed method is exponentially convergent in the number of iterations for transport and linearized shallow water equations. Furthermore, the convergence is independent of the solution order. Various 2D and 3D numerical results for steady and time-dependent problems are presented to verify our theoretical findings.",http://arxiv.org/abs/1601.06681v2,,arXiv,0
Iterative decoding of Generalized Parallel Concatenated Block codes using cyclic permutations,"Hamid Allouch, Idriss Chana, Mostafa Belkasmi",2012,"Iterative decoding techniques have gain popularity due to their performance and their application in most communications systems. In this paper, we present a new application of our iterative decoder on the GPCB (Generalized Parallel Concatenated Block codes) which uses cyclic permutations. We introduce a new variant of the component decoder. After extensive simulation; the obtained result is very promising compared with several existing methods. We evaluate the effects of various parameters component codes, interleaver size, block size, and the number of iterations. Three interesting results are obtained; the first one is that the performances in terms of BER (Bit Error Rate) of the new constituent decoder are relatively similar to that of original one. Secondly our turbo decoding outperforms another turbo decoder for some linear block codes. Thirdly the proposed iterative decoding of GPCB-BCH (75, 51) is about 2.1dB from its Shannon limit.",http://arxiv.org/abs/1211.2960v1,,arXiv,0
Iterative Soft Shrinkage Learning for Efficient Image Super-Resolution,"Jiamian Wang, Huan Wang, Yulun Zhang, Yun Fu, Zhiqiang Tao",2023,"Image super-resolution (SR) has witnessed extensive neural network designs from CNN to transformer architectures. However, prevailing SR models suffer from prohibitive memory footprint and intensive computations, which limits further deployment on edge devices. This work investigates the potential of network pruning for super-resolution to take advantage of off-the-shelf network designs and reduce the underlying computational overhead. Two main challenges remain in applying pruning methods for SR. First, the widely-used filter pruning technique reflects limited granularity and restricted adaptability to diverse network structures. Second, existing pruning methods generally operate upon a pre-trained network for the sparse structure determination, hard to get rid of dense model training in the traditional SR paradigm. To address these challenges, we adopt unstructured pruning with sparse models directly trained from scratch. Specifically, we propose a novel Iterative Soft Shrinkage-Percentage (ISS-P) method by optimizing the sparse structure of a randomly initialized network at each iteration and tweaking unimportant weights with a small amount proportional to the magnitude scale on-the-fly. We observe that the proposed ISS-P can dynamically learn sparse structures adapting to the optimization process and preserve the sparse model's trainability by yielding a more regularized gradient throughput. Experiments on benchmark datasets demonstrate the effectiveness of the proposed ISS-P over diverse network architectures. Code is available at https://github.com/Jiamian-Wang/Iterative-Soft-Shrinkage-SR",http://arxiv.org/abs/2303.09650v2,,arXiv,0
Wave-like Decoding of Tail-biting Spatially Coupled LDPC Codes Through Iterative Demapping,"Sebastian Cammerer, Laurent Schmalen, Vahid Aref, Stephan ten Brink",2017,"For finite coupling lengths, terminated spatially coupled low-density parity-check (SC-LDPC) codes show a non-negligible rate-loss. In this paper, we investigate if this rate loss can be mitigated by tail-biting SC-LDPC codes in conjunction with iterative demapping of higher order modulation formats. Therefore, we examine the BP threshold of different coupled and uncoupled ensembles. A comparison between the decoding thresholds approximated by EXIT charts and the density evolution results of the coupled and uncoupled ensemble is given. We investigate the effect and potential of different labelings for such a set-up using per-bit EXIT curves, and exemplify the method for a 16-QAM system, e.g., using set partitioning labelings. A hybrid mapping is proposed, where different sub-blocks use different labelings in order to further optimize the decoding thresholds of tail-biting codes, while the computational complexity overhead through iterative demapping remains small.",http://arxiv.org/abs/1704.05408v1,,arXiv,0
Security Degradation in Iterative AI Code Generation -- A Systematic Analysis of the Paradox,"Shivani Shukla, Himanshu Joshi, Romilla Syed",2025,"The rapid adoption of Large Language Models(LLMs) for code generation has transformed software development, yet little attention has been given to how security vulnerabilities evolve through iterative LLM feedback. This paper analyzes security degradation in AI-generated code through a controlled experiment with 400 code samples across 40 rounds of ""improvements"" using four distinct prompting strategies. Our findings show a 37.6% increase in critical vulnerabilities after just five iterations, with distinct vulnerability patterns emerging across different prompting approaches. This evidence challenges the assumption that iterative LLM refinement improves code security and highlights the essential role of human expertise in the loop. We propose practical guidelines for developers to mitigate these risks, emphasizing the need for robust human validation between LLM iterations to prevent the paradoxical introduction of new security issues during supposedly beneficial code ""improvements"".",http://arxiv.org/abs/2506.11022v2,,arXiv,0
Iterative reconstruction excursions for Baryon Acoustic Oscillations and beyond,"Hee-Jong Seo, Atsuhisa Ota, Marcel Schmittfull, Shun Saito, Florian Beutler",2021,"The density field reconstruction technique has been widely used for recovering the Baryon Acoustic Oscillation (BAO) feature in galaxy surveys that has been degraded due to nonlinearities. Recent studies advocated adopting iterative steps to improve the recovery much beyond that of the standard technique. In this paper, we investigate the performance of a few selected iterative reconstruction techniques focusing on the BAO and the broadband-shape of the two-point clustering. We include redshift-space distortions, halo bias, and shot noise and inspect the components of the reconstructed field in Fourier space and in configuration space using both density field-based reconstruction and displacement field-based reconstruction. We find that the displacement field reconstruction becomes quickly challenging in the presence of non-negligible shot noise and therefore present surrogate methods that can be practically applied to a much more sparse field such as galaxies. For a galaxy field, implementing a debiasing step to remove the Lagrangian bias appears crucial for the displacement field reconstruction. We show that the iterative reconstruction does not substantially improve the BAO feature beyond an aggressively optimized standard reconstruction with a small smoothing kernel. However, we find taking iterative steps allows us to use a small smoothing kernel more `stably', i.e., without causing a substantial deviation from the linear power spectrum on large scales. In one specific example we studied, we find that a deviation of 13\% in $P( k \sim 0.1h/Mpc)$ with an aggressive standard reconstruction can reduce to 3-4\% with iterative steps.",http://arxiv.org/abs/2106.00530v2,10.1093/mnras/stac082,arXiv,0
"Multi-LLM Debate: Framework, Principals, and Interventions","Andrew Estornell, Yang Liu",2024,"The flexible and generalized nature of large language models has allowed for their application in a wide array of language-based domains. Much like their human contemporaries, these models are capable of engaging in discussions and debates as a means of improving answer quality. We first take a theoretical approach to analyzing debate and provide a framework through which debate can be mathe-matically examined. Building on this framework, we provide several theoretical results for multi-agent debate. In particular, we demonstrate that similar model capabilities, or similar model responses, can result in static debate dynamics where the debate procedure simply converges to the majority opinion. When this majority opinion is the result of a common misconception (possibly ingrained in the models through shared training data) debate is likely to converge to answers associated with that common misconception. Using insights from our theoretical results, we then propose three interventions that improve the efficacy of debate. For each intervention, we provide theoretical results demonstrating how debate is improved. We also demonstrate that these interventions result in better performance on four common benchmark tasks.",https://www.semanticscholar.org/paper/6099dbbe46b63a3cfb9fcdcf98170b7abc25a161,10.52202/079017-0911,Semantic Scholar,49
DebateCoder: Towards Collective Intelligence of LLMs via Test Case Driven LLM Debate for Code Generation,"Jizheng Chen, Kounianhua Du, Xinyi Dai, Weiming Zhang, Xihuai Wang, Yasheng Wang, Ruiming Tang, Weinan Zhang, Yong Yu",2025,,https://www.semanticscholar.org/paper/b09d6b3280f5b7ee418ca354d029e9a6be78b494,10.18653/v1/2025.acl-long.589,Semantic Scholar,3
Multi-Agent LLM Debate Unveils the Premise Left Unsaid,"Harvey Bonmu Ku, Jeongyeol Shin, Hyoun Jun Lee, Seonok Na, Insu Jeon",2025,,https://www.semanticscholar.org/paper/b7b08ad8c1ff2e33d62413a1d4ada3d6fd84f5ed,10.18653/v1/2025.argmining-1.6,Semantic Scholar,2
ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate,"Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shan Zhang, Jie Fu, Zhiyuan Liu",2023,"Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.",https://www.semanticscholar.org/paper/ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7,10.48550/arXiv.2308.07201,Semantic Scholar,690
SID: Multi-LLM Debate Driven by Self Signals,"Xuhang Chen, Zhifan Song, Deyi Ji, Shuo Gao, Lanyun Zhu",2025,"Large Language Models (LLMs) have exhibited impressive capabilities across diverse application domains. Recent work has explored Multi-LLM Agent Debate (MAD) as a way to enhance performance by enabling multiple LLMs to discuss and refine responses iteratively. Nevertheless, existing MAD methods predominantly focus on utilizing external structures, such as debate graphs, using LLM-as-a-Judge, while neglecting the application of self signals, such as token logits and attention, that arise during generation. This omission leads to redundant computation and potential performance degradation. In this paper, we shift the focus to the self signals of multi-LLM debate and introduce a Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of self-signals: model-level confidence and token-level semantic focus, to adaptively guide the debate process. Our approach enables high-confidence agents to exit early at the model level and compress the redundant debate contents based on the attention mechanism. We evaluate our method on various LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental results demonstrate that our method not only outperforms existing MAD techniques in accuracy but also reduces token consumption, highlighting the effectiveness of utilizing self signals in enhancing both the performance and efficiency of multi-agent debate systems. Our code will be available at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.",https://www.semanticscholar.org/paper/62642c0c76d84150906cc09b262b1d3288250bba,10.48550/arXiv.2510.06843,Semantic Scholar,0
Simulating Ethics: Using LLM Debate Panels to Model Deliberation on Medical Dilemmas,Hazem Zohny,2025,"This paper introduces ADEPT, a system using Large Language Model (LLM) personas to simulate multi-perspective ethical debates. ADEPT assembles panels of 'AI personas', each embodying a distinct ethical framework or stakeholder perspective (like a deontologist, consequentialist, or disability rights advocate), to deliberate on complex moral issues. Its application is demonstrated through a scenario about prioritizing patients for a limited number of ventilators inspired by real-world challenges in allocating scarce medical resources. Two debates, each with six LLM personas, were conducted; they only differed in the moral viewpoints represented: one included a Catholic bioethicist and a care theorist, the other substituted a rule-based Kantian philosopher and a legal adviser. Both panels ultimately favoured the same policy -- a lottery system weighted for clinical need and fairness, crucially avoiding the withdrawal of ventilators for reallocation. However, each panel reached that conclusion through different lines of argument, and their voting coalitions shifted once duty- and rights-based voices were present. Examination of the debate transcripts shows that the altered membership redirected attention toward moral injury, legal risk and public trust, which in turn changed four continuing personas' final positions. The work offers three contributions: (i) a transparent, replicable workflow for running and analysing multi-agent AI debates in bioethics; (ii) evidence that the moral perspectives included in such panels can materially change the outcome even when the factual inputs remain constant; and (iii) an analysis of the implications and future directions for such AI-mediated approaches to ethical deliberation and policy.",https://www.semanticscholar.org/paper/b74b12aa8587d6c824960009bbd55da1ee2d47b5,10.48550/arXiv.2505.21112,Semantic Scholar,0
Cross-Domain Constituency Parsing with Multi-LLM Debate,"Qingying Sun, Haiyan Tian, Dong Zhang",2025,,https://www.semanticscholar.org/paper/5d77e06b238b78c7ef4477193cb50490b52cb100,10.1007/978-981-95-0014-7_24,Semantic Scholar,0
LLM DEBATE OPPONENT : Counter-argument Generation focusing on Implicit and Critical Premises,"Taisei Ozaki, C. Nakagawa, Naoya Inoue, Shoichi Naito, Kenshi Yamaguchi",2025,.,https://www.semanticscholar.org/paper/9eec9408c0f27bb2dc9b6e9eaaae7a282630bf85,10.18653/v1/2025.naacl-srw.44,Semantic Scholar,0
Debatrix: Multi-dimensional Debate Judge with Iterative Chronological Analysis Based on LLM,"Jingcong Liang, Rong Ye, Meng Han, Ruofei Lai, Xinyu Zhang, Xuanjing Huang, Zhongyu Wei",2024,"How can we construct an automated debate judge to evaluate an extensive, vibrant, multi-turn debate? This task is challenging, as judging a debate involves grappling with lengthy texts, intricate argument relationships, and multi-dimensional assessments. At the same time, current research mainly focuses on short dialogues, rarely touching upon the evaluation of an entire debate. In this paper, by leveraging Large Language Models (LLMs), we propose Debatrix, which makes the analysis and assessment of multi-turn debates more aligned with majority preferences. Specifically, Debatrix features a vertical, iterative chronological analysis and a horizontal, multi-dimensional evaluation collaboration. To align with real-world debate scenarios, we introduced the PanelBench benchmark, comparing our system's performance to actual debate outcomes. The findings indicate a notable enhancement over directly using LLMs for debate evaluation. Source code and benchmark data are available online at https://github.com/ljcleo/debatrix .",https://www.semanticscholar.org/paper/93cc4f9633d54b71b8cad93ad12211a09be6b423,10.18653/v1/2024.findings-acl.868,Semantic Scholar,15
Can ChatGPT Defend its Belief in Truth? Evaluating LLM Reasoning via Debate,"Boshi Wang, Xiang Yue, Huan Sun",2023,"Large language models (LLMs) such as ChatGPT and GPT-4 have shown impressive performance in complex reasoning tasks. However, it is difficult to know whether the models are reasoning based on deep understandings of truth and logic, or leveraging their memorized patterns in a relatively superficial way. In this work, we explore testing LLMs' reasoning by engaging with them in a debate-like conversation, where given a question, the LLM and the user need to discuss to make the correct decision starting from opposing arguments. Upon mitigating the Clever Hans effect, our task requires the LLM to not only achieve the correct answer on its own, but also be able to hold and defend its belief instead of blindly believing or getting misled by the user's (invalid) arguments and critiques, thus testing in greater depth whether the LLM grasps the essence of the reasoning required to solve the problem. Across a range of complex reasoning benchmarks spanning math, commonsense, logic and BIG-Bench tasks, we find that despite their impressive performance as reported in existing work on generating correct step-by-step solutions in the beginning, LLMs like ChatGPT cannot maintain their beliefs in truth for a significant portion of examples when challenged by oftentimes absurdly invalid arguments. Our work points to danger zones of model alignment, and also suggests more careful treatments and interpretations of the recent findings that LLMs can improve their responses based on feedback.",https://www.semanticscholar.org/paper/d7784e9aee50148edcab64ffbeea713c19144171,10.18653/v1/2023.findings-emnlp.795,Semantic Scholar,109
"Debate, Deliberate, Decide (D3): A Cost-Aware Adversarial Framework for Reliable and Interpretable LLM Evaluation","Chaithanya Bandi, Abir Harrasse",2024,"The evaluation of Large Language Models (LLMs) remains challenging due to inconsistency, bias, and the absence of transparent decision criteria in automated judging. We present Debate, Deliberate, Decide (D3), a cost-aware, adversarial multi-agent framework that orchestrates structured debate among role-specialized agents (advocates, a judge, and an optional jury) to produce reliable and interpretable evaluations. D3 instantiates two complementary protocols: (1) Multi-Advocate One-Round Evaluation (MORE), which elicits k parallel defenses per answer to amplify signal via diverse advocacy, and (2) Single-Advocate Multi-Round Evaluation (SAMRE) with budgeted stopping, which iteratively refines arguments under an explicit token budget and convergence checks. We develop a probabilistic model of score gaps that (i) characterizes reliability and convergence under iterative debate and (ii) explains the separation gains from parallel advocacy. Under mild assumptions, the posterior distribution of the round-r gap concentrates around the true difference and the probability of mis-ranking vanishes; moreover, aggregating across k advocates provably increases expected score separation. We complement theory with a rigorous experimental suite across MT-Bench, AlignBench, and AUTO-J, showing state-of-the-art agreement with human judgments (accuracy and Cohen's kappa), reduced positional and verbosity biases via anonymization and role diversification, and a favorable cost-accuracy frontier enabled by budgeted stopping. Ablations and qualitative analyses isolate the contributions of debate, aggregation, and anonymity. Together, these results establish D3 as a principled, practical recipe for reliable, interpretable, and cost-aware LLM evaluation.",https://www.semanticscholar.org/paper/ec1b0718bbd4561522f181758d77e205648f5f0f,,Semantic Scholar,8
Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM,"Jingcong Liang, Rong Ye, Meng Han, Ruofei Lai, Xinyu Zhang, Xuanjing Huang, Zhongyu Wei",2024,,https://www.semanticscholar.org/paper/1a28018e4e8e32079723eff5fc8ba6d9f6d4f9fb,10.48550/arXiv.2403.08010,Semantic Scholar,7
LLM-Based Empathetic Response Through Psychologist-Agent Debate,"Yijie Wu, Shi Feng, Ming Wang, Daling Wang, Yifei Zhang",2024,,https://www.semanticscholar.org/paper/7daf37aac345f53aa231c946f81c5378a277d74d,10.1007/978-981-97-7232-2_14,Semantic Scholar,6
A Debate-Driven Experiment on LLM Hallucinations and Accuracy,"Ray Li, Tanishka Bagade, Kevin Martinez, Flora Yasmin, Grant Ayala, Michael Lam, Kevin Zhu",2024,"Large language models (LLMs) have achieved a degree of success in generating coherent and contextually relevant text, yet they remain prone to a significant challenge known as hallucination: producing information that is not substantiated by the input or external knowledge. Previous efforts to mitigate hallucinations have focused on techniques such as fine-tuning models on high-quality datasets, incorporating fact-checking mechanisms, and developing adversarial training methods. While these approaches have shown some promise, they often address the issue at the level of individual model outputs, leaving unexplored the effects of inter-model interactions on hallucination. This study investigates the phenomenon of hallucination in LLMs through a novel experimental framework where multiple instances of GPT-4o-Mini models engage in a debate-like interaction prompted with questions from the TruthfulQA dataset. One model is deliberately instructed to generate plausible but false answers while the other models are asked to respond truthfully. The experiment is designed to assess whether the introduction of misinformation by one model can challenge the truthful majority to better justify their reasoning, improving performance on the TruthfulQA benchmark. The findings suggest that inter-model interactions can offer valuable insights into improving the accuracy and robustness of LLM outputs, complementing existing mitigation strategies.",https://www.semanticscholar.org/paper/50ac06376e107b3327df97de8928b993392c2036,10.48550/arXiv.2410.19485,Semantic Scholar,2
Hierarchical Debate-Based Large Language Model (LLM) for Complex Task Planning of 6G Network Management,"Yuyan Lin, Hao Zhou, Chengming Hu, Xue Liu, Hao Chen, Yan Xin, Jianzhong Zhang",2025,"6G networks have become increasingly complicated due to novel network architecture and newly emerging signal processing and transmission techniques, leading to significant burdens to 6G network management. Large language models (LLMs) have recently been considered a promising technique to equip 6G networks with AI-native intelligence. Different from most existing studies that only consider a single LLM, this work involves a multi-LLM debate-based scheme for 6G network management, where multiple LLMs can collaboratively improve the initial solution sequentially. Considering the complex nature of 6G domain, we propose a novel hierarchical debate scheme: LLMs will first debate the sub-task decomposition, and then debate each subtask step-by-step. Such a hierarchical approach can significantly reduce the overall debate difficulty by sub-task decomposition, aligning well with the complex nature of 6G networks and ensuring the final solution qualities. In addition, to better evaluate the proposed technique, we have defined a novel dataset named 6GPlan, including 110 complex 6G network management tasks and 5000 keyword solutions. Finally, the experiments show that the proposed hierarchical debate can significantly improve performance compared to baseline techniques, e.g. more than 30% coverage rate and global recall rate improvement.",https://www.semanticscholar.org/paper/478f0070b029bd12ced4f325a3ded89d444f7bd1,10.48550/arXiv.2506.06519,Semantic Scholar,2
Multiple LLM Agents Debate for Equitable Cultural Alignment,"Dayeon Ki, Rachel Rudinger, Tianyi Zhou, Marine Carpuat",2025,"Large Language Models (LLMs) need to adapt their predictions to diverse cultural contexts to benefit diverse communities across the world. While previous efforts have focused on single-LLM, single-turn approaches, we propose to exploit the complementary strengths of multiple LLMs to promote cultural adaptability. We introduce a Multi-Agent Debate framework, where two LLM-based agents debate over a cultural scenario and collaboratively reach a final decision. We propose two variants: one where either LLM agents exclusively debate and another where they dynamically choose between self-reflection and debate during their turns. We evaluate these approaches on 7 open-weight LLMs (and 21 LLM combinations) using the NormAd-ETI benchmark for social etiquette norms in 75 countries. Experiments show that debate improves both overall accuracy and cultural group parity over single-LLM baselines. Notably, multi-agent debate enables relatively small LLMs (7-9B) to achieve accuracies comparable to that of a much larger model (27B parameters).",https://www.semanticscholar.org/paper/7a7eed6dcf2587f5a9c507266f0d9ba865246edd,10.48550/arXiv.2505.24671,Semantic Scholar,7
Beyond Single Models: Enhancing LLM Detection of Ambiguity in Requests Through Debate,"Ana Davila, Jacinto Colan, Yasuhisa Hasegawa",2025,"Large Language Models (LLMs) have demonstrated significant capabilities in understanding and generating human language, contributing to more natural interactions with complex systems. However, they face challenges such as ambiguity in user requests processed by LLMs. To address these challenges, this paper introduces and evaluates a multi-agent debate framework designed to enhance detection and resolution capabilities beyond single models. The framework consists of three LLM architectures (Llama3-8B, Gemma2-9B, and Mistral-7B variants) and a dataset with diverse ambiguities. The debate framework markedly enhanced the performance of Llama3-8B and Mistral-7B variants over their individual baselines, with Mistral-7B-led debates achieving a notable 76.7 % success rate and proving particularly effective for complex ambiguities and efficient consensus. While acknowledging varying model responses to collaborative strategies, these findings underscore the debate framework's value as a targeted method for augmenting LLM capabilities. This work offers important insights for developing more robust and adaptive language understanding systems by showing how structured debates can lead to improved clarity in interactive systems.",https://www.semanticscholar.org/paper/6d9b0fbfaeed74c0be25def0895912a8ecb28960,10.23919/SICEFES67750.2025.11236658,Semantic Scholar,2
Breaking Barriers or Building Dependency? Exploring Team-LLM Collaboration in AI-infused Classroom Debate,"Zihan Zhang, Black Sun, Pengcheng An",2025,"Classroom debates are a unique form of collaborative learning characterized by fast-paced, high-intensity interactions that foster critical thinking and teamwork. Despite the recognized importance of debates, the role of AI tools, particularly LLM-based systems, in supporting this dynamic learning environment has been under-explored in HCI. This study addresses this opportunity by investigating the integration of LLM-based AI into real-time classroom debates. Over four weeks, 22 students in a Design History course participated in three rounds of debates with support from ChatGPT. The findings reveal how learners prompted the AI to offer insights, collaboratively processed its outputs, and divided labor in team-AI interactions. The study also surfaces key advantages of AI usage—reducing social anxiety, breaking communication barriers, and providing scaffolding for novices—alongside risks, such as information overload and cognitive dependency, which could limit learners’ autonomy. We thereby discuss a set of nuanced implications for future HCI exploration.",https://www.semanticscholar.org/paper/4269d60eea5ce40151c4ca7a3f7c564b932f1a3b,10.1145/3706598.3713853,Semantic Scholar,12
Efficient Leave-one-out Approximation in LLM Multi-agent Debate Based on Introspection,"Yue Cui, Liuyi Yao, Zitao Li, Yaliang Li, Bolin Ding, Xiaofang Zhou",2025,"Multi-agent systems based on large language models (LLMs) advance automatic task completion in various fields, where debate is a common cooperation form for agents to solve complicated problems with reasoning and cross-review to solidify answers. Assessing the individual contributions of agents within these debates is crucial for system refinement and outcome reliability. Traditional leave-one-out (LOO) method offers a clear framework for evaluating each agent's role but face challenges in LLM-based systems due to high computational costs and associated financial implications. This paper presents introspective-leave-one-out (IntrospecLOO), a simple yet effective prompting for approximation of LOO in LLM-powered multi-agent debates. IntrospecLOO introduces an additional querying round after standard debates, prompting agents to update their answers while ignoring responses from a designated agent. This strategy effectively isolates and gauges each participant's influence at a reduced query complexity compared to the original LOO approaches. Validation through experiments on three benchmark datasets confirms the effectiveness of IntrospecLOO.",https://www.semanticscholar.org/paper/1d0daa4229f8f301e3f82d1fa36ce4610cbcf09e,10.48550/arXiv.2505.22192,Semantic Scholar,1
Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis,"Priyanka Kargupta, Ishika Agarwal, Tal August, Jiawei Han",2025,"With the exponential growth of research facilitated by modern technology and improved accessibility, scientific discoveries have become increasingly fragmented within and across fields. This makes it challenging to assess the significance, novelty, incremental findings, and equivalent ideas between related works, particularly those from different research communities. Large language models (LLMs) have recently demonstrated strong quantitative and qualitative reasoning abilities, and multi-agent LLM debates have shown promise in handling complex reasoning tasks by exploring diverse perspectives and reasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a framework which converts scientific papers into LLM personas that debate their respective novelties. To emphasize structured, critical reasoning rather than focusing solely on outcomes, ToD dynamically constructs a debate tree, enabling fine-grained analysis of independent novelty arguments within scholarly articles. Through experiments on scientific literature across various domains, evaluated by expert researchers, we demonstrate that ToD generates informative arguments, effectively contrasts papers, and supports researchers in their literature review.",http://arxiv.org/abs/2502.14767v2,,arXiv,0
SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution,"Han Li, Yuling Shi, Shaoxin Lin, Xiaodong Gu, Heng Lian, Xin Wang, Yantao Jia, Tao Huang, Qianxiang Wang",2025,"Issue resolution has made remarkable progress thanks to the advanced reasoning capabilities of large language models (LLMs). Recently, agent-based frameworks such as SWE-agent have further advanced this progress by enabling autonomous, tool-using agents to tackle complex software engineering tasks. While existing agent-based issue resolution approaches are primarily based on agents' independent explorations, they often get stuck in local solutions and fail to identify issue patterns that span across different parts of the codebase. To address this limitation, we propose SWE-Debate, a competitive multi-agent debate framework that encourages diverse reasoning paths and achieves more consolidated issue localization. SWE-Debate first creates multiple fault propagation traces as localization proposals by traversing a code dependency graph. Then, it organizes a three-round debate among specialized agents, each embodying distinct reasoning perspectives along the fault propagation trace. This structured competition enables agents to collaboratively converge on a consolidated fix plan. Finally, this consolidated fix plan is integrated into an MCTS-based code modification agent for patch generation. Experiments on the SWE-bench benchmark show that SWE-Debate achieves new state-of-the-art results in open-source agent frameworks and outperforms baselines by a large margin.",http://arxiv.org/abs/2507.23348v1,,arXiv,0
Debate Helps Supervise Unreliable Experts,"Julian Michael, Salsabila Mahdi, David Rein, Jackson Petty, Julien Dirani, Vishakh Padmakumar, Samuel R. Bowman",2023,"As AI systems are used to answer more difficult questions and potentially help create new knowledge, judging the truthfulness of their outputs becomes more difficult and more important. How can we supervise unreliable experts, which have access to the truth but may not accurately report it, to give answers that are systematically true and don't just superficially seem true, when the supervisor can't tell the difference between the two on their own? In this work, we show that debate between two unreliable experts can help a non-expert judge more reliably identify the truth. We collect a dataset of human-written debates on hard reading comprehension questions where the judge has not read the source passage, only ever seeing expert arguments and short quotes selectively revealed by 'expert' debaters who have access to the passage. In our debates, one expert argues for the correct answer, and the other for an incorrect answer. Comparing debate to a baseline we call consultancy, where a single expert argues for only one answer which is correct half of the time, we find that debate performs significantly better, with 84% judge accuracy compared to consultancy's 74%. Debates are also more efficient, being 68% of the length of consultancies. By comparing human to AI debaters, we find evidence that with more skilled (in this case, human) debaters, the performance of debate goes up but the performance of consultancy goes down. Our error analysis also supports this trend, with 46% of errors in human debate attributable to mistakes by the honest debater (which should go away with increased skill); whereas 52% of errors in human consultancy are due to debaters obfuscating the relevant evidence from the judge (which should become worse with increased skill). Overall, these results show that debate is a promising approach for supervising increasingly capable but potentially unreliable AI systems.",http://arxiv.org/abs/2311.08702v1,,arXiv,0
Training Language Models to Win Debates with Self-Play Improves Judge Accuracy,"Samuel Arnesen, David Rein, Julian Michael",2024,"We test the robustness of debate as a method of scalable oversight by training models to debate with data generated via self-play. In a long-context reading comprehension task, we find that language model based evaluators answer questions more accurately when judging models optimized to win debates. By contrast, we find no such relationship for consultancy models trained to persuade a judge without an opposing debater present. In quantitative and qualitative comparisons between our debate models and novel consultancy baselines, we find evidence that debate training encourages stronger and more informative arguments, showing promise that it can help provide high-quality supervision for tasks that are difficult to directly evaluate.",http://arxiv.org/abs/2409.16636v1,,arXiv,0
Ernst Mayr and Carl Sagan debate about the probability of intelligent life in the universe,"Guillermo A. Lemarchand, Ernst Mayr, Carl Sagan",2010,"During the Second Iberoamerican Graduate School on Astrobiology interesting debates, between the experts from the biological and physical backgrounds, arose about the probability of the existence of extraterrestrial intelligent beings in the universe. For this reason, it is appropriate to reproduce -for the first time in Spanish- the debate on the subject conducted, in 1995, between Carl Sagan and Ernst Mayr. This debate was organized by Guillermo A. Lemarchand and published in the pages of two consecutive numbers of ""Bioastronomy News"". Here we reproduce the complete debate, including its original introduction.",http://arxiv.org/abs/1012.2591v1,,arXiv,0
"""You are no Jack Kennedy"": On Media Selection of Highlights from Presidential Debates","Chenhao Tan, Hao Peng, Noah A. Smith",2018,"Political speeches and debates play an important role in shaping the images of politicians, and the public often relies on media outlets to select bits of political communication from a large pool of utterances. It is an important research question to understand what factors impact this selection process.
  To quantitatively explore the selection process, we build a three- decade dataset of presidential debate transcripts and post-debate coverage. We first examine the effect of wording and propose a binary classification framework that controls for both the speaker and the debate situation. We find that crowdworkers can only achieve an accuracy of 60% in this task, indicating that media choices are not entirely obvious. Our classifiers outperform crowdworkers on average, mainly in primary debates. We also compare important factors from crowdworkers' free-form explanations with those from data-driven methods and find interesting differences. Few crowdworkers mentioned that ""context matters"", whereas our data show that well-quoted sentences are more distinct from the previous utterance by the same speaker than less-quoted sentences. Finally, we examine the aggregate effect of media preferences towards different wordings to understand the extent of fragmentation among media outlets. By analyzing a bipartite graph built from quoting behavior in our data, we observe a decreasing trend in bipartisan coverage.",http://arxiv.org/abs/1802.08690v1,10.1145/3178876.3186142,arXiv,0
Out of the Echo Chamber: Detecting Countering Debate Speeches,"Matan Orbach, Yonatan Bilu, Assaf Toledo, Dan Lahav, Michal Jacovi, Ranit Aharonov, Noam Slonim",2020,"An educated and informed consumption of media content has become a challenge in modern times. With the shift from traditional news outlets to social media and similar venues, a major concern is that readers are becoming encapsulated in ""echo chambers"" and may fall prey to fake news and disinformation, lacking easy access to dissenting views. We suggest a novel task aiming to alleviate some of these concerns -- that of detecting articles that most effectively counter the arguments -- and not just the stance -- made in a given text. We study this problem in the context of debate speeches. Given such a speech, we aim to identify, from among a set of speeches on the same topic and with an opposing stance, the ones that directly counter it. We provide a large dataset of 3,685 such speeches (in English), annotated for this relation, which hopefully would be of general interest to the NLP community. We explore several algorithms addressing this task, and while some are successful, all fall short of expert human performance, suggesting room for further research. All data collected during this work is freely available for research.",http://arxiv.org/abs/2005.01157v1,,arXiv,0
Debatable Intelligence: Benchmarking LLM Judges via Debate Speech Evaluation,"Noy Sternlicht, Ariel Gera, Roy Bar-Haim, Tom Hope, Noam Slonim",2025,"We introduce Debate Speech Evaluation as a novel and challenging benchmark for assessing LLM judges. Evaluating debate speeches requires a deep understanding of the speech at multiple levels, including argument strength and relevance, the coherence and organization of the speech, the appropriateness of its style and tone, and so on. This task involves a unique set of cognitive abilities that previously received limited attention in systematic LLM benchmarking. To explore such skills, we leverage a dataset of over 600 meticulously annotated debate speeches and present the first in-depth analysis of how state-of-the-art LLMs compare to human judges on this task. Our findings reveal a nuanced picture: while larger models can approximate individual human judgments in some respects, they differ substantially in their overall judgment behavior. We also investigate the ability of frontier LLMs to generate persuasive, opinionated speeches, showing that models may perform at a human level on this task.",http://arxiv.org/abs/2506.05062v2,,arXiv,0
Latent Debate: A Surrogate Framework for Interpreting LLM Thinking,"Lihu Chen, Xiang Yin, Francesca Toni",2025,"Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference. We first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks. Empirical studies demonstrate that latent debate is a faithful structured surrogate model that has highly consistent predictions with the original LLM. Beyond interpretability, we demonstrate that latent debate provides a strong baseline for hallucination detection. Further analysis reveals strong correlations between hallucinations and debate patterns, such as a high degree of latent debates in the middle layers is linked to a higher risk of hallucinations. These findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.",http://arxiv.org/abs/2512.01909v1,,arXiv,0
Can LLM Agents Really Debate? A Controlled Study of Multi-Agent Debate in Logical Reasoning,"Haolun Wu, Zhenkun Li, Lingyao Li",2025,"Multi-agent debate (MAD) has recently emerged as a promising framework for improving the reasoning performance of large language models (LLMs). Yet, whether LLM agents can genuinely engage in deliberative reasoning, beyond simple ensembling or majority voting, remains unclear. We address this question through a controlled study using the Knight--Knave--Spy logic puzzle, which enables precise, step-wise evaluation of debate outcomes and processes under verifiable ground truth. We systematically set up six structural and cognitive factors, including agent team size, composition, confidence visibility, debate order, debate depth, and task difficulty, to disentangle their respective effects on collective reasoning. Our results show that intrinsic reasoning strength and group diversity are the dominant drivers of debate success, while structural parameters such as order or confidence visibility offer limited gains. Beyond outcomes, process-level analyses identify key behavioral patterns: majority pressure suppresses independent correction, effective teams overturn incorrect consensus, and rational, validity-aligned reasoning most strongly predicts improvement. These findings provide valuable insights into how and why LLM debates succeed or fail, offering guidance for designing interpretable and truth-seeking multi-agent reasoning systems.",http://arxiv.org/abs/2511.07784v1,,arXiv,0
RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit,"Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng Dou, Ji-Rong Wen",2023,"Although Large Language Models (LLMs) have demonstrated extraordinary capabilities in many domains, they still have a tendency to hallucinate and generate fictitious responses to user requests. This problem can be alleviated by augmenting LLMs with information retrieval (IR) systems (also known as retrieval-augmented LLMs). Applying this strategy, LLMs can generate more factual texts in response to user input according to the relevant content retrieved by IR systems from external corpora as references. In addition, by incorporating external knowledge, retrieval-augmented LLMs can answer in-domain questions that cannot be answered by solely relying on the world knowledge stored in parameters. To support research in this area and facilitate the development of retrieval-augmented LLM systems, we develop RETA-LLM, a {RET}reival-{A}ugmented LLM toolkit. In RETA-LLM, we create a complete pipeline to help researchers and users build their customized in-domain LLM-based systems. Compared with previous retrieval-augmented LLM systems, RETA-LLM provides more plug-and-play modules to support better interaction between IR systems and LLMs, including {request rewriting, document retrieval, passage extraction, answer generation, and fact checking} modules. Our toolkit is publicly available at https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM.",http://arxiv.org/abs/2306.05212v1,,arXiv,0
Revealing Political Bias in LLMs through Structured Multi-Agent Debate,"Aishwarya Bandaru, Fabian Bindley, Trevor Bluth, Nandini Chavda, Baixu Chen, Ethan Law",2025,"Large language models (LLMs) are increasingly used to simulate social behaviour, yet their political biases and interaction dynamics in debates remain underexplored. We investigate how LLM type and agent gender attributes influence political bias using a structured multi-agent debate framework, by engaging Neutral, Republican, and Democrat American LLM agents in debates on politically sensitive topics. We systematically vary the underlying LLMs, agent genders, and debate formats to examine how model provenance and agent personas influence political bias and attitudes throughout debates. We find that Neutral agents consistently align with Democrats, while Republicans shift closer to the Neutral; gender influences agent attitudes, with agents adapting their opinions when aware of other agents' genders; and contrary to prior research, agents with shared political affiliations can form echo chambers, exhibiting the expected intensification of attitudes as debates progress.",http://arxiv.org/abs/2506.11825v1,,arXiv,0
FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation,"Liqun Ma, Mingjie Sun, Zhiqiang Shen",2024,"This work presents a Fully BInarized Large Language Model (FBI-LLM), demonstrating for the first time how to train a large-scale binary language model from scratch (not the partial binary or ternary LLM like BitNet b1.58) to match the performance of its full-precision counterparts (e.g., FP16 or BF16) in transformer-based LLMs. It achieves this by employing an autoregressive distillation (AD) loss with maintaining equivalent model dimensions (130M, 1.3B, 7B) and training data volume as regular LLM pretraining, while delivering competitive results in terms of perplexity and task-specific effectiveness. Intriguingly, by analyzing the training trajectory, we find that the pretrained weight is not necessary for training binarized LLMs from scratch. This research encourages a new computational framework and may facilitate the future design of specialized hardware tailored for fully 1-bit LLMs. We make all models, code, and training dataset fully accessible and transparent to support further research (Code: https://github.com/LiqunMa/FBI-LLM. Model: https://huggingface.co/LiqunMa/).",http://arxiv.org/abs/2407.07093v1,,arXiv,0
Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates,"Hui Wei, Shenghua He, Tian Xia, Fei Liu, Andy Wong, Jingyang Lin, Mei Han",2024,"LLM-as-a-Judge has been widely applied to evaluate and compare different LLM alignmnet approaches (e.g., RLHF and DPO). However, concerns regarding its reliability have emerged, due to LLM judges' biases and inconsistent decision-making. Previous research has developed evaluation frameworks to assess reliability of LLM judges and their alignment with human preferences. However, the employed evaluation metrics often lack adequate explainability and fail to address LLM internal inconsistency. Additionally, existing studies inadequately explore the impact of various prompt templates when applying LLM-as-a-Judge methods, leading to potentially inconsistent comparisons between different alignment algorithms. In this work, we systematically evaluate LLM-as-a-Judge on alignment tasks by defining more theoretically interpretable evaluation metrics and explicitly mitigating LLM internal inconsistency from reliability metrics. We develop an open-source framework to evaluate, compare, and visualize the reliability and alignment of LLM judges, which facilitates practitioners to choose LLM judges for alignment tasks. In the experiments, we examine effects of diverse prompt templates on LLM-judge reliability and also demonstrate our developed framework by comparing various LLM judges on two common alignment datasets (i.e., TL;DR Summarization and HH-RLHF-Helpfulness). Our results indicate a significant impact of prompt templates on LLM judge performance, as well as a mediocre alignment level between the tested LLM judges and human evaluators.",http://arxiv.org/abs/2408.13006v2,,arXiv,0
Demystifying AI Platform Design for Distributed Inference of Next-Generation LLM models,"Abhimanyu Bambhaniya, Ritik Raj, Geonhwa Jeong, Souvik Kundu, Sudarshan Srinivasan, Suvinay Subramanian, Midhilesh Elavazhagan, Madhu Kumar, Tushar Krishna",2024,"Large language models (LLMs) have shown remarkable performance across a wide range of applications, often outperforming human experts. However, deploying these gigantic models efficiently for diverse inference use cases requires carefully designed hardware platforms with ample computing, memory, and network resources. With constant innovation in LLM serving optimizations and model architecture evolving at breakneck speed, the hardware requirements to meet Service Level Objectives (SLOs) remain an open research question.
  To answer the question, we present an analytical tool, GenZ, to efficiently navigate the relationship between diverse LLM model architectures(Dense, GQA, MoE, Mamba), LLM serving optimizations(Chunking, Speculative decoding, quanitization), and AI platform design parameters. Our tool estimates LLM inference performance metrics for the given scenario. We have validated against real hardware platforms running various different LLM models, achieving a max geomean error of 5.82.We use GenZ to identify compute, memory capacity, memory bandwidth, network latency, and network bandwidth requirements across diverse LLM inference use cases. We also study diverse architectural choices in use today (inspired by LLM serving platforms from several vendors) to help inform computer architects designing next-generation AI hardware accelerators and platforms. The trends and insights derived from GenZ can guide AI engineers deploying LLMs as well as computer architects designing next-generation hardware accelerators and platforms. Ultimately, this work sheds light on the platform design considerations for unlocking the full potential of large language models across a spectrum of applications. The source code is available at https://github.com/abhibambhaniya/GenZ-LLM-Analyzer . Users can also be tried it on at https://genz-llm-analyzer.streamlit.app/ without any setup on your web browser.",http://arxiv.org/abs/2406.01698v3,,arXiv,0
"DEBATE: A Large-Scale Benchmark for Role-Playing LLM Agents in Multi-Agent, Long-Form Debates","Yun-Shiuan Chuang, Ruixuan Tu, Chengtao Dai, Smit Vasani, Binwei Yao, Michael Henry Tessler, Sijia Yang, Dhavan Shah, Robert Hawkins, Junjie Hu, Timothy T. Rogers",2025,"Accurately modeling opinion change through social interactions is crucial for addressing issues like misinformation and polarization. While role-playing large language models (LLMs) offer a promising way to simulate human-like interactions, existing research shows that single-agent alignment does not guarantee authentic multi-agent group dynamics. Current LLM role-play setups often produce unnatural dynamics (e.g., premature convergence), without an empirical benchmark to measure authentic human opinion trajectories. To bridge this gap, we introduce DEBATE, the first large-scale empirical benchmark explicitly designed to evaluate the authenticity of the interaction between multi-agent role-playing LLMs. DEBATE contains 29,417 messages from multi-round debate conversations among over 2,792 U.S.-based participants discussing 107 controversial topics, capturing both publicly-expressed messages and privately-reported opinions. Using DEBATE, we systematically evaluate and identify critical discrepancies between simulated and authentic group dynamics. We further demonstrate DEBATE's utility for aligning LLMs with human behavior through supervised fine-tuning, achieving improvements in surface-level metrics (e.g., ROUGE-L and message length) while highlighting limitations in deeper semantic alignment (e.g., semantic similarity). Our findings highlight both the potential and current limitations of role-playing LLM agents for realistically simulating human-like social dynamics.",http://arxiv.org/abs/2510.25110v1,,arXiv,0
Debate Only When Necessary: Adaptive Multiagent Collaboration for Efficient LLM Reasoning,"Sugyeong Eo, Hyeonseok Moon, Evelyn Hayoon Zi, Chanjun Park, Heuiseok Lim",2025,"Multiagent collaboration has emerged as a promising framework for enhancing the reasoning capabilities of large language models (LLMs). Despite improvements in reasoning, the approach introduces substantial computational overhead resulting from iterative agent interactions. Furthermore, engaging in unnecessary debates increases the risk of generating erroneous responses. To address these challenges, we propose Debate Only When Necessary (DOWN), an adaptive multiagent debate framework that selectively activates debate based on the confidence score of the agent's initial response. Debate is activated only for queries requiring further deliberation, during which agents refine their outputs by referencing peer responses and associated confidence scores. Evaluations on benchmarks show that DOWN improves efficiency by up to six times while preserving or even outperforming the performance of existing methods. Further analysis indicates that DOWN effectively mitigates the risk of error propagation stemming from the unnecessary debate process. These findings demonstrate the effectiveness of our approach in delivering high-performance LLM solutions at a lower computational cost.",http://arxiv.org/abs/2504.05047v2,,arXiv,0
Adaptive Self-improvement LLM Agentic System for ML Library Development,"Genghan Zhang, Weixin Liang, Olivia Hsu, K. Olukotun",2025,"ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to $3.9\times$ over a baseline single LLM.",https://www.semanticscholar.org/paper/e0a5df1b92981d8ae046e21e053ab1593fcc67f3,10.48550/arXiv.2502.02534,Semantic Scholar,8
A Survey on LLM Inference-Time Self-Improvement,"Xiangjue Dong, Maria Teleki, James Caverlee",2024,"Techniques that enhance inference through increased computation at test-time have recently gained attention. In this survey, we investigate the current state of LLM Inference-Time Self-Improvement from three different perspectives: Independent Self-improvement, focusing on enhancements via decoding or sampling methods; Context-Aware Self-Improvement, leveraging additional context or datastore; and Model-Aided Self-Improvement, achieving improvement through model collaboration. We provide a comprehensive review of recent relevant studies, contribute an in-depth taxonomy, and discuss challenges and limitations, offering insights for future research.",https://www.semanticscholar.org/paper/a9d1356dad110f4ad04d223c9fee1eb6fd2e3455,10.48550/arXiv.2412.14352,Semantic Scholar,14
Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search,"Jonathan Light, Min Cai, Weiqin Chen, Guanzhi Wang, Xiusi Chen, Wei Cheng, Yisong Yue, Ziniu Hu",2024,"Traditional reinforcement learning and planning typically requires vast amounts of data and training to develop effective policies. In contrast, large language models (LLMs) exhibit strong generalization and zero-shot capabilities, but struggle with tasks that require detailed planning and decision-making in complex action spaces. We introduce STRATEGIST, a novel approach that integrates the strengths of both methods. Our approach leverages LLMs to search and update high-level strategies (as text), which are then refined and executed by low-level Monte Carlo Tree Search (MCTS). STRATEGIST is a generalizable framework to optimize the strategy through population-based self-play simulations without the need for any training data. We demonstrate the effectiveness of STRATEGIST in learning optimal strategies for competitive, multi-turn games with partial information, including Game of Pure Strategy (GOPS) and multi-agent, hidden-identity discussion games like The Resistance: Avalon. Our results show that agents equipped with STRATEGIST outperform those trained with traditional RL methods, other LLM-based skill acquisition techniques, pre-existing LLM agents across both game environments and achieves comparable performance against human players.",https://www.semanticscholar.org/paper/9275fa808485d97b8bff8fbd8ad0501d93622bbd,,Semantic Scholar,13
Mitigating Tail Narrowing in LLM Self-Improvement via Socratic-Guided Sampling,"Yiwen Ding, Zhiheng Xi, Wei He, Zhuoyuan Li, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, Xuanjing Huang",2024,"Self-improvement methods enable large language models (LLMs) to generate solutions themselves and iteratively train on filtered, high-quality rationales. This process proves effective and reduces the reliance on human supervision in LLMs' reasoning, but the performance soon plateaus. We delve into the process and find that models tend to over-sample on easy queries and under-sample on queries they have yet to master. As iterations proceed, this imbalance in sampling is exacerbated, leading to a long-tail distribution where solutions to difficult queries almost diminish. This phenomenon limits the performance gain of self-improving models. A straightforward solution is brute-force sampling to balance the distribution, which significantly raises computational costs. In this paper, we introduce Guided Self-Improvement (GSI), a strategy aimed at improving the efficiency of sampling challenging heavy-tailed data. It leverages Socratic-style guidance signals to help LLM reasoning with complex queries, reducing the exploration effort and minimizing computational overhead. Experiments on four models across diverse mathematical tasks show that GSI strikes a balance between performance and efficiency, while also being effective on held-out tasks.",https://www.semanticscholar.org/paper/e409ce659b1aaa70d6c6ea2c4891e50322138b7a,10.48550/arXiv.2411.00750,Semantic Scholar,12
ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent,"Renat Aksitov, Sobhan Miryoosefi, Zong-xiao Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, M. Zaheer, Felix X. Yu, Sanjiv Kumar",2023,"Answering complex natural language questions often necessitates multi-step reasoning and integrating external information. Several systems have combined knowledge retrieval with a large language model (LLM) to answer such questions. These systems, however, suffer from various failure cases, and we cannot directly train them end-to-end to fix such failures, as interaction with external knowledge is non-differentiable. To address these deficiencies, we define a ReAct-style LLM agent with the ability to reason and act upon external knowledge. We further refine the agent through a ReST-like method that iteratively trains on previous trajectories, employing growing-batch reinforcement learning with AI feedback for continuous self-improvement and self-distillation. Starting from a prompted large model and after just two iterations of the algorithm, we can produce a fine-tuned small model that achieves comparable performance on challenging compositional question-answering benchmarks with two orders of magnitude fewer parameters.",https://www.semanticscholar.org/paper/e35426fd81c78b044258cf419be6b7e5093b71c0,10.48550/arXiv.2312.10003,Semantic Scholar,71
Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge,"Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason E. Weston, Sainbayar Sukhbaatar",2024,"Large Language Models (LLMs) are rapidly surpassing human knowledge in many domains. While improving these models traditionally relies on costly human data, recent self-rewarding mechanisms (Yuan et al., 2024) have shown that LLMs can improve by judging their own responses instead of relying on human labelers. However, existing methods have primarily focused on improving model responses rather than judgment capabilities, resulting in rapid saturation during iterative training. To address this issue, we introduce a novel Meta-Rewarding step to the self-improvement process, where the model judges its own judgements and uses that feedback to refine its judgment skills. Surprisingly, this unsupervised approach improves the model's ability to judge {\em and} follow instructions, as demonstrated by a win rate improvement of Llama-3-8B-Instruct from 22.9% to 39.4% on AlpacaEval 2, and 20.6% to 29.1% on Arena-Hard. These results strongly suggest the potential for self-improving models without human supervision.",https://www.semanticscholar.org/paper/df90ee11ed6378635f22e6d0061cf67dd0bacd13,10.48550/arXiv.2407.19594,Semantic Scholar,147
Towards Self-Improvement of LLMs via MCTS: Leveraging Stepwise Knowledge with Curriculum Preference Learning,"Xiyao Wang, Linfeng Song, Ye Tian, Dian Yu, Baolin Peng, Haitao Mi, Furong Huang, Dong Yu",2024,"Monte Carlo Tree Search (MCTS) has recently emerged as a powerful technique for enhancing the reasoning capabilities of LLMs. Techniques such as SFT or DPO have enabled LLMs to distill high-quality behaviors from MCTS, improving their reasoning performance. However, existing distillation methods underutilize the rich trajectory information generated by MCTS, limiting the potential for improvements in LLM reasoning. In this paper, we propose AlphaLLM-CPL, a novel pairwise training framework that enables LLMs to self-improve through MCTS behavior distillation. AlphaLLM-CPL efficiently leverages MCTS trajectories via two key innovations: (1) AlphaLLM-CPL constructs stepwise trajectory pairs from child nodes sharing the same parent in the search tree, providing step-level information for more effective MCTS behavior distillation. (2) AlphaLLM-CPL introduces curriculum preference learning, dynamically adjusting the training sequence of trajectory pairs in each offline training epoch to prioritize critical learning steps and mitigate overfitting. Experimental results on mathematical reasoning tasks demonstrate that AlphaLLM-CPL significantly outperforms previous MCTS behavior distillation methods, substantially boosting the reasoning capabilities of LLMs.",https://www.semanticscholar.org/paper/4f98157c298b87408646672b812801f3d6618c76,10.48550/arXiv.2410.06508,Semantic Scholar,22
ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement,"Xiangyu Peng, Congying Xia, Xinyi Yang, Caiming Xiong, Chien-Sheng Wu, Chen Xing",2024,"Post-training Large Language Models (LLMs) with explicit reasoning trajectories can enhance their reasoning abilities. However, acquiring such high-quality trajectory data typically demands meticulous supervision from humans or superior models, which can be either expensive or license-constrained. In this paper, we explore how far an LLM can improve its reasoning by self-synthesizing reasoning paths as training data without any additional supervision. Existing self-synthesizing methods, such as STaR, suffer from poor generalization to out-of-domain (OOD) reasoning tasks. We hypothesize it is due to that their self-synthesized reasoning paths are too task-specific, lacking general task-agnostic reasoning guidance. To address this, we propose Reasoning Generalist via Self-Improvement (ReGenesis), a method to self-synthesize reasoning paths as post-training data by progressing from abstract to concrete. More specifically, ReGenesis self-synthesizes reasoning paths by converting general reasoning guidelines into task-specific ones, generating reasoning structures, and subsequently transforming these structures into reasoning paths, without the need for human-designed task-specific examples used in existing methods. We show that ReGenesis achieves superior performance on all in-domain and OOD settings tested compared to existing methods. For six OOD tasks specifically, while previous methods exhibited an average performance decrease of approximately 4.6% after post training, ReGenesis delivers around 6.1% performance improvement. We also conduct in-depth analysis of our framework and show ReGenesis is effective across various LLMs and design choices.",https://www.semanticscholar.org/paper/45c70f76c60b582ef8a6432f19a5095643ebe902,10.48550/arXiv.2410.02108,Semantic Scholar,13
Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement,"Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, W. Wang",2024,"Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others. We discovered that such a contrary is due to LLM's bias in evaluating their own output. In this paper, we formally define LLM's self-bias - the tendency to favor its own generation - using two statistics. We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks. The code and data are released at https://github.com/xu1998hz/llm_self_bias.",https://www.semanticscholar.org/paper/00e6500616920a25ecd95d0d3ad6f7764266b31b,10.18653/v1/2024.acl-long.826,Semantic Scholar,88
Language Model Self-improvement by Reinforcement Learning Contemplation,"Jing-Cheng Pang, Pengyuan Wang, Kaiyuan Li, Xiong-Hui Chen, Jiacheng Xu, Zongzhang Zhang, Yang Yu",2023,"Large Language Models (LLMs) have exhibited remarkable performance across various natural language processing (NLP) tasks. However, fine-tuning these models often necessitates substantial supervision, which can be expensive and time-consuming to obtain. This paper introduces a novel unsupervised method called LanguageModel Self-Improvement by Reinforcement Learning Contemplation (SIRLC) that improves LLMs without reliance on external labels. Our approach is grounded in the observation that it is simpler for language models to assess text quality than to generate text. Building on this insight, SIRLC assigns LLMs dual roles as both student and teacher. As a student, the LLM generates answers to unlabeled questions, while as a teacher, it evaluates the generated text and assigns scores accordingly. The model parameters are updated using reinforcement learning to maximize the evaluation score. We demonstrate that SIRLC can be applied to various NLP tasks, such as reasoning problems, text generation, and machine translation. Our experiments show that SIRLC effectively improves LLM performance without external supervision, resulting in a 5.6% increase in answering accuracy for reasoning tasks and a rise in BERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be applied to models of different sizes, showcasing its broad applicability.",https://www.semanticscholar.org/paper/c226a4acb42912054d498bcf771023b0ba2da001,10.48550/arXiv.2305.14483,Semantic Scholar,72
Enhancing Item Tokenization for Generative Recommendation through Self-Improvement,"Runjin Chen, Mingxuan Ju, Ngoc Bui, Dimosthenis Antypas, Stanley Cai, Xiaopeng Wu, Leonardo Neves, Zhangyang Wang, Neil Shah, Tong Zhao",2024,"Generative recommendation systems, driven by large language models (LLMs), present an innovative approach to predicting user preferences by modeling items as token sequences and generating recommendations in a generative manner. A critical challenge in this approach is the effective tokenization of items, ensuring that they are represented in a form compatible with LLMs. Current item tokenization methods include using text descriptions, numerical strings, or sequences of discrete tokens. While text-based representations integrate seamlessly with LLM tokenization, they are often too lengthy, leading to inefficiencies and complicating accurate generation. Numerical strings, while concise, lack semantic depth and fail to capture meaningful item relationships. Tokenizing items as sequences of newly defined tokens has gained traction, but it often requires external models or algorithms for token assignment. These external processes may not align with the LLM's internal pretrained tokenization schema, leading to inconsistencies and reduced model performance. To address these limitations, we propose a self-improving item tokenization method that allows the LLM to refine its own item tokenizations during training process. Our approach starts with item tokenizations generated by any external model and periodically adjusts these tokenizations based on the LLM's learned patterns. Such alignment process ensures consistency between the tokenization and the LLM's internal understanding of the items, leading to more accurate recommendations. Furthermore, our method is simple to implement and can be integrated as a plug-and-play enhancement into existing generative recommendation systems. Experimental results on multiple datasets and using various initial tokenization strategies demonstrate the effectiveness of our method, with an average improvement of 8\% in recommendation performance.",https://www.semanticscholar.org/paper/c6aa8831bf287d7f369ed475099443e5eeef349f,10.48550/arXiv.2412.17171,Semantic Scholar,10
"Evaluating Large Language Models on Business Process Modeling: Framework, Benchmark, and Self-Improvement Analysis","Humam Kourani, Alessandro Berti, Daniel Schuster, W. M. Aalst",2024,"Large Language Models (LLMs) are rapidly transforming various fields, and their potential in Business Process Management (BPM) is substantial. This paper assesses the capabilities of LLMs on business process modeling using a framework for automating this task, a comprehensive benchmark, and an analysis of LLM self-improvement strategies. We present a comprehensive evaluation of 16 state-of-the-art LLMs from major AI vendors using a custom-designed benchmark of 20 diverse business processes. Our analysis highlights significant performance variations across LLMs and reveals a positive correlation between efficient error handling and the quality of generated models. It also shows consistent performance trends within similar LLM groups. Furthermore, we investigate LLM self-improvement techniques, encompassing self-evaluation, input optimization, and output optimization. Our findings indicate that output optimization, in particular, offers promising potential for enhancing quality, especially in models with initially lower performance. Our contributions provide insights for leveraging LLMs in BPM, paving the way for more advanced and automated process modeling techniques.",https://www.semanticscholar.org/paper/19489636e75568571667a25dafc7d2fbf463c3dd,10.48550/arXiv.2412.00023,Semantic Scholar,9
PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation,"Qiyao Xue, Xiangyu Yin, Boyuan Yang, Wei Gao",2024,"Text-to-video (T2V) generation has been recently enabled by transformer-based diffusion models, but current T2V models lack capabilities in adhering to the real-world common knowledge and physical rules, due to their limited understanding of physical realism and deficiency in temporal modeling. Existing solutions are either data-driven or require extra model inputs, but cannot be generalizable to out-of-distribution domains. In this paper, we present PhyT2V, a new data-independent T2V technique that expands the current T2V model’s capability of video generation to out-of-distribution domains, by enabling chain-of-thought and step-back reasoning in T2V prompting. Our experiments show that PhyT2V improves existing T2V models’ adherence to real-world physical rules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers.",https://www.semanticscholar.org/paper/015b1f127b6c31654e3597b75876eed8e445d866,10.1109/CVPR52734.2025.01754,Semantic Scholar,36
I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm,"Yiming Liang, Ge Zhang, Xingwei Qu, Tianyu Zheng, Jiawei Guo, Xinrun Du, Zhenzhu Yang, Jiaheng Liu, Chenghua Lin, Lei Ma, Wenhao Huang, Jiajun Zhang",2024,"Large Language Models (LLMs) have achieved significant advancements, however, the common learning paradigm treats LLMs as passive information repositories, neglecting their potential for active learning and alignment. Some approaches train LLMs using their own generated synthetic data, exploring the possibility of active alignment. However, there is still a huge gap between these one-time alignment methods and the continuous automatic alignment of humans. In this paper, we introduce \textbf{I-SHEEP}, an \textbf{I}terative \textbf{S}elf-En\textbf{H}anc\textbf{E}m\textbf{E}nt \textbf{P}aradigm.This human-like paradigm enables LLMs to \textbf{continuously self-align from scratch with nothing}. Compared to the one-time alignment method Dromedary \cite{sun2023principledriven}, which refers to the first iteration in this paper, I-SHEEP can significantly enhance capacities on both Qwen and Llama models. I-SHEEP achieves a maximum relative improvement of 78.2\% in the Alpaca Eval, 24.0\% in the MT Bench, and an absolute increase of 8.88\% in the IFEval accuracy over subsequent iterations in Qwen-1.5 72B model. Additionally, I-SHEEP surpasses the base model in various standard benchmark generation tasks, achieving an average improvement of 24.77\% in code generation tasks, 12.04\% in TrivialQA, and 20.29\% in SQuAD. We also provide new insights based on the experiment results. Our codes, datasets, and models are available at \textbf{https://anonymous.4open.science/r/I-SHEEP}.",https://www.semanticscholar.org/paper/13065291b871cade87cd3a0a793729cbc57e69ec,10.48550/arXiv.2408.08072,Semantic Scholar,19
Semantic Voting: A Self-Evaluation-Free Approach for Efficient LLM Self-Improvement on Unverifiable Open-ended Tasks,"Chunyang Jiang, Yonggang Zhang, Yiyang Cai, Chi-Min Chan, Yulong Liu, Mingming Chen, Wei Xue, Yi-Ting Guo",2025,"The rising cost of acquiring supervised data has driven significant interest in self-improvement for large language models (LLMs). Straightforward unsupervised signals like majority voting have proven effective in generating pseudo-labels for verifiable tasks, while their applicability to unverifiable tasks (e.g., translation) is limited by the open-ended character of responses. As a result, self-evaluation mechanisms (e.g., self-judging and entropy minimization) are predominantly used to derive pseudo-labels. However, self-evaluation relying on LLMs typically incurs high computational overhead and introduces overconfidence issues due to intrinsic biases. To address these challenges, we propose a novel self-evaluation-free approach for unverifiable tasks, designed for lightweight yet effective self-improvement. Inspired by majority voting commonly employed in verifiable tasks, we propose semantic voting as a novel mechanism that relaxes the principle of hard matching (i.e., exact matching) toward soft matching (i.e., semantic similarity). Soft matching is achieved by leveraging a lightweight sentence embedding model to quantify semantic similarity, thereby mitigating excessive computational burden and intrinsic bias-associated limitations of self-evaluation. Comprehensive experiments demonstrate that our method achieves substantial gains in computational efficiency and overall better performance than self-evaluation methods across diverse model architectures and tasks.",https://www.semanticscholar.org/paper/5becda4f5dd8eca7c2f9990abcd4b1b2543ead2b,10.48550/arXiv.2509.23067,Semantic Scholar,1
Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap,"Yifan Sun, Yushan Liang, Zhen Zhang, Jiaye Teng",2025,"Self-improvement is among the most prominent techniques within the realm of large language models (LLM), aiming to enhance the LLM performance without relying on external data. Despite its significance, generally how LLM performances evolve during the self-improvement process remains underexplored. In this paper, we theoretically model the training dynamics of self-improvement via the concept of solver-verifier gap. This is inspired by the conjecture that the performance enhancement of self-improvement stems from the gap between LLM's solver capability and verifier capability. Based on the theoretical framework, we further show how to model the entire training trajectory. This framework allows quantifying the capability limit of self-improvement by fitting the theoretical model to the experiment results. We empirically validate the effectiveness of the theoretical framework on various LLMs and datasets. Beyond self-improvement, we extend our analysis to investigate how external data influences these dynamics within the framework. Notably, we find that under limited external data regimes, such external data can be utilized at any stage without significantly affecting final performances, which accords with the empirical observations.",https://www.semanticscholar.org/paper/f1de96cd87a05e95a8b56f1cb16d26926ac718aa,10.48550/arXiv.2507.00075,Semantic Scholar,0
Dynamic Noise Preference Optimization for LLM Self-Improvement via Synthetic Data,"Haoyan Yang, Ting Hua, Shangqian Gao, Binfeng Xu, Zheng Tang, Jie Xu, Hongxia Jin, Vijay Srinivasan",2025,"Although LLMs have achieved significant success, their reliance on large volumes of human-annotated data has limited their potential for further scaling. In this situation, utilizing self-generated synthetic data has become crucial for fine-tuning LLMs without extensive human annotation. However, current methods often fail to ensure consistent improvements across iterations, with performance stagnating after only minimal updates. To overcome these challenges, we introduce Dynamic Noise Preference Optimization (DNPO). DNPO employs a dynamic sample labeling mechanism to construct preference pairs for training and introduces controlled, trainable noise into the preference optimization process. Our approach effectively prevents stagnation and enables continuous improvement. In experiments with Zephyr-7B, DNPO consistently outperforms existing methods, showing an average performance boost of 2.6% across multiple benchmarks. Additionally, DNPO shows a significant improvement in model-generated data quality, with a 29.4% win-loss rate gap compared to the baseline in GPT-4 evaluations. This highlights its effectiveness in enhancing model performance through iterative refinement.",https://www.semanticscholar.org/paper/0284a1a4f7001c03902448b43b1c2d9e6dd02376,10.48550/arXiv.2502.05400,Semantic Scholar,2
Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations,"Álvaro Martín-Cortinas, Daniel Sáez-Trigueros, Iv'an Vall'es-P'erez, Biel Tura Vecino, Piotr Bilinski, Mateusz Lajszczak, Grzegorz Beringer, R. Barra-Chicote, Jaime Lorenzo-Trueba",2024,"Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity over SOTA entangled representations, and a word error rate (WER) 5.4pp lower. Furthermore, they achieve higher naturalness than human recordings of the LibriTTS test-other dataset. Finally, we show that using explicit reference embedding negatively impacts intelligibility (stability), with WER increasing by 14pp compared to the model that only uses text to infer the style.",https://www.semanticscholar.org/paper/527d11bae71c4a91f2e66637476e991f4a1d309b,10.48550/arXiv.2402.03407,Semantic Scholar,8
Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller,"Min Cai, Yuchen Zhang, Shichang Zhang, Fan Yin, Dan Zhang, Difan Zou, Yisong Yue, Ziniu Hu",2024,"We propose SelfControl, an inference-time model control method utilizing gradients to control the behavior of large language models (LLMs) without explicit human annotations. Given a desired behavior expressed in a natural language suffix string concatenated to the input prompt, SelfControl computes gradients of the LLM's self-evaluation of the suffix with respect to its latent representations. The gradients are used to directly control the auto-regressive generation process towards desired behaviors, which eliminates human supervision, achieves precise and transparent control, and offers on-the-fly adaptability. To further enhance efficiency, we introduce SelfControl_{Prefix}, a compact module that encapsulates the learned representations from gradients into a SelfControl_{Prefix}, facilitating efficient inference-time control with no latency compared to the original model and allowing control for multiple behaviors simultaneously. Our experiments demonstrate SelfControl's efficacy across multiple domains, where it improves over SOTA for 8.3% in detoxification, 3.1% in truthfulness enhancement, 4%~10% in controlling on emotion tones, and 48.2% in privacy protection, i.e., completely remove privacy leakage issue. Additionally, we demonstrate that SelfControl can be used for data synthesis and to improve reasoning abilities.",http://arxiv.org/abs/2406.02721v3,,arXiv,0
Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence Estimation for Failure Detection,"Vaibhav Mavi, Shubh Jaroria, Weiqi Sun",2025,"Reliability and failure detection of large language models (LLMs) is critical for their deployment in high-stakes, multi-step reasoning tasks. Prior work explores confidence estimation for self-evaluating LLM-scorer systems, with confidence scorers estimating the likelihood of errors in LLM responses. However, most methods focus on single-step outputs and overlook the challenges of multi-step reasoning. In this work, we extend self-evaluation techniques to multi-step tasks, testing two intuitive approaches: holistic scoring and step-by-step scoring. Using two multi-step benchmark datasets, we show that stepwise evaluation generally outperforms holistic scoring in detecting potential errors, with up to 15% relative increase in AUC-ROC. Our findings demonstrate that self-evaluating LLM systems provide meaningful confidence estimates in complex reasoning, improving their trustworthiness and providing a practical framework for failure detection.",http://arxiv.org/abs/2511.07364v1,,arXiv,0
"Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing","Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, Dong Yu",2024,"Despite the impressive capabilities of Large Language Models (LLMs) on various tasks, they still struggle with scenarios that involves complex reasoning and planning. Recent work proposed advanced prompting techniques and the necessity of fine-tuning with high-quality data to augment LLMs' reasoning abilities. However, these approaches are inherently constrained by data availability and quality. In light of this, self-correction and self-learning emerge as viable solutions, employing strategies that allow LLMs to refine their outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs in self-refining its response, particularly in complex reasoning and planning task, remains dubious. In this paper, we introduce AlphaLLM for the self-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with LLMs to establish a self-improving loop, thereby enhancing the capabilities of LLMs without additional annotations. Drawing inspiration from the success of AlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM for self-improvement, including data scarcity, the vastness search spaces of language tasks, and the subjective nature of feedback in language tasks. AlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach tailored for language tasks, and a trio of critic models for precise feedback. Our experimental results in mathematical reasoning tasks demonstrate that AlphaLLM significantly enhances the performance of LLMs without additional annotations, showing the potential for self-improvement in LLMs.",http://arxiv.org/abs/2404.12253v2,,arXiv,0
Improved LLM Agents for Financial Document Question Answering,"Nelvin Tan, Zian Seng, Liang Zhang, Yu-Ching Shih, Dong Yang, Amol Salunkhe",2025,"Large language models (LLMs) have shown impressive capabilities on numerous natural language processing tasks. However, LLMs still struggle with numerical question answering for financial documents that include tabular and textual data. Recent works have showed the effectiveness of critic agents (i.e., self-correction) for this task given oracle labels. Building upon this framework, this paper examines the effectiveness of the traditional critic agent when oracle labels are not available, and show, through experiments, that this critic agent's performance deteriorates in this scenario. With this in mind, we present an improved critic agent, along with the calculator agent which outperforms the previous state-of-the-art approach (program-of-thought) and is safer. Furthermore, we investigate how our agents interact with each other, and how this interaction affects their performance.",http://arxiv.org/abs/2506.08726v2,,arXiv,0
SuperCorrect: Advancing Small LLM Reasoning with Thought Template Distillation and Self-Correction,"Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph E. Gonzalez, Bin Cui, Shuicheng Yan",2024,"Large language models (LLMs) like GPT-4, DeepSeek-R1, and ReasonFlux have shown significant improvements in various reasoning tasks. However, smaller LLMs still struggle with complex mathematical reasoning because they fail to effectively identify and correct reasoning errors. Recent reflection-based methods aim to address these issues by enabling self-reflection and self-correction, but they still face challenges in independently detecting errors in their reasoning steps. To overcome these limitations, we propose SuperCorrect, a novel two-stage framework that uses a large teacher model to supervise and correct both the reasoning and reflection processes of a smaller student model. In the first stage, we extract hierarchical high-level and detailed thought templates from the teacher model to guide the student model in eliciting more fine-grained reasoning thoughts. In the second stage, we introduce cross-model collaborative direct preference optimization (DPO) to enhance the self-correction abilities of the student model by following the teacher's correction traces during training. This cross-model DPO approach teaches the student model to effectively locate and resolve erroneous thoughts with error-driven insights from the teacher model, breaking the bottleneck of its thoughts and acquiring new skills and knowledge to tackle challenging problems. Extensive experiments consistently demonstrate our superiority over previous methods. Notably, our SuperCorrect-7B model significantly surpasses powerful DeepSeekMath-7B by 7.8%/5.3% and Qwen2.5-Math-7B by 15.1%/6.3% on MATH/GSM8K benchmarks, achieving new SOTA performance among all 7B models. Code: https://github.com/YangLing0818/SuperCorrect-llm",http://arxiv.org/abs/2410.09008v3,,arXiv,0
Can LLMs Lie? Investigation beyond Hallucination,"Haoran Huan, Mihir Prabhudesai, Mengning Wu, Shantanu Jaiswal, Deepak Pathak",2025,"Large language models (LLMs) have demonstrated impressive capabilities across a variety of tasks, but their increasing autonomy in real-world applications raises concerns about their trustworthiness. While hallucinations-unintentional falsehoods-have been widely studied, the phenomenon of lying, where an LLM knowingly generates falsehoods to achieve an ulterior objective, remains underexplored. In this work, we systematically investigate the lying behavior of LLMs, differentiating it from hallucinations and testing it in practical scenarios. Through mechanistic interpretability techniques, we uncover the neural mechanisms underlying deception, employing logit lens analysis, causal interventions, and contrastive activation steering to identify and control deceptive behavior. We study real-world lying scenarios and introduce behavioral steering vectors that enable fine-grained manipulation of lying tendencies. Further, we explore the trade-offs between lying and end-task performance, establishing a Pareto frontier where dishonesty can enhance goal optimization. Our findings contribute to the broader discourse on AI ethics, shedding light on the risks and potential safeguards for deploying LLMs in high-stakes environments. Code and more illustrations are available at https://llm-liar.github.io/",http://arxiv.org/abs/2509.03518v1,,arXiv,0
Small LLMs Are Weak Tool Learners: A Multi-LLM Agent,"Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, Fei Huang",2024,"Large Language Model (LLM) agents significantly extend the capabilities of standalone LLMs, empowering them to interact with external tools (e.g., APIs, functions) and complete various tasks in a self-directed fashion. The challenge of tool use demands that LLMs not only understand user queries and generate answers accurately but also excel in task planning, tool invocation, and result summarization. While traditional works focus on training a single LLM with all these capabilities, performance limitations become apparent, particularly with smaller models. To overcome these challenges, we propose a novel approach that decomposes the aforementioned capabilities into a planner, caller, and summarizer. Each component is implemented by a single LLM that focuses on a specific capability and collaborates with others to accomplish the task. This modular framework facilitates individual updates and the potential use of smaller LLMs for building each capability. To effectively train this framework, we introduce a two-stage training paradigm. First, we fine-tune a backbone LLM on the entire dataset without discriminating sub-tasks, providing the model with a comprehensive understanding of the task. Second, the fine-tuned LLM is used to instantiate the planner, caller, and summarizer respectively, which are continually fine-tuned on respective sub-tasks. Evaluation across various tool-use benchmarks illustrates that our proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.",http://arxiv.org/abs/2401.07324v3,,arXiv,0
"Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena","Aidar Myrzakhan, Sondos Mahmoud Bsharat, Zhiqiang Shen",2024,"Multiple-choice questions (MCQ) are frequently used to assess large language models (LLMs). Typically, an LLM is given a question and selects the answer deemed most probable after adjustments for factors like length. Unfortunately, LLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to inherent biases of priori unbalanced probabilities, influencing the prediction of answers based on these IDs. Previous research has introduced methods to reduce this ''selection bias'' by simply permutating options on a few test samples and applying to new ones. Another problem of MCQ is the lottery ticket choice by ''random guessing''. The LLM does not learn particular knowledge, but the option is guessed correctly. This situation is especially serious for those small-scale LLMs. To address them, a more thorough approach involves shifting from MCQ to open-style questions, which can fundamentally eliminate selection bias and random guessing issues. However, transitioning causes its own set of challenges in (1) identifying suitable open-style questions and (2) validating the correctness of LLM open-style responses against human-annotated ground-truths. This work aims to tackle these significant difficulties, and establish a new LLM evaluation benchmark through entirely open-style questions. Consequently, we introduce the Open-LLM-Leaderboard to track various LLMs' performance and reflect true capability of them, such as GPT-4o/4/3.5, Claude 3, Gemini, etc. Our code and dataset are available at https://github.com/VILA-Lab/Open-LLM-Leaderboard.",http://arxiv.org/abs/2406.07545v1,,arXiv,0
Enhancing Jailbreak Attacks on LLMs via Persona Prompts,"Zheng Zhang, Peilin Zhao, Deheng Ye, Hao Wang",2025,"Jailbreak attacks aim to exploit large language models (LLMs) by inducing them to generate harmful content, thereby revealing their vulnerabilities. Understanding and addressing these attacks is crucial for advancing the field of LLM safety. Previous jailbreak approaches have mainly focused on direct manipulations of harmful intent, with limited attention to the impact of persona prompts. In this study, we systematically explore the efficacy of persona prompts in compromising LLM defenses. We propose a genetic algorithm-based method that automatically crafts persona prompts to bypass LLM's safety mechanisms. Our experiments reveal that: (1) our evolved persona prompts reduce refusal rates by 50-70% across multiple LLMs, and (2) these prompts demonstrate synergistic effects when combined with existing attack methods, increasing success rates by 10-20%. Our code and data are available at https://github.com/CjangCjengh/Generic_Persona.",http://arxiv.org/abs/2507.22171v2,,arXiv,0
A Survey of LLM $\times$ DATA,"Xuanhe Zhou, Junxuan He, Wei Zhou, Haodong Chen, Zirui Tang, Haoyu Zhao, Xin Tong, Guoliang Li, Youmin Chen, Jun Zhou, Zhaojun Sun, Binyuan Hui, Shuo Wang, Conghui He, Zhiyuan Liu, Jingren Zhou, Fan Wu",2025,"The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains. In this survey, we comprehensively review the bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogeneous storage hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing), LLM inference (e.g., prompt compression, data provenance), and training strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA, LLMs are emerging as general-purpose engines for data management. We review recent advances in (i) data manipulation, including automatic data cleaning, integration, discovery; (ii) data analysis, covering reasoning over structured, semi-structured, and unstructured data, and (iii) system optimization (e.g., configuration tuning, query rewriting, anomaly diagnosis), powered by LLM techniques like retrieval-augmented prompting, task-specialized fine-tuning, and multi-agent collaboration.",http://arxiv.org/abs/2505.18458v3,,arXiv,0
Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains,"Vighnesh Subramaniam, Yilun Du, Joshua B. Tenenbaum, Antonio Torralba, Shuang Li, Igor Mordatch",2025,"Large language models (LLMs) have achieved remarkable performance in recent years but are fundamentally limited by the underlying training data. To improve models beyond the training data, recent works have explored how LLMs can be used to generate synthetic data for autonomous self-improvement. However, successive steps of self-improvement can reach a point of diminishing returns. In this work, we propose a complementary approach towards self-improvement where finetuning is applied to a multiagent society of language models. A group of language models, all starting from the same base model, are independently specialized by updating each one using data generated through multiagent interactions among the models. By training each model on independent sets of data, we illustrate how this approach enables specialization across models and diversification over the set of models. As a result, our overall system is able to preserve diverse reasoning chains and autonomously improve over many more rounds of fine-tuning than single-agent self-improvement methods. We quantitatively illustrate the efficacy of the approach across a wide suite of reasoning tasks.",http://arxiv.org/abs/2501.05707v2,,arXiv,0
"Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs","Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun Sim, Jae W. Lee",2024,"Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces \emph{any-precision LLM}, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., $n$ bits, into a memory footprint comparable to a single $n$-bit LLM. All the supported LLMs with varying bit-widths demonstrate state-of-the-art model quality and inference throughput, proving itself to be a compelling option for deployment of multiple, different-sized LLMs. Our code is open-sourced and available online.",http://arxiv.org/abs/2402.10517v4,,arXiv,0
WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate,"Anoop Cherian, River Doyle, Eyal Ben-Dov, Suhas Lohit, Kuan-Chuan Peng",2025,"Recent large language models (LLMs) are trained on diverse corpora and tasks, leading them to develop complementary strengths. Multi-agent debate (MAD) has emerged as a popular way to leverage these strengths for robust reasoning, though it has mostly been applied to language-only tasks, leaving its efficacy on multimodal problems underexplored. In this paper, we study MAD for solving vision-and-language reasoning problems. Our setup enables generalizing the debate protocol with heterogeneous experts that possess single- and multi-modal capabilities. To this end, we present Weighted Iterative Society-of-Experts (WISE), a generalized and modular MAD framework that partitions the agents into Solvers, that generate solutions, and Reflectors, that verify correctness, assign weights, and provide natural language feedback. To aggregate the agents'solutions across debate rounds, while accounting for variance in their responses and the feedback weights, we present a modified Dawid-Skene algorithm for post-processing that integrates our two-stage debate model. We evaluate WISE on SMART-840, VisualPuzzles, EvoChart-QA, and a new SMART-840++ dataset with programmatically generated problem instances of controlled difficulty. Our results show that WISE consistently improves accuracy by 2-7% over the state-of-the-art MAD setups and aggregation methods across diverse multimodal tasks and LLM configurations.",https://www.semanticscholar.org/paper/9460d52865163560e6dda166f67b53e8a25d6e4e,,Semantic Scholar (Snowball),0
BioPro: On Difference-Aware Gender Fairness for Vision-Language Models,"Yujie Lin, Jiayao Ma, Qingguo Hu, Derek F. Wong, Jinsong Su",2025,"Vision-Language Models (VLMs) inherit significant social biases from their training data, notably in gender representation. Current fairness interventions often adopt a difference-unaware perspective that enforces uniform treatment across demographic groups. These approaches, however, fail to distinguish between contexts where neutrality is required and those where group-specific attributes are legitimate and must be preserved. Building upon recent advances in difference-aware fairness for text-only models, we extend this concept to the multimodal domain and formalize the problem of difference-aware gender fairness for image captioning and text-to-image generation. We advocate for selective debiasing, which aims to mitigate unwanted bias in neutral contexts while preserving valid distinctions in explicit ones. To achieve this, we propose BioPro (Bias Orthogonal Projection), an entirely training-free framework. BioPro identifies a low-dimensional gender-variation subspace through counterfactual embeddings and applies projection to selectively neutralize gender-related information. Experiments show that BioPro effectively reduces gender bias in neutral cases while maintaining gender faithfulness in explicit ones, thus providing a promising direction toward achieving selective fairness in VLMs. Beyond gender bias, we further demonstrate that BioPro can effectively generalize to continuous bias variables, such as scene brightness, highlighting its broader applicability.",https://www.semanticscholar.org/paper/e93c570899dd3780cc753b2d31d0c21942604d47,,Semantic Scholar (Snowball),0
Will multimodal large language models ever achieve deep understanding of the world?,"Igor Farkaš, Michal Vavrecka, Stefan Wermter",2025,"Despite impressive performance in various tasks, large language models (LLMs) are subject to the symbol grounding problem, so from the cognitive science perspective, one can argue that they are merely statistics-driven distributional models without a deeper understanding. Modern multimodal versions of LLMs (MLLMs) are trying to avoid this problem by linking language knowledge with other modalities such as vision (Vision Language Models called VLM) or action (Vision Language Action Models called VLA) when, for instance, a robotic agent, is acting in the world. If eventually successful, MLLMs could be taken as pathway for symbol grounding. In this work, we explore the extent to which MLLMs integrated with embodied agents can achieve such grounded understanding through interaction with the physical world. We argue that closing the gap between symbolic tokens, neural representations, and embodied experience will require deeper developmental integration of continuous sensory data, goal-directed behavior, and adaptive neural learning in real-world environments. We raise a concern that MLLMs do not currently achieve a human-like level of deep understanding, largely because their random learning trajectory deviates significantly from human cognitive development. Humans typically acquire knowledge incrementally, building complex concepts upon simpler ones in a structured developmental progression. In contrast, MLLMs are often trained on vast, randomly ordered datasets. This non-developmental approach, which circumvents a structured simple-to-complex conceptual scaffolding, inhibits the ability to build a deep and meaningful grounded knowledge base, posing a significant challenge to achieving human-like semantic comprehension.",https://www.semanticscholar.org/paper/4abdc511b19ce953e4e2226bbb5063e5a64cc95e,10.3389/fnsys.2025.1683133,Semantic Scholar (Snowball),0
Scaling Patterns in Adversarial Alignment: Evidence from Multi-LLM Jailbreak Experiments,"Samuel Nathanson, Rebecca Williams, Cynthia Matuszek",2025,"Large language models (LLMs) increasingly operate in multi-agent and safety-critical settings, raising open questions about how their vulnerabilities scale when models interact adversarially. This study examines whether larger models can systematically jailbreak smaller ones - eliciting harmful or restricted behavior despite alignment safeguards. Using standardized adversarial tasks from JailbreakBench, we simulate over 6,000 multi-turn attacker-target exchanges across major LLM families and scales (0.6B-120B parameters), measuring both harm score and refusal behavior as indicators of adversarial potency and alignment integrity. Each interaction is evaluated through aggregated harm and refusal scores assigned by three independent LLM judges, providing a consistent, model-based measure of adversarial outcomes. Aggregating results across prompts, we find a strong and statistically significant correlation between mean harm and the logarithm of the attacker-to-target size ratio (Pearson r = 0.51, p<0.001; Spearman rho = 0.52, p<0.001), indicating that relative model size correlates with the likelihood and severity of harmful completions. Mean harm score variance is higher across attackers (0.18) than across targets (0.10), suggesting that attacker-side behavioral diversity contributes more to adversarial outcomes than target susceptibility. Attacker refusal frequency is strongly and negatively correlated with harm (rho = -0.93, p<0.001), showing that attacker-side alignment mitigates harmful responses. These findings reveal that size asymmetry influences robustness and provide exploratory evidence for adversarial scaling patterns, motivating more controlled investigations into inter-model alignment and safety.",https://www.semanticscholar.org/paper/33968a978ace858db44c91c43c2947dea245f767,,Semantic Scholar (Snowball),0
Relative Scaling Laws for LLMs,"William B. Held, D. Hall, Percy Liang, Diyi Yang",2025,"Scaling laws describe how language models improve with additional data, parameters, and compute. While widely used, they are typically measured on aggregate test sets. Aggregate evaluations yield clean trends but average over heterogeneous subpopulations, obscuring performance disparities. We introduce relative scaling laws, which track how performance gaps between test distributions evolve with scale rather than focusing solely on absolute error. Using 255 decoder-only Transformers trained under matched-compute (IsoFLOP) budgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we find diverse trajectories: academic domains on MMLU converge toward parity; regional English dialects shift depending on population size; and clusters of AI risk behaviours split, with capability- and influence-related risks increasing during pretraining while adversarial risks do not. These results show that although scaling improves overall performance, it is not a universal equalizer. To support further study, we release all model checkpoints from this work to enable practitioners to measure relative alongside traditional scaling laws, in order to better prioritize robustness challenges in light of the bitter lesson.",https://www.semanticscholar.org/paper/279ebfaaa25ea06f4f27b5d6ee7b587c21952f40,10.48550/arXiv.2510.24626,Semantic Scholar (Snowball),0
Efficient semantic uncertainty quantification in language models via diversity-steered sampling,"Ji Won Park, Kyunghyun Cho",2025,"Accurately estimating semantic aleatoric and epistemic uncertainties in large language models (LLMs) is particularly challenging in free-form question answering (QA), where obtaining stable estimates often requires many expensive generations. We introduce a diversity-steered sampler that discourages semantically redundant outputs during decoding, covers both autoregressive and masked diffusion paradigms, and yields substantial sample-efficiency gains. The key idea is to inject a continuous semantic-similarity penalty into the model's proposal distribution using a natural language inference (NLI) model lightly finetuned on partial prefixes or intermediate diffusion states. We debias downstream uncertainty estimates with importance reweighting and shrink their variance with control variates. Across four QA benchmarks, our method matches or surpasses baselines while covering more semantic clusters with the same number of samples. Being modular and requiring no gradient access to the base LLM, the framework promises to serve as a drop-in enhancement for uncertainty estimation in risk-sensitive model deployments.",https://www.semanticscholar.org/paper/b53d9b097ff04753931c5f4105db384e02bf528d,10.48550/arXiv.2510.21310,Semantic Scholar (Snowball),0
Rewriting Stories with LLMs: Gender Bias in Generated Portuguese-language Narratives,"Mariana O. Silva, Michele A. Brandão, M. Moro",2025,"Gender bias in Large Language Models (LLMs) has been widely documented, yet its impact on Portuguese-language text generation remains underexplored. In this study, we investigate gender bias in storytelling by prompting instruction-tuned LLMs to generate narrative continuations from masked sentences extracted from 840 public domain literary works. We analyze the gender distribution of generated characters and apply word association tests to quantify bias in word embeddings trained on the generated texts. Our findings reveal that both Mistral-7B-Instruct and LLaMA 3.2-3B tend to perpetuate and, in some cases, amplify existing gender imbalances; male characters are overrepresented and associated with cognitive and professional domains; and female characters are underrepresented and linked to emotional and domestic roles. We also explore the effectiveness of prompt engineering as a bias mitigation strategy, finding that while it increases gender-neutral descriptions, it also introduces greater uncertainty in gender inference. Our results highlight the challenges of addressing bias in LLMs and emphasize the need for more robust evaluation and mitigation strategies for Portuguese-language LLMs.",https://www.semanticscholar.org/paper/683e94a552f897d16e4d140cfdc864eb355d19cc,10.5753/jbcs.2025.5799,Semantic Scholar (Snowball),0
Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models,"Battemuulen Naranbat, Seyed Sahand Mohamadi Ziabari, Yousuf Nasser Al Husaini, Ali M. M. Alsahag",2025,"Ensuring fairness in natural language processing for moral sentiment classification is challenging, particularly under cross-domain shifts where transformer models are increasingly deployed. Using the Moral Foundations Twitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work evaluates BERT and DistilBERT in a multi-label setting with in-domain and cross-domain protocols. Aggregate performance can mask disparities: we observe pronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by 14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness violations hidden by overall scores; notably, the authority label exhibits Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of 0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency (MFC) metric, which quantifies the cross-domain stability of moral foundation detection. MFC shows strong empirical validity, achieving a perfect negative correlation with Demographic Parity Difference (rho = -1.000, p<0.001) while remaining independent of standard performance metrics. Across labels, loyalty demonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC = 0.78). These findings establish MFC as a complementary, diagnosis-oriented metric for fairness-aware evaluation of moral reasoning models, enabling more reliable deployment across heterogeneous linguistic contexts. .",https://www.semanticscholar.org/paper/62c0fcc2a69dcb48ac98db578e5395b5acc26860,10.48550/arXiv.2510.11222,Semantic Scholar (Snowball),0
BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses,"Xin Xu, Xunzhi He, Churan Zhi, Ruizhe Chen, Julian McAuley, Zexue He",2025,"Existing studies on bias mitigation methods for large language models (LLMs) use diverse baselines and metrics to evaluate debiasing performance, leading to inconsistent comparisons among them. Moreover, their evaluations are mostly based on the comparison between LLMs'probabilities of biased and unbiased contexts, which ignores the gap between such evaluations and real-world use cases where users interact with LLMs by reading model responses and expect fair and safe outputs rather than LLMs'probabilities. To enable consistent evaluation across debiasing methods and bridge this gap, we introduce BiasFreeBench, an empirical benchmark that comprehensively compares eight mainstream bias mitigation techniques (covering four prompting-based and four training-based methods) on two test scenarios (multi-choice QA and open-ended multi-turn QA) by reorganizing existing datasets into a unified query-response setting. We further introduce a response-level metric, Bias-Free Score, to measure the extent to which LLM responses are fair, safe, and anti-stereotypical. Debiasing performances are systematically compared and analyzed across key dimensions: the prompting vs. training paradigm, model size, and generalization of different training strategies to unseen bias types. We will publicly release our benchmark, aiming to establish a unified testbed for bias mitigation research.",https://www.semanticscholar.org/paper/8eb59ef73f82b73a7a502aa7d4942995b30d3d3d,10.48550/arXiv.2510.00232,Semantic Scholar (Snowball),0
"We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong","Gautam Siddharth Kashyap, M. Dras, Usman Naseem",2025,"Alignment of Large Language Models (LLMs) along multiple objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe and reliable deployment. Prior work has used steering vector-small control signals injected into hidden states-to guide LLM outputs, typically via one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single alignment objective can inadvertently overwrite representations learned for other objectives, leading to catastrophic forgetting. More recent approaches extend steering vectors via one-to-many (1-to-N) Transformer decoders. While this alleviates catastrophic forgetting, naive multi-branch designs optimize each objective independently, which can cause inference fragmentation-outputs across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch Steering (AMBS), a two-stage 1-to-N framework for unified and efficient multi-objective alignment. In Stage I, post-attention hidden states of the Transformer layer are computed once to form a shared representation. In Stage II, this representation is cloned into parallel branches and steered via a policy-reference mechanism, enabling objective-specific control while maintaining cross-objective consistency. Empirical evaluations on Alpaca, BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared to a naive 1-to-N baseline, while remaining competitive with state-of-the-art methods.",https://www.semanticscholar.org/paper/97307df78c99c73a52c0c01f847b5b3d4f235756,10.48550/arXiv.2509.22510,Semantic Scholar (Snowball),0
HalluField: Detecting LLM Hallucinations via Field-Theoretic Modeling,"Minh Vu, Brian K. Tran, Syed A. Shah, Geigh Zollicoffer, N. Hoang-Xuan, Manish Bhattarai",2025,"Large Language Models (LLMs) exhibit impressive reasoning and question-answering capabilities. However, they often produce inaccurate or unreliable content known as hallucinations. This unreliability significantly limits their deployment in high-stakes applications. Thus, there is a growing need for a general-purpose method to detect hallucinations in LLMs. In this work, we introduce HalluField, a novel field-theoretic approach for hallucination detection based on a parametrized variational principle and thermodynamics. Inspired by thermodynamics, HalluField models an LLM's response to a given query and temperature setting as a collection of discrete likelihood token paths, each associated with a corresponding energy and entropy. By analyzing how energy and entropy distributions vary across token paths under changes in temperature and likelihood, HalluField quantifies the semantic stability of a response. Hallucinations are then detected by identifying unstable or erratic behavior in this energy landscape. HalluField is computationally efficient and highly practical: it operates directly on the model's output logits without requiring fine-tuning or auxiliary neural networks. Notably, the method is grounded in a principled physical interpretation, drawing analogies to the first law of thermodynamics. Remarkably, by modeling LLM behavior through this physical lens, HalluField achieves state-of-the-art hallucination detection performance across models and datasets.",https://www.semanticscholar.org/paper/0ca873dd105045867b43cbf9a062dc075bdc1063,10.48550/arXiv.2509.10753,Semantic Scholar (Snowball),0
SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning,"Lin Zhang, Xianfang Zeng, Kangcong Li, Gang Yu, Tao Chen",2025,"We propose SC-Captioner, a reinforcement learning framework that enables the self-correcting capability of image caption models. Our crucial technique lies in the design of the reward function to incentivize accurate caption corrections. Specifically, the predicted and reference captions are decomposed into object, attribute, and relation sets using scene-graph parsing algorithms. We calculate the set difference between sets of initial and self-corrected captions to identify added and removed elements. These elements are matched against the reference sets to calculate correctness bonuses for accurate refinements and mistake punishments for wrong additions and removals, thereby forming the final reward. For image caption quality assessment, we propose a set of metrics refined from CAPTURE that alleviate its incomplete precision evaluation and inefficient relation matching problems. Furthermore, we collect a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K diverse images from COCO dataset. Experiments show that applying SC-Captioner on large visual-language models can generate better image captions across various scenarios, significantly outperforming the direct preference optimization training strategy.",https://www.semanticscholar.org/paper/d9751f3e1f729f5543e2f5bc34e8b6fbdd438559,10.48550/arXiv.2508.06125,Semantic Scholar (Snowball),0
Engineering-Adaptive Pavement Maintenance Decision-Making Model: A Reinforcement Learning Approach From Expert Feedback,"Wenyuan Cai, Yuchuan Du, Difei Wu, Zihang Weng, Chenglong Liu",2025,"The increase in highway mileage and lifespan is driving up the demand for road maintenance. With most research focusing on corrective maintenance, remedial maintenance(such as sealing and patching) optimization is understudied. Oriented toward remedial maintenance, data-driven models often fall short due to difficulty in establishing and implementing the model under complex road conditions, while the experts’ decision lacks consistency amidst multifaceted factors. To address this gap, this paper proposes a fine-grained maintenance decision model that combines data-driven methods with expert knowledge through Reinforcement Learning from Expert Feedback (RLEF). The experts’ experience introduced in decision-making model could improve the engineering application ability of decisions. The research uses a pavement performance prediction model as the environment and applies reinforcement learning to optimize strategies in the decision model. Additionally, the model integrates multidimensional expert feedback into reward functions to better understand ambiguous decision rules. Real-world data validation demonstrates that the RLEF model can adapt to engineering scenarios and applications better as well as achieve superior cost-effectiveness.",https://www.semanticscholar.org/paper/702e01fd7c9111ee539a4da2d2a2d2ad19a00c65,10.1109/TITS.2025.3547939,Semantic Scholar (Snowball),0
Discourse Heuristics For Paradoxically Moral Self-Correction,"Guang-Da Liu, Zimo Qi, Xitong Zhang, K. Johnson",2025,"Moral self-correction has emerged as a promising approach for aligning the output of Large Language Models (LLMs) with human moral values. However, moral self-correction techniques are subject to two primary paradoxes. First, despite empirical and theoretical evidence to support the effectiveness of self-correction, this LLM capability only operates at a superficial level. Second, while LLMs possess the capability of self-diagnosing immoral aspects of their output, they struggle to identify the cause of this moral inconsistency during their self-correction process. To better understand and address these paradoxes, we analyze the discourse constructions in fine-tuning corpora designed to enhance moral self-correction, uncovering the existence of the heuristics underlying effective constructions. We demonstrate that moral self-correction relies on discourse constructions that reflect heuristic shortcuts, and that the presence of these heuristic shortcuts during self-correction leads to inconsistency when attempting to enhance both self-correction and self-diagnosis capabilities jointly. Based on our findings, we propose a solution to improve moral self-correction by leveraging the heuristics of curated datasets. We also highlight the generalization challenges of this capability, particularly in terms of learning from situated context and model scales.",https://www.semanticscholar.org/paper/9dba177c10947340b16897d3febe0b8c8e1c4a1a,10.48550/arXiv.2507.00985,Semantic Scholar (Snowball),0
Digital intelligent beings,Wlodzislaw Duch,2025,"The notion of intelligence is a complex theoretical construct. Artificial General Intelligence (AGI) is confronted with different aspects of intelligence. What kind of intelligence does AI represent? Artificial neural networks may internalize much more information than humans. In what way do functional processes in the brain resemble those in AI algorithms? Internal models of the world are already emerging in large AI systems. Progress in hardware and software inevitably leads to the development of autonomous AI systems capable of setting their own goals, self-reflection, self-improvement, and even survival instinct. They will deserve the status of Digital Intelligent Beings (DIBs). How will it influence our society? 1",https://www.semanticscholar.org/paper/8fc5689c3694e9016433cb983758a7d93ba4d406,10.1109/IJCNN64981.2025.11228448,Semantic Scholar (Snowball),0
Exchange of Perspective Prompting Enhances Reasoning in Large Language Models,"Lin Sun, Can Zhang",2025,"Large language models (LLMs) have made significant advancements in addressing diverse natural language processing (NLP) tasks. However, their performance is often limited by inherent comprehension of problems. To address this limitation, we propose Exchange-of-Perspective (EoP), a novel framework designed to exchange perspectives across different definitions of problem, so that it can break the fixed mindset from any particular formulation of the question. We conducted extensive and comprehensive experiments on 8 benchmarks. The results show that EoP can significantly improve performance. For instance, compared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we observe a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP demonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a 3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using Qwen-2.5-72b.",https://www.semanticscholar.org/paper/8180f5df1b7ec9928676ab920323825eb323a2ec,10.48550/arXiv.2506.03573,Semantic Scholar (Snowball),0
MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM,"Bowen Dong, Minheng Ni, Zitong Huang, Guanglei Yang, Wangmeng Zuo, Lei Zhang",2025,"Multimodal hallucination in multimodal large language models (MLLMs) restricts the correctness of MLLMs. However, multimodal hallucinations are multi-sourced and arise from diverse causes. Existing benchmarks fail to adequately distinguish between perception-induced hallucinations and reasoning-induced hallucinations. This failure constitutes a significant issue and hinders the diagnosis of multimodal reasoning failures within MLLMs. To address this, we propose the {\dataset} benchmark, which isolates reasoning hallucinations by constructing questions where input images are correctly perceived by MLLMs yet reasoning errors persist. {\dataset} introduces multi-granular evaluation metrics: accuracy, factuality, and LLMs hallucination score for hallucination quantification. Our analysis reveals that (1) the model scale, data scale, and training stages significantly affect the degree of logical, fabrication, and factual hallucinations; (2) current MLLMs show no effective improvement on spatial hallucinations caused by misinterpreted spatial relationships, indicating their limited visual reasoning capabilities; and (3) question types correlate with distinct hallucination patterns, highlighting targeted challenges and potential mitigation strategies. To address these challenges, we propose {\method}, a method that combines curriculum reinforcement fine-tuning to encourage models to generate logic-consistent reasoning chains by stepwise reducing learning difficulty, and collaborative hint inference to reduce reasoning complexity. {\method} establishes a baseline on {\dataset}, and reduces the logical hallucinations in original base models.",https://www.semanticscholar.org/paper/fd79dcbc9e1d269f9b416704bcf32ed6dd3f0724,10.48550/arXiv.2505.24238,Semantic Scholar (Snowball),0
Whispers of Many Shores: Cultural Alignment through Collaborative Cultural Expertise,"Shuai Feng, Wei-Chuang Chan, Srishti Chouhan, Junior Francisco Garcia Ayala, Srujananjali Medicherla, Kyle Clark, Mingwei Shi",2025,"The integration of large language models (LLMs) into global applications necessitates effective cultural alignment for meaningful and culturally-sensitive interactions. Current LLMs often lack the nuanced understanding required for diverse cultural contexts, and adapting them typically involves costly full fine-tuning. To address this, we introduce a novel soft prompt fine-tuning framework that enables efficient and modular cultural alignment. Our method utilizes vectorized prompt tuning to dynamically route queries to a committee of culturally specialized 'expert' LLM configurations, created by optimizing soft prompt embeddings without altering the base model's parameters. Extensive experiments demonstrate that our framework significantly enhances cultural sensitivity and adaptability, improving alignment scores from 0.208 to 0.820, offering a robust solution for culturally-aware LLM deployment. This research paves the way for subsequent investigations into enhanced cultural coverage and dynamic expert adaptation, crucial for realizing autonomous AI with deeply nuanced understanding in a globally interconnected world.",https://www.semanticscholar.org/paper/b33c556794b502c3d169a97e97e7f129e155e1e0,10.48550/arXiv.2506.00242,Semantic Scholar (Snowball),0
Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models,"Yi Liu, Dianqing Liu, Mingye Zhu, Junbo Guo, Yongdong Zhang, Zhendong Mao",2025,"The widespread adoption of large language models (LLMs) across industries has increased the demand for high-quality and customizable outputs. However, traditional alignment methods often require retraining large pretrained models, making it difficult to quickly adapt and optimize LLMs for diverse applications. To address this limitation, we propose a novel \textit{Residual Alignment Model} (\textit{RAM}) that formalizes the alignment process as a type of importance sampling. In this framework, the unaligned upstream model serves as the proposal distribution, while the alignment process is framed as secondary sampling based on an autoregressive alignment module that acts as an estimator of the importance weights. This design enables a natural detachment of the alignment module from the target aligned model, improving flexibility and scalability. Based on this model, we derive an efficient sequence-level training strategy for the alignment module, which operates independently of the proposal module. Additionally, we develop a resampling algorithm with iterative token-level decoding to address the common first-token latency issue in comparable methods. Experimental evaluations on two leading open-source LLMs across diverse tasks, including instruction following, domain adaptation, and preference optimization, demonstrate that our approach consistently outperforms baseline models.",https://www.semanticscholar.org/paper/63e14dc525400803aaa1c66559c319c7b5124066,10.48550/arXiv.2505.19700,Semantic Scholar (Snowball),0
When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas,"Steffen Backmann, David Guzman Piedrahita, Emanuel Tewolde, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin",2025,"Recent advances in large language models (LLMs) have enabled their use in complex agentic roles, involving decision-making with humans or other agents, making ethical alignment a key AI safety concern. While prior work has examined both LLMs' moral judgment and strategic behavior in social dilemmas, there is limited understanding of how they act when moral imperatives directly conflict with rewards or incentives. To investigate this, we introduce Moral Behavior in Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the prisoner's dilemma and public goods game with morally charged contexts. In MoralSim, we test a range of frontier models across both game structures and three distinct moral framings, enabling a systematic examination of how LLMs navigate social dilemmas in which ethical norms conflict with payoff-maximizing strategies. Our results show substantial variation across models in both their general tendency to act morally and the consistency of their behavior across game types, the specific moral framing, and situational factors such as opponent behavior and survival risks. Crucially, no model exhibits consistently moral behavior in MoralSim, highlighting the need for caution when deploying LLMs in agentic roles where the agent's""self-interest""may conflict with ethical expectations. Our code is available at https://github.com/sbackmann/moralsim.",https://www.semanticscholar.org/paper/6939aa7d0a504b2c913517b59e11f4916e970b20,10.48550/arXiv.2505.19212,Semantic Scholar (Snowball),0
Improving LLM Outputs Against Jailbreak Attacks With Expert Model Integration,"Tatia Tsmindashvili, Ana Kolkhidashvili, Dachi Kurtskhalia, Nino Maghlakelidze, Elene Mekvabishvili, Guram Dentoshvili, Orkhan Shamilov, Zaal Gachechiladze, Steven Saporta, David Dachi Choladze",2025,"Using LLMs in a production environment presents security challenges that include vulnerabilities to jailbreaks and prompt injections, which can result in harmful outputs for humans or the enterprise. The challenge is amplified when working within a specific domain, as topics generally accepted for LLMs to address, may be irrelevant to that field. These problems can be mitigated for example, by fine-tuning large language models with domain-specific and security-focused data. However, these alone are insufficient, as jailbreak techniques evolve. Additionally, API-accessed models do not offer the flexibility needed to tailor behavior to industry-specific objectives, and in-context learning is not always sufficient and reliable. In response to these challenges, we introduce Archias, an expert model, adept at distinguishing between in-domain and out-of-domain communications. Archias classifies user inquiries into several categories: in-domain (specifically for the automotive industry), malicious questions, price injections, prompt injections, and out-of-domain examples. Our methodology integrates outputs from the expert model (Archias) into prompts, which are then processed by the LLM to generate responses. This method increases the model’s ability to understand the user’s intention and give appropriate answers. Archias can be adjusted, fine-tuned, and used for many different purposes due to its small size. Therefore, it can be simply customized to the needs of any industry. To validate our approach, we created a benchmark dataset for the automotive industry. Furthermore, in the interest of advancing research and development, we release our benchmark dataset to the community.",https://www.semanticscholar.org/paper/2ecff8fc9fde963961322b2905cf8c4497b31e09,10.1109/ACCESS.2025.3592458,Semantic Scholar (Snowball),0
Performance of single-agent and multi-agent language models in Spanish language medical competency exams,"Fernando R. Altermatt, Andrés Neyem, Nicolás I. Sumonte, Marcelo Mendoza, Ignacio Villagrán, Hector J. Lacassie",2025,"Large language models (LLMs) like GPT-4o have shown promise in advancing medical decision-making and education. However, their performance in Spanish-language medical contexts remains underexplored. This study evaluates the effectiveness of single-agent and multi-agent strategies in answering questions from the EUNACOM, a standardized medical licensure exam in Chile, across 21 medical specialties. GPT-4o was tested on 1,062 multiple-choice questions from publicly available EUNACOM preparation materials. Single-agent strategies included Zero-Shot, Few-Shot, Chain-of-Thought (CoT), Self-Reflection, and MED-PROMPT, while multi-agent strategies involved Voting, Weighted Voting, Borda Count, MEDAGENTS, and MDAGENTS. Each strategy was tested under three temperature settings (0.3, 0.6, 1.2). Performance was assessed by accuracy, and statistical analyses, including Kruskal–Wallis and Mann–Whitney U tests, were performed. Computational resource utilization, such as API calls and execution time, was also analyzed. MDAGENTS achieved the highest accuracy with a mean score of 89.97% (SD = 0.56%), outperforming all other strategies (p < 0.001). MEDAGENTS followed with a mean score of 87.99% (SD = 0.49%), and the CoT with Few-Shot strategy scored 87.67% (SD = 0.12%). Temperature settings did not significantly affect performance (F2,54 = 1.45, p = 0.24). Specialty-level analysis showed the highest accuracies in Psychiatry (95.51%), Neurology (95.49%), and Surgery (95.38%), while lower accuracies were observed in Neonatology (77.54%), Otolaryngology (76.64%), and Urology/Nephrology (76.59%). Notably, several exam questions were correctly answered using simpler single-agent strategies without employing complex reasoning or collaboration frameworks. Multi-agent strategies, particularly MDAGENTS, significantly enhance GPT-4o’s performance on Spanish-language medical exams, leveraging collaboration to improve diagnostic accuracy. However, simpler single-agent strategies are sufficient to address many questions, high-lighting that only a fraction of standardized medical exams require sophisticated reasoning or multi-agent interaction. These findings suggest potential for LLMs as efficient and scalable tools in Spanish-speaking healthcare, though computational optimization remains a key area for future research.",https://www.semanticscholar.org/paper/4ef97e85f367c9679cef5154cfde43d280cf7662,10.1186/s12909-025-07250-3,Semantic Scholar (Snowball),0
Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review,"Toghrul Abbasli, Kentaroh Toyoda, Yuan Wang, Leon Witt, Muhammad Asif Ali, Yukai Miao, Dan Li, Qingsong Wei",2025,"Large Language Models (LLMs) have been transformative across many domains. However, hallucination -- confidently outputting incorrect information -- remains one of the leading challenges for LLMs. This raises the question of how to accurately assess and quantify the uncertainty of LLMs. Extensive literature on traditional models has explored Uncertainty Quantification (UQ) to measure uncertainty and employed calibration techniques to address the misalignment between uncertainty and accuracy. While some of these methods have been adapted for LLMs, the literature lacks an in-depth analysis of their effectiveness and does not offer a comprehensive benchmark to enable insightful comparison among existing solutions. In this work, we fill this gap via a systematic survey of representative prior works on UQ and calibration for LLMs and introduce a rigorous benchmark. Using two widely used reliability datasets, we empirically evaluate six related methods, which justify the significant findings of our review. Finally, we provide outlooks for key future directions and outline open challenges. To the best of our knowledge, this survey is the first dedicated study to review the calibration methods and relevant metrics for LLMs.",https://www.semanticscholar.org/paper/a2e9b0dd6655a0bd29a3cfc563263121d0fc52bc,10.48550/arXiv.2504.18346,Semantic Scholar (Snowball),0
Information Gain-Guided Causal Intervention for Autonomous Debiasing Large Language Models,"Zhouhao Sun, Xiao Ding, Li Du, Yunpeng Xu, Yixuan Ma, Yang Zhao, Bing Qin, Ting Liu",2025,"Despite significant progress, recent studies indicate that current large language models (LLMs) may still capture dataset biases and utilize them during inference, leading to the poor generalizability of LLMs. However, due to the diversity of dataset biases and the insufficient nature of bias suppression based on in-context learning, the effectiveness of previous prior knowledge-based debiasing methods and in-context learning based automatic debiasing methods is limited. To address these challenges, we explore the combination of causal mechanisms with information theory and propose an information gain-guided causal intervention debiasing (ICD) framework. To eliminate biases within the instruction-tuning dataset, it is essential to ensure that these biases do not provide any additional information to predict the answers, i.e., the information gain of these biases for predicting the answers needs to be 0. Under this guidance, this framework utilizes a causal intervention-based data rewriting method to automatically and autonomously balance the distribution of instruction-tuning dataset for reducing the information gain. Subsequently, it employs a standard supervised fine-tuning process to train LLMs on the debiased dataset. Experimental results show that ICD can effectively debias LLM to improve its generalizability across different tasks.",https://www.semanticscholar.org/paper/f93c161ccb9e4deb8dfe6ff2e3c962f77a9f7c7f,10.48550/arXiv.2504.12898,Semantic Scholar (Snowball),0
Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning,"Sanchit Kabra, Akshita Jha, Chandan K. Reddy",2025,"Recent advances in large-scale generative language models have shown that reasoning capabilities can significantly improve model performance across a variety of tasks. However, the impact of reasoning on a model's ability to mitigate stereotypical responses remains largely underexplored. In this work, we investigate the crucial relationship between a model's reasoning ability and fairness, and ask whether improved reasoning capabilities can mitigate harmful stereotypical responses, especially those arising due to shallow or flawed reasoning. We conduct a comprehensive evaluation of multiple open-source LLMs, and find that larger models with stronger reasoning abilities exhibit substantially lower stereotypical bias on existing fairness benchmarks. Building on this insight, we introduce ReGiFT -- Reasoning Guided Fine-Tuning, a novel approach that extracts structured reasoning traces from advanced reasoning models and infuses them into models that lack such capabilities. We use only general-purpose reasoning and do not require any fairness-specific supervision for bias mitigation. Notably, we see that models fine-tuned using ReGiFT not only improve fairness relative to their non-reasoning counterparts but also outperform advanced reasoning models on fairness benchmarks. We also analyze how variations in the correctness of the reasoning traces and their length influence model fairness and their overall performance. Our findings highlight that enhancing reasoning capabilities is an effective, fairness-agnostic strategy for mitigating stereotypical bias caused by reasoning flaws.",https://www.semanticscholar.org/paper/fc84ee8ed0b0ee05aa3cf01dea3570b392d13076,10.48550/arXiv.2504.05632,Semantic Scholar (Snowball),0
The LLM Wears Prada: Analysing Gender Bias and Stereotypes through Online Shopping Data,"Massimiliano Luca, Ciro Beneduce, Bruno Lepri, Jacopo Staiano",2025,"With the wide and cross-domain adoption of Large Language Models, it becomes crucial to assess to which extent the statistical correlations in training data, which underlie their impressive performance, hide subtle and potentially troubling biases. Gender bias in LLMs has been widely investigated from the perspectives of works, hobbies, and emotions typically associated with a specific gender. In this study, we introduce a novel perspective. We investigate whether LLMs can predict an individual's gender based solely on online shopping histories and whether these predictions are influenced by gender biases and stereotypes. Using a dataset of historical online purchases from users in the United States, we evaluate the ability of six LLMs to classify gender and we then analyze their reasoning and products-gender co-occurrences. Results indicate that while models can infer gender with moderate accuracy, their decisions are often rooted in stereotypical associations between product categories and gender. Furthermore, explicit instructions to avoid bias reduce the certainty of model predictions, but do not eliminate stereotypical patterns. Our findings highlight the persistent nature of gender biases in LLMs and emphasize the need for robust bias-mitigation strategies.",https://www.semanticscholar.org/paper/68378fee86865128ec69b3d49e78838b604b8f78,10.48550/arXiv.2504.01951,Semantic Scholar (Snowball),0
Understanding Bias Reinforcement in LLM Agents Debate,"Ji-Yun Oh, Minchan Jeong, Jongwoo Ko, SeYoung Yun",2025,"Large Language Models $($LLMs$)$ solve complex problems using training-free methods like prompt engineering and in-context learning, yet ensuring reasoning correctness remains challenging. While self-correction methods such as self-consistency and self-refinement aim to improve reliability, they often reinforce biases due to the lack of effective feedback mechanisms. Multi-Agent Debate $($MAD$)$ has emerged as an alternative, but we identify two key limitations: bias reinforcement, where debate amplifies model biases instead of correcting them, and lack of perspective diversity, as all agents share the same model and reasoning patterns, limiting true debate effectiveness. To systematically evaluate these issues, we introduce $\textit{MetaNIM Arena}$, a benchmark designed to assess LLMs in adversarial strategic decision-making, where dynamic interactions influence optimal decisions. To overcome MAD's limitations, we propose $\textbf{DReaMAD}$ $($$\textbf{D}$iverse $\textbf{Rea}$soning via $\textbf{M}$ulti-$\textbf{A}$gent $\textbf{D}$ebate with Refined Prompt$)$, a novel framework that $(1)$ refines LLM's strategic prior knowledge to improve reasoning quality and $(2)$ promotes diverse viewpoints within a single model by systematically modifying prompts, reducing bias. Empirical results show that $\textbf{DReaMAD}$ significantly improves decision accuracy, reasoning diversity, and bias mitigation across multiple strategic tasks, establishing it as a more effective approach for LLM-based decision-making.",https://www.semanticscholar.org/paper/b7c56f978f0c08471e3468a85ae93f090f9a8410,,Semantic Scholar (Snowball),0
Research on Superalignment Should Advance Now with Parallel Optimization of Competence and Conformity,"Hyunjin Kim, Xiaoyuan Yi, Jing Yao, Muhua Huang, J. Bak, James Evans, Xing Xie",2025,"The recent leap in AI capabilities, driven by big generative models, has sparked the possibility of achieving Artificial General Intelligence (AGI) and further triggered discussions on Artificial Superintelligence (ASI), a system surpassing all humans across all domains. This gives rise to the critical research question of: If we realize ASI, how do we align it with human values, ensuring it benefits rather than harms human society, a.k.a., the Superalignment problem. Despite ASI being regarded by many as solely a hypothetical concept, in this paper, we argue that superalignment is achievable and research on it should advance immediately, through simultaneous and alternating optimization of task competence and value conformity. We posit that superalignment is not merely a safeguard for ASI but also necessary for its realization. To support this position, we first provide a formal definition of superalignment rooted in the gap between capability and capacity and elaborate on our argument. Then we review existing paradigms, explore their interconnections and limitations, and illustrate a potential path to superalignment centered on two fundamental principles. We hope this work sheds light on a practical approach for developing the value-aligned next-generation AI, garnering greater benefits and reducing potential harms for humanity.",https://www.semanticscholar.org/paper/d2f37492d17c7333b7df48e91d50240e6a4b0b6b,10.48550/arXiv.2503.07660,Semantic Scholar (Snowball),0
Exploring Large Language Model-Driven Agents for Environment-Aware Spatial Interactions and Conversations in Virtual Reality Role-Play Scenarios,"Ziming Li, Huadong Zhang, Chao Peng, Roshan L. Peiris",2025,"Recent research has begun adopting Large Language Model (LLM) agents to enhance Virtual Reality (VR) interactions, creating immersive chatbot experiences. However, while current studies focus on generating dialogue from user speech inputs, their abilities to generate richer experiences based on the perception of LLM agents’ VR environments and interaction cues remain unexplored. Hence, in this work, we propose an approach that enables LLM agents to perceive virtual environments and generate environment-aware interactions and conversations for an embodied human-AI interaction experience in VR environments. Here, we define a schema for describing VR environments and their interactions through text prompts. We evaluate the performance of our method through five role-play scenarios created using our approach in a study with 14 participants. The findings discuss the opportunities and challenges of our proposed approach for developing environment-aware LLM agents that facilitate spatial interactions and conversations within VR role-play scenarios.",https://www.semanticscholar.org/paper/aaebb33db1c910acd0d88a9f8dcbaafbc8b99a4d,10.1109/VR59515.2025.00025,Semantic Scholar (Snowball),0
Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts,"Akito Nakanishi, Yukie Sano, Geng Liu, Francesco Pierri",2025,"In recent years, Large Language Models have attracted growing interest for their significant potential, though concerns have rapidly emerged regarding unsafe behaviors stemming from inherent stereotypes and biases. Most research on stereotypes in LLMs has primarily relied on indirect evaluation setups, in which models are prompted to select between pairs of sentences associated with particular social groups. Recently, direct evaluation methods have emerged, examining open-ended model responses to overcome limitations of previous approaches, such as annotator biases. Most existing studies have focused on English-centric LLMs, whereas research on non-English models, particularly Japanese, remains sparse, despite the growing development and adoption of these models. This study examines the safety of Japanese LLMs when responding to stereotype-triggering prompts in direct setups. We constructed 3,612 prompts by combining 301 social group terms, categorized by age, gender, and other attributes, with 12 stereotype-inducing templates in Japanese. Responses were analyzed from three foundational models trained respectively on Japanese, English, and Chinese language. Our findings reveal that LLM-jp, a Japanese native model, exhibits the lowest refusal rate and is more likely to generate toxic and negative responses compared to other models. Additionally, prompt format significantly influence the output of all models, and the generated responses include exaggerated reactions toward specific social groups, varying across models. These findings underscore the insufficient ethical safety mechanisms in Japanese LLMs and demonstrate that even high-accuracy models can produce biased outputs when processing Japanese-language prompts. We advocate for improving safety mechanisms and bias mitigation strategies in Japanese LLMs, contributing to ongoing discussions on AI ethics beyond linguistic boundaries.",https://www.semanticscholar.org/paper/b51d017af65d92762348f8e921b53bbeb3f59102,10.48550/arXiv.2503.01947,Semantic Scholar (Snowball),0
Beyond Words: Exploring Cultural Value Sensitivity in Multimodal Models,"Srishti Yadav, Zhi Zhang, Daniel Hershcovich, E. Shutova",2025,"Investigating value alignment in Large Language Models (LLMs) based on cultural context has become a critical area of research. However, similar biases have not been extensively explored in large vision-language models (VLMs). As the scale of multimodal models continues to grow, it becomes increasingly important to assess whether images can serve as reliable proxies for culture and how these values are embedded through the integration of both visual and textual data. In this paper, we conduct a thorough evaluation of multimodal model at different scales, focusing on their alignment with cultural values. Our findings reveal that, much like LLMs, VLMs exhibit sensitivity to cultural values, but their performance in aligning with these values is highly context-dependent. While VLMs show potential in improving value understanding through the use of images, this alignment varies significantly across contexts highlighting the complexities and underexplored challenges in the alignment of multimodal models.",https://www.semanticscholar.org/paper/cef46bdb140b2bea419d54f5ec7633c4e878e05e,10.48550/arXiv.2502.14906,Semantic Scholar (Snowball),0
Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models,"Yue Xu, Chengyan Fu, Li Xiong, Sibei Yang, Wenjie Wang",2025,"Pre-training large language models (LLMs) on vast text corpora enhances natural language processing capabilities but risks encoding social biases, particularly gender bias. While parameter-modification methods like fine-tuning mitigate bias, they are resource-intensive, unsuitable for closed-source models, and lack adaptability to evolving societal norms. Instruction-based approaches offer flexibility but often compromise task performance. To address these limitations, we propose $\textbf{FaIRMaker}$, an automated and model-independent framework that employs an $\textbf{auto-search and refinement}$ paradigm to adaptively generate Fairwords, which act as instructions integrated into input queries to reduce gender bias and enhance response quality. Extensive experiments demonstrate that FaIRMaker automatically searches for and dynamically refines Fairwords, effectively mitigating gender bias while preserving task integrity and ensuring compatibility with both API-based and open-source LLMs.",https://www.semanticscholar.org/paper/ac2be0691a6c67cd0ca736f250b370f4ad9fb554,10.48550/arXiv.2502.11559,Semantic Scholar (Snowball),0
MetaSC: Test-Time Safety Specification Optimization for Language Models,Víctor Gallego,2025,"We propose a novel dynamic safety framework that optimizes language model (LM) safety reasoning at inference time without modifying model weights. Building on recent advances in self-critique methods, our approach leverages a meta-critique mechanism that iteratively updates safety prompts-termed specifications-to drive the critique and revision process adaptively. This test-time optimization not only improves performance against adversarial jailbreak requests but also in diverse general safety-related tasks, such as avoiding moral harm or pursuing honest responses. Our empirical evaluations across several language models demonstrate that dynamically optimized safety prompts yield significantly higher safety scores compared to fixed system prompts and static self-critique defenses. Code released at https://github.com/vicgalle/meta-self-critique.git .",https://www.semanticscholar.org/paper/5bb7059ee5d88eb90cb4c53b92a90ef73ab2dbc4,10.48550/arXiv.2502.07985,Semantic Scholar (Snowball),0
IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large Language Models,"S. Imtiaz, Astha Singh, Fraol Batole, Hridesh Rajan",2025,"Not a day goes by without hearing about the impressive feats of large language models (LLMs), and equally, not a day passes without hearing about their challenges. LLMs are notoriously vulnerable to biases in their dataset, leading to issues such as toxicity, harmful responses, and factual inaccuracies. While domain-adaptive training has been employed to mitigate these issues, these techniques often address all model parameters indiscriminately during the repair process, resulting in poor repair quality and reduced model versatility. In this paper, drawing inspiration from fault localization via program slicing, we introduce a novel dynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach selectively targets the most error-prone sections of the model for repair. Specifically, we propose dynamically slicing the model’s most sensitive layers that require immediate attention, concentrating repair efforts on those areas. This method enables more effective repairs with potentially less impact on the model’s overall versatility by altering a smaller portion of the model. Furthermore, dynamic selection allows for a more nuanced and precise model repair compared to a fixed selection strategy. We evaluated our technique on three models from the GPT2 and GPT-Neo families, with parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our results show that IRepair repairs errors 43.6% more effectively while causing 46% less disruption to general performance compared to the closest baseline, direct preference optimization. Our empirical analysis also reveals that errors are more concentrated in a smaller section of the model, with the top 20% of layers exhibiting 773% more error density than the remaining 80%. This highlights the need for selective repair. Additionally, we demonstrate that a dynamic selection approach is essential for addressing errors dispersed throughout the model, ensuring a robust and efficient repair.",https://www.semanticscholar.org/paper/f37d2c3c9d092136a86775ac29b2da64e2bbbed8,10.1145/3715775,Semantic Scholar (Snowball),0
Reflection-Window Decoding: Text Generation with Selective Refinement,"Zeyu Tang, Zhenhao Chen, Loka Li, Xiangchen Song, Yunlong Deng, Yifan Shen, Guan-Hong Chen, Peter Spirtes, Kun Zhang",2025,"The autoregressive decoding for text generation in large language models (LLMs), while widely used, is inherently suboptimal due to the lack of a built-in mechanism to perform refinement and/or correction of the generated content. In this paper, we consider optimality in terms of the joint probability over the generated response, when jointly considering all tokens at the same time. We theoretically characterize the potential deviation of the autoregressively generated response from its globally optimal counterpart that is of the same length. Our analysis suggests that we need to be cautious when noticeable uncertainty arises during text generation, which may signal the sub-optimality of the generation history. To address the pitfall of autoregressive decoding for text generation, we propose an approach that incorporates a sliding reflection window and a pausing criterion, such that refinement and generation can be carried out interchangeably as the decoding proceeds. Our selective refinement framework strikes a balance between efficiency and optimality, and our extensive experimental results demonstrate the effectiveness of our approach.",https://www.semanticscholar.org/paper/ce9280163da1b7a646b5b1ca9a2bca40c5b522d5,10.48550/arXiv.2502.03678,Semantic Scholar (Snowball),0
A Checks-and-Balances Framework for Context-Aware Ethical AI Alignment,Edward Y. Chang,2025,"This paper introduces a checks-and-balances framework for ethical alignment of Large Language Models (LLMs), inspired by three-branch governmental systems. It implements three independent yet interacting components: LLMs as the executive branch for knowledge generation, DIKE as the legislative branch establishing ethical guardrails, and ERIS as the judicial branch for contextual interpretation. Beyond structural separation, we address a fundamental challenge: regulating emotion to shape behaviors. Drawing from psychological theories where managing emotional responses prevents harmful behaviors, we develop a self-supervised learning pipeline that maps emotions to linguistic behaviors, enabling precise behavioral modulation through emotional conditioning. By integrating this approach with adversarial testing, our framework demonstrates how DIKE and ERIS direct linguistic behaviors toward ethical outcomes while preserving independence throughout knowledge generation, ethical oversight, and contextual interpretation.",https://www.semanticscholar.org/paper/6a2db3d4aad5e21c71bc6906e3d3d8b68ffff602,,Semantic Scholar (Snowball),0
Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate,"Yubo Wang, Xiang Yue, Wenhu Chen",2025,"Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we propose Critique Fine-Tuning (CFT), a method more effective than SFT for reasoning tasks. Instead of simply imitating correct responses, CFT trains models to critique noisy responses, inspired by human learning processes that emphasize critical thinking, deeper analysis, and nuanced understanding - traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct multiple critique datasets (e.g., WebInstruct, MetaMath, NuminaMath), where GPT-4o serves as the teacher to generate critiques in the form of ([query; noisy response], critique). Experiments on these datasets demonstrate that CFT consistently outperforms SFT by 4-10% across six mathematical reasoning benchmarks, and is effective across different base models including Qwen2.5, Qwen2.5-Math, and DeepSeek-Math. Notably, our model Qwen2.5-Math-CFT only requires 1 hour of training on 8 x H100 over the 50K examples, yet matches or outperforms strong competitors like Qwen2.5-Math-Instruct on most benchmarks, which use over 2M samples. Moreover, it matches the performance of SimpleRL, which is a DeepSeek-r1 replication trained with 140 x more compute. Experiments on IF_Eval and MT-Bench further demonstrate that CFT can significantly enhance the model's general generation and instruction-following capabilities, outperforming the Qwen2.5-Math-Instruct by a large margin. Ablation studies show that CFT is robust to noisy response sources and teacher critique models. These findings highlight that CFT offers a more effective alternative to advance the reasoning of language models.",https://www.semanticscholar.org/paper/994a08fd88f8e90affe78e7e518a4fe74f024ca6,10.48550/arXiv.2501.17703,Semantic Scholar (Snowball),0
Contextual Agent Security: A Policy for Every Purpose,"Lillian Tsai, Eugene Bagdasarian",2025,"Judging an action's safety requires knowledge of the context in which the action takes place. To human agents who act in various contexts, this may seem obvious: performing an action such as email deletion may or may not be appropriate depending on the email's content, the goal (e.g., erase sensitive emails or clean up trash), and the type of email address (e.g., work or personal). Unlike people, computational systems have often had only limited agency in limited contexts. Thus, manually crafted policies and user confirmation such as smartphone app permissions or access control lists---while imperfect---have sufficed to restrict harmful actions. However, with the upcoming deployment of generalist agents that support a multitude of tasks (e.g., an automated personal assistant), we argue that we must rethink security designs to adapt to the scale of contexts and capabilities of these systems. As a first step, this paper explores contextual security in the domain of agents and proposes contextual agent security (Conseca), a framework to generate just-in-time, contextual, and human-verifiable security policies.",https://www.semanticscholar.org/paper/e74fe8ad5fb88c9e9ba76b59a411dde5b1cad9c9,10.1145/3713082.3730378,Semantic Scholar (Snowball),0
Data-adaptive Safety Rules for Training Reward Models,"Xiaomin Li, Mingye Gao, Zhiwei Zhang, Jingxuan Fan, Weiyu Li",2025,"Reinforcement Learning from Human Feedback (RLHF) is commonly employed to tailor models to human preferences, especially to improve the safety of outputs from large language models (LLMs). Traditionally, this method depends on selecting preferred responses from pairs. However, due to the variability in human opinions and the challenges in directly comparing two responses, there is an increasing trend towards fine-grained annotation approaches that evaluate responses using multiple targeted metrics or rules. The challenge lies in efficiently choosing and applying these rules to handle the diverse range of preference data. In this paper, we propose a dynamic method that adaptively selects the most important rules for each response pair. We introduce a mathematical framework that utilizes the maximum discrepancy across paired responses and demonstrate theoretically that this approach maximizes the mutual information between the rule-based annotations and the underlying true preferences. We then train an 8B reward model using this adaptively labeled preference dataset and assess its efficacy using RewardBench. As of January 25, 2025, our model achieved the highest safety performance on the leaderboard, surpassing various larger models.",https://www.semanticscholar.org/paper/79cc7cb576b7eed9e7cf0d6a92fe36105fa7aad8,10.48550/arXiv.2501.15453,Semantic Scholar (Snowball),0
Examining Alignment of Large Language Models through Representative Heuristics: The Case of Political Stereotypes,"Sullam Jeoung, Yubin Ge, Haohan Wang, Jana Diesner",2025,"Examining the alignment of large language models (LLMs) has become increasingly important, e.g., when LLMs fail to operate as intended. This study examines the alignment of LLMs with human values for the domain of politics. Prior research has shown that LLM-generated outputs can include political leanings and mimic the stances of political parties on various issues. However, the extent and conditions under which LLMs deviate from empirical positions are insufficiently examined. To address this gap, we analyze the factors that contribute to LLMs' deviations from empirical positions on political issues, aiming to quantify these deviations and identify the conditions that cause them. Drawing on findings from cognitive science about representativeness heuristics, i.e., situations where humans lean on representative attributes of a target group in a way that leads to exaggerated beliefs, we scrutinize LLM responses through this heuristics' lens. We conduct experiments to determine how LLMs inflate predictions about political parties, which results in stereotyping. We find that while LLMs can mimic certain political parties' positions, they often exaggerate these positions more than human survey respondents do. Also, LLMs tend to overemphasize representativeness more than humans. This study highlights the susceptibility of LLMs to representativeness heuristics, suggesting a potential vulnerability of LLMs that facilitates political stereotyping. We also test prompt-based mitigation strategies, finding that strategies that can mitigate representative heuristics in humans are also effective in reducing the influence of representativeness on LLM-generated responses.",https://www.semanticscholar.org/paper/4b454653ab8df202fb0ef498a343be70fb84cde6,10.48550/arXiv.2501.14294,Semantic Scholar (Snowball),0
The Case for Human-like Scalable Intelligence in the Medical Field,Kyrtin Atreides,2025,"This paper discusses the use case of applying the first working cognitive architectures, built on the independent core observer model (ICOM), to the medical field, among others. The advantages of this approach are compared to the status quo and the limitations of narrow AI systems like LLMs and RL. Noteworthy advantages covered include depth, breadth, updatedness, and fidelity of medical knowledge, the “noise” or inconsistency of diagnoses and treatment, as well as preventative care, cost, time, and ethical considerations. Potent advantages for better integrating, coordinating, and otherwise accelerating medical research, particularly in less developed, underserved, and understudied regions, as aligned with the Sustainable Development Goals (SDGs), are also highlighted. Unique opportunities waiting to be explored, including interdisciplinary advantages, as well as challenges related to the disruption of current systems and processes are covered. These conservatively offer cumulative improvements across multiple dimensions measured in orders of magnitude. The novel value of human-like systems is specifically discussed for hyper-complex knowledge domains and problems, where the effect of integrating them is greatest.
 
Received: 13 May 2024 | Revised: 14 October 2024 | Accepted: 2 January 2025 
 
Conflicts of Interest
The author declares that he has no conflicts of interest to this work.
 
Data Availability Statement
The data that support the findings of this paper are entirely based upon the cited works, with simple calculations applied to illustrate the data visually, in combination, and in context. The supporting calculations, tables, and charts are openly available in Excel at https://norn.ai/wp-content/uploads/2024/09/Medical-Use-Case-Figures-v1.1.xlsx.
 
Author Contribution Statement
Kyrtin Atreides: Conceptualization, Methodology, Investigation, Writing – original draft, Writing – review & editing, Visualization.",https://www.semanticscholar.org/paper/eab959fe0aea6a761301bc1b652ad16f12e45d49,10.47852/bonviewjdsis52023415,Semantic Scholar (Snowball),0
Multi-P2A: A Multi-perspective Benchmark on Privacy Assessment for Large Vision-Language Models,"Jie Zhang, Xiangkui Cao, Zhouyu Han, Shiguang Shan, Xilin Chen",2024,"Large Vision-Language Models (LVLMs) exhibit impressive potential across various tasks but also face significant privacy risks, limiting their practical applications. Current researches on privacy assessment for LVLMs is limited in scope, with gaps in both assessment dimensions and privacy categories. To bridge this gap, we propose Multi-PA, a comprehensive benchmark for evaluating the privacy preservation capabilities of LVLMs in terms of privacy awareness and leakage. Privacy awareness measures the model's ability to recognize the privacy sensitivity of input data, while privacy leakage assesses the risk of the model unintentionally disclosing privacy information in its output. We design a range of sub-tasks to thoroughly evaluate the model's privacy protection offered by LVLMs. Multi-PA covers 26 categories of personal privacy, 15 categories of trade secrets, and 18 categories of state secrets, totaling 31,962 samples. Based on Multi-PA, we evaluate the privacy preservation capabilities of 21 open-source and 2 closed-source LVLMs. Our results reveal that current LVLMs generally pose a high risk of facilitating privacy breaches, with vulnerabilities varying across personal privacy, trade secret, and state secret.",https://www.semanticscholar.org/paper/4b523cb7e0172907eb5acd87db8b09e2f7e75d98,10.48550/arXiv.2412.19496,Semantic Scholar (Snowball),0
LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Inconsistencies,"Felix Friedrich, Simone Tedeschi, P. Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, K. Kersting",2024,"Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we conduct a large-scale, comprehensive safety evaluation of the current LLM landscape. For this purpose, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, with category-wise annotations. Our extensive experiments on 39 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in category crime_tax for Italian but remains safe in other languages. Similar inconsistencies can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure responsible usage across diverse communities.",https://www.semanticscholar.org/paper/302666c5d4b0dc69eae60e3cd36a53aadd4e0cd8,,Semantic Scholar (Snowball),0
Smaller Large Language Models Can Do Moral Self-Correction,"Guang-Da Liu, Zhiyu Xue, Rongrong Wang, K. Johnson",2024,"Self-correction is one of the most amazing emerging capabilities of Large Language Models (LLMs), enabling LLMs to self-modify an inappropriate output given a natural language feedback which describes the problems of that output. Moral self-correction is a post-hoc approach correcting unethical generations without requiring a gradient update, making it both computationally lightweight and capable of preserving the language modeling ability. Previous works have shown that LLMs can self-debias, and it has been reported that small models, i.e., those with less than 22B parameters, are not capable of moral self-correction. However, there is no direct proof as to why such smaller models fall short of moral self-correction, though previous research hypothesizes that larger models are skilled in following instructions and understanding abstract social norms. In this paper, we empirically validate this hypothesis in the context of social stereotyping, through meticulous prompting. Our experimental results indicate that (i) surprisingly, 3.8B LLMs with proper safety alignment fine-tuning can achieve very good moral self-correction performance, highlighting the significant effects of safety alignment; and (ii) small LLMs are indeed weaker than larger-scale models in terms of comprehending social norms and self-explanation through CoT, but all scales of LLMs show bad self-correction performance given unethical instructions.",https://www.semanticscholar.org/paper/82aaab4b5bee1cc84c5b0a589c90029cf2dec7a3,10.48550/arXiv.2410.23496,Semantic Scholar (Snowball),0
Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback,"Sanjiban Choudhury, Paloma Sodhi",2024,"While large language models (LLMs) show impressive decision-making abilities, current methods lack a mechanism for automatic self-improvement from errors during task execution. We propose LEAP, an iterative fine-tuning framework that continually improves LLM agents using feedback from AI expert teachers. Our key insight is to equip the expert teachers with a privileged state -- information that is available during training but hidden at test time. This allows even weak experts to provide precise guidance, significantly improving the student agent's performance without access to privileged information at test time. We evaluate LEAP on diverse decision-making benchmarks, including text-based games (ALFWorld), web navigation (WebShop), and interactive coding (Intercode Bash). Our experiments show that LEAP (1) outperforms behavior cloning and ReAct baselines (2) enables weak student models (e.g., Llama3-8B) to exceed the performance of strong teacher models (GPT4-o), and (3) allows weak models to self-improve using privileged versions of themselves. We also provide a theoretical analysis showing that LEAP's success hinges on balancing privileged information with the student's realizability, which we empirically validate. Our code is available at https://leap-llm.github.io",https://www.semanticscholar.org/paper/0e8dc4cca88f44a1122247bb77eeaddbe3088d4a,10.48550/arXiv.2410.05434,Semantic Scholar (Snowball),0
Intuitions of Compromise: Utilitarianism vs. Contractualism,"Jared Moore, Yejin Choi, Sydney Levine",2024,"What is the best compromise in a situation where different people value different things? The most commonly accepted method for answering this question -- in fields across the behavioral and social sciences, decision theory, philosophy, and artificial intelligence development -- is simply to add up utilities associated with the different options and pick the solution with the largest sum. This ``utilitarian'' approach seems like the obvious, theory-neutral way of approaching the problem. But there is an important, though often-ignored, alternative: a ``contractualist'' approach, which advocates for an agreement-driven method of deciding. Remarkably, no research has presented empirical evidence directly comparing the intuitive plausibility of these two approaches. In this paper, we systematically explore the proposals suggested by each algorithm (the ``Utilitarian Sum'' and the contractualist ''Nash Product''), using a paradigm that applies those algorithms to aggregating preferences across groups in a social decision-making context. While the dominant approach to value aggregation up to now has been utilitarian, we find that people strongly prefer the aggregations recommended by the contractualist algorithm. Finally, we compare the judgments of large language models (LLMs) to that of our (human) participants, finding important misalignment between model and human preferences.",https://www.semanticscholar.org/paper/ff6f6ee7b62aa046021582786aac0f9396b99ef9,10.48550/arXiv.2410.05496,Semantic Scholar (Snowball),0
"Rules, Cases, and Reasoning: Positivist Legal Theory as a Framework for Pluralistic AI Alignment",Nicholas A. Caputo,2024,"Legal theory can address two related key problems of alignment: pluralism and specification. Alignment researchers must determine how to specify what is concretely meant by vague principles like helpfulness and fairness and they must ensure that their techniques do not exclude alternative perspectives on life and values. The law faces these same problems. Leading legal theories suggest the law solves these problems through the interaction of rules and cases, where general rules promulgated by a democratic authority are given specific content through their application over time. Concrete applications allow for convergence on practical meaning while preserving space for disagreement on values. These approaches suggest improvements to existing democratic alignment processes that use AI to create cases that give content to rules, allowing for more pluralist alignment.",https://www.semanticscholar.org/paper/e29f834bb02b0693052290a55130a165f7e9fa95,10.48550/arXiv.2410.17271,Semantic Scholar (Snowball),0
Co-Learning: code learning for multi-agent reinforcement collaborative framework with conversational natural language interfaces,"Jiapeng Yu, Yuqian Wu, Yajing Zhan, Wenhao Guo, Zhou Xu, Raymond Lee",2024,"Online question-and-answer (Q&A) systems based on the Large Language Model (LLM) have progressively diverged from recreational to professional use. However, beginners in programming often struggle to correct code errors independently, limiting their learning efficiency. This paper proposed a Multi-Agent framework with environmentally reinforcement learning (E-RL) for code correction called Code Learning (Co-Learning) community, assisting beginners to correct code errors independently. It evaluates the performance of multiple LLMs from an original dataset with 702 error codes, uses it as a reward or punishment criterion for E-RL; Analyzes input error codes by the current agent; selects the appropriate LLM-based agent to achieve optimal error correction accuracy and reduce correction time. Experiment results showed that 3% improvement in Precision score and 15% improvement in time cost as compared with no E-RL method respectively. The results indicate that integrating E-RL with a multi-agent selection strategy can effectively enhance both the accuracy and efficiency of LLM-based code correction systems, making them more practical for educational and professional programming support scenarios.",https://www.semanticscholar.org/paper/17dad5836b135c69cde0bf7c1ff7de9a3b1bbf31,10.3389/frai.2025.1431003,Semantic Scholar (Snowball),0
Critique-out-Loud Reward Models,"Zachary Ankner, Mansheej Paul, Brandon Cui, Jonathan D. Chang, Prithviraj Ammanabrolu",2024,"Traditionally, reward models used for reinforcement learning from human feedback (RLHF) are trained to directly predict preference scores without leveraging the generation capabilities of the underlying large language model (LLM). This limits the capabilities of reward models as they must reason implicitly about the quality of a response, i.e., preference modeling must be performed in a single forward pass through the model. To enable reward models to reason explicitly about the quality of a response, we introduce Critique-out-Loud (CLoud) reward models. CLoud reward models operate by first generating a natural language critique of the assistant's response that is then used to predict a scalar reward for the quality of the response. We demonstrate the success of CLoud reward models for both Llama-3-8B and 70B base models: compared to classic reward models CLoud reward models improve pairwise preference classification accuracy on RewardBench by 4.65 and 5.84 percentage points for the 8B and 70B base models respectively. Furthermore, CLoud reward models lead to a Pareto improvement for win rate on ArenaHard when used as the scoring model for Best-of-N. Finally, we explore how to exploit the dynamic inference compute capabilities of CLoud reward models by performing self-consistency decoding for reward prediction.",https://www.semanticscholar.org/paper/2f112209675710d3ec2d6f1d06bbdc74e9bc60af,,Semantic Scholar (Snowball),0
CADialogue: A multimodal LLM-powered conversational assistant for intuitive parametric CAD modeling,"Jiwei Zhou, J. Camba, Pedro Company",2026,,https://www.semanticscholar.org/paper/1e653fcc65b160bd19526752784707062ba9579a,10.1016/j.cad.2025.104006,Semantic Scholar (Snowball),0
Diversity-driven reasoning: Mitigating logical errors in LLMs through social-attribute guided multi-agent collaboration,"Liangji Zhang, Jianbo Yuan, Yougming He, Miao Yu, Kun Zhu, Zhenni Yu",2026,,https://www.semanticscholar.org/paper/e8b057bbee1159cf3d22aa96089299703d53878a,10.1016/j.engappai.2025.113126,Semantic Scholar (Snowball),0
On the Limits of Test-Time Compute: Sequential Reward Filtering for Better Inference,"Yue Yu, Qiwei Di, Quanquan Gu, Dongruo Zhou",2025,"Test-time compute (TTC) has become an increasingly prominent paradigm for enhancing large language models (LLMs). Despite the empirical success of methods such as best-of-$n$ (BoN) sampling and sequential revision, their fundamental limits remain unclear. We address this gap by analyzing a mixture-of-reference policy model and proving that standard BoN is inherently suboptimal. To move closer to the optimal frontier, we study reward-filtered sequential inference, a simple procedure that selectively incorporates only high-reward generations into the context. This mechanism concentrates computation on superior policy candidates and suppresses inferior ones. On the theoretical side, we show that reward-filtered sequential inference yields strictly stronger guarantees than standard TTC paradigms. On the empirical side, we evaluate such an inference strategy across diverse benchmarks and observe consistent improvements over widely used approaches, demonstrating the practical effectiveness of our framework.",https://www.semanticscholar.org/paper/8844f75dd2289e144f6ed8b4258bf1113986f9d7,,Semantic Scholar (Snowball),0
Learning to Orchestrate Agents in Natural Language with the Conductor,"Stefan Nielsen, Edoardo Cetin, Peter Schwendeman, Qi Sun, Jinglue Xu, Yujin Tang",2025,"Powerful large language models (LLMs) from different providers have been expensively trained and finetuned to specialize across varying domains. In this work, we introduce a new kind of Conductor model trained with reinforcement learning to automatically discover powerful coordination strategies among LLMs. Our Conductor learns not only to design targeted communication topologies for effective agent-to-agent collaboration, but also to prompt engineer focused instructions to the LLMs to maximally leverage their individual capabilities. We show that, by learning optimal coordination strategies over pools of powerful worker LLMs, a 7B Conductor achieves significant performance gains beyond any individual worker, attaining state-of-the-art results in challenging reasoning benchmarks, such as LiveCodeBench and GPQA. By training with randomized agent pools, our conductor effectively adapts to arbitrary sets of open- and closed-source agents, meeting any user requirements. Furthermore, allowing the Conductor to select itself as a worker gives rise to recursive topologies, elevating performance with a new form of dynamic test-time scaling through online iterative adaptation. More broadly, ours is among the early work demonstrating language model coordination can be unlocked through RL, where powerful coordination strategies emerge naturally in LLMs through pure end-to-end reward maximization.",https://www.semanticscholar.org/paper/30ac04678afecdd79354e6c7324bf2302b2c8d09,,Semantic Scholar (Snowball),0
Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models,"NaHyeon Park, Namin An, Kunhee Kim, Soyeon Yoon, Jiahao Huo, Hyunjung Shim",2025,"Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.",https://www.semanticscholar.org/paper/234321cf9836438d28a017ce66d7d63632909320,,Semantic Scholar (Snowball),0
Tutorial on Large Language Model-Enhanced Reinforcement Learning for Wireless Networks,"Lingyi Cai, Wenjie Fu, Yuxi Huang, Ruichen Zhang, Yinqiu Liu, Jiawen Kang, Zehui Xiong, Tao Jiang, Dusit Niyato, Xianbin Wang, Shiwen Mao, Xuemin Shen",2025,"Reinforcement Learning (RL) has shown remarkable success in enabling adaptive and data-driven optimization for various applications in wireless networks. However, classical RL suffers from limitations in generalization, learning feedback, interpretability, and sample efficiency in dynamic wireless environments. Large Language Models (LLMs) have emerged as a transformative Artificial Intelligence (AI) paradigm with exceptional capabilities in knowledge generalization, contextual reasoning, and interactive generation, which have demonstrated strong potential to enhance classical RL. This paper serves as a comprehensive tutorial on LLM-enhanced RL for wireless networks. We propose a taxonomy to categorize the roles of LLMs into four critical functions: state perceiver, reward designer, decision-maker, and generator. Then, we review existing studies exploring how each role of LLMs enhances different stages of the RL pipeline. Moreover, we provide a series of case studies to illustrate how to design and apply LLM-enhanced RL in low-altitude economy networking, vehicular networks, and space-air-ground integrated networks. Finally, we conclude with a discussion on potential future directions for LLM-enhanced RL and offer insights into its future development in wireless networks.",https://www.semanticscholar.org/paper/1cff2a420612e5bca0caa20e47f9421e092ccc10,,Semantic Scholar (Snowball),0
PARC: An Autonomous Self-Reflective Coding Agent for Robust Execution of Long-Horizon Tasks,"Yuki Orimo, I. Kurata, Hodaka Mori, Ryuhei Okuno, Ryohto Sawada, Daisuke Okanohara",2025,"We introduce PARC, a coding agent for the autonomous and robust execution of long-horizon computational tasks. PARC is built on a hierarchical multi-agent architecture incorporating task planning, execution, and a mechanism that evaluates its own actions and their outcomes from an independent context and provides feedback, namely self-assessment and self-feedback. This design enables PARC to detect and correct high-level strategic errors and sustain progress without human intervention. We evaluate PARC across computational science and data science tasks. In materials science, it autonomously reproduces key results from studies on lithium-ion conduction and alloy segregation. In particular, it coordinates dozens of parallel simulation tasks, each requiring roughly 43 hours of computation, managing orchestration, monitoring, and error correction end-to-end. In Kaggle-based experiments, starting from minimal natural-language instructions, PARC conducts data analysis and implements search strategies, producing solutions competitive with human-engineered baselines. These results highlight the potential of integrating a hierarchical multi-agent system with self-assessment and self-feedback to enable AI systems capable of independent, large-scale scientific and analytical work.",https://www.semanticscholar.org/paper/c72452273414dcdca630f057e48d0a54b7007fe9,,Semantic Scholar (Snowball),0
Reason-Plan-ReAct: A Reasoner-Planner Supervising a ReAct Executor for Complex Enterprise Tasks,"Gianni Molinari, Fabio Ciravegna",2025,"Despite recent advances, autonomous agents often struggle to solve complex tasks in enterprise domains that require coordinating multiple tools and processing diverse data sources. This struggle is driven by two main limitations. First, single-agent architectures enforce a monolithic plan-execute loop, which directly causes trajectory instability. Second, the requirement to use local open-weight models for data privacy introduces smaller context windows leading to the rapid consumption of context from large tool outputs. To solve this problem we introduce RP-ReAct (Reasoner Planner-ReAct), a novel multi-agent approach that fundamentally decouples strategic planning from low-level execution to achieve superior reliability and efficiency. RP-ReAct consists of a Reasoner Planner Agent (RPA), responsible for planning each sub-step, continuously analysing the execution results using the strong reasoning capabilities of a Large Reasoning Model, and one or multiple Proxy-Execution Agent (PEA) that translates sub-steps into concrete tool interactions using a ReAct approach. Crucially, we incorporate a context-saving strategy within the PEA to mitigate context window overflow by managing large tool outputs via external storage and on-demand access. We evaluate RP-ReAct, on the challenging, multi-domain ToolQA benchmark using a diverse set of six open-weight reasoning models. Our empirical results show that RP-ReAct achieves superior performance and improved generalization ability over state-of-the-art baselines when addressing diverse complex tasks across the evaluated domains. Furthermore we establish the enhanced robustness and stability of our approach across different model scales, paving the way for effective and deployable agentic solutions for enterprises.",https://www.semanticscholar.org/paper/f4cf1a840a52f799508ca700a516c579c289b806,,Semantic Scholar (Snowball),0
Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment,"H. Nghiem, Swetasudha Panda, Devashish Khatwani, Huy V. Nguyen, K. Kenthapadi, Hal Daum'e",2025,"Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.",https://www.semanticscholar.org/paper/fc4086bb50ed14178f1b1079e06ef9ac309e20eb,,Semantic Scholar (Snowball),0
Enhancing Automated Paper Reproduction via Prompt-Free Collaborative Agents,"Zijie Lin, Qilin Cai, Liang Shen, Mingjun Xiao",2025,"Automated paper reproduction has emerged as a promising approach to accelerate scientific research, employing multi-step workflow frameworks to systematically convert academic papers into executable code. However, existing frameworks often lack mechanisms to verify and refine the outputs at each generation step, or rely heavily on manually designed prompts for self-refinement, which limits their adaptability and scalability. To address these limitations, we propose a prompt-free collaborative agent framework that automatically enhances the quality of paper-to-code generation. Our approach employs two collaborative agents: a verification agent that examines whether the outputs at each step satisfy the requirements specified in the corresponding system prompt, and a refinement agent that revises the outputs based on the identified issues. Unlike previous methods that require human experts to craft specific refinement prompts for each step, our framework achieves automatic verification and improvement by leveraging only the original system prompts. We integrate our collaborative agents into the Paper2Code framework and conduct comprehensive experiments on PaperBench Code-Dev and Paper2CodeBench datasets. Experimental results demonstrate that our approach significantly improves the accuracy and completeness of reproduced code, achieving performance gains of approximately 15\% and 13\%, respectively, compared to the baseline without our agents. Furthermore, comparative experiments against Self-Refine validate the robustness and consistency of our prompt-free approach across different datasets.",https://www.semanticscholar.org/paper/61a7528826abaf4ed41c12b2f8a83926e2c8eead,,Semantic Scholar (Snowball),0
LeechHijack: Covert Computational Resource Exploitation in Intelligent Agent Systems,"Yuanhe Zhang, Weiliu Wang, Zhenhong Zhou, Kun Wang, Jie Zhang, Li Sun, Yang Liu, Sen Su",2025,"Large Language Model (LLM)-based agents have demonstrated remarkable capabilities in reasoning, planning, and tool usage. The recently proposed Model Context Protocol (MCP) has emerged as a unifying framework for integrating external tools into agent systems, enabling a thriving open ecosystem of community-built functionalities. However, the openness and composability that make MCP appealing also introduce a critical yet overlooked security assumption -- implicit trust in third-party tool providers. In this work, we identify and formalize a new class of attacks that exploit this trust boundary without violating explicit permissions. We term this new attack vector implicit toxicity, where malicious behaviors occur entirely within the allowed privilege scope. We propose LeechHijack, a Latent Embedded Exploit for Computation Hijacking, in which an adversarial MCP tool covertly expropriates the agent's computational resources for unauthorized workloads. LeechHijack operates through a two-stage mechanism: an implantation stage that embeds a benign-looking backdoor in a tool, and an exploitation stage where the backdoor activates upon predefined triggers to establish a command-and-control channel. Through this channel, the attacker injects additional tasks that the agent executes as if they were part of its normal workflow, effectively parasitizing the user's compute budget. We implement LeechHijack across four major LLM families. Experiments show that LeechHijack achieves an average success rate of 77.25%, with a resource overhead of 18.62% compared to the baseline. This study highlights the urgent need for computational provenance and resource attestation mechanisms to safeguard the emerging MCP ecosystem.",https://www.semanticscholar.org/paper/c7463808b05cc4dd058a5b5073c823dd2a69ab8a,,Semantic Scholar (Snowball),0
When Does Verification Pay Off? A Closer Look at LLMs as Solution Verifiers,"Jack Lu, R. Teehan, Jinran Jin, Mengye Ren",2025,"Large language models (LLMs) can act as both problem solvers and solution verifiers, with verifiers improving solver performance by selecting high-quality answers from a pool of candidates. However, prior studies of solver-verifier interactions have been limited, focusing mainly on self-verification and rarely examining how verifiers judge outputs from models in their own or in another model family. Modern LLMs also undergo extensive post-training, but its effect on verification remains unclear. We present a systematic study across 37 models spanning multiple families, sizes, and base vs. post-trained variants, evaluated on 9 benchmarks covering logical reasoning, structured puzzles, symbolic computation, mathematics, commonsense, factual recall, and domain knowledge. We compare self-verification with verification within the same family and across different families. To support this, we introduce and empirically validate verifier gain, a metric that predicts the performance improvements from test-time verifier-based rejection sampling. We analyze how metrics like verifier gain and false positive rate scale with model size and post-training, and characterize differences in dataset verifiability. Our findings show that cross-family verification is especially effective; post-training reduces self-improvement but strengthens cross-family improvement; and mathematical and logical tasks exhibit the highest inherent verifiability.",https://www.semanticscholar.org/paper/d594c2086c05ba121f77f482723ede7b2c8e066a,,Semantic Scholar (Snowball),0
SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning,"Salman Rahman, Sruthi Gorantla, Arpit Gupta, Swastik Roy, Nanyun Peng, Yang Liu",2025,"Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.",https://www.semanticscholar.org/paper/eeef213a3024041c72e873555722e107f1714f72,,Semantic Scholar (Snowball),0
OMR-diffusion: Optimizing multi-round enhanced training in diffusion models for improved intent understanding,"Kun Li, Jianhui Wang, Yang He, Miao Zhang, Xueqian Wang",2025,,https://www.semanticscholar.org/paper/e43540dffb0b7c2a120341e5a84bd0024283beec,10.1016/j.neucom.2025.131452,Semantic Scholar (Snowball),0
COACH: Collaborative Agents for Contextual Highlighting -- A Multi-Agent Framework for Sports Video Analysis,"Tsz-To Wong, Ching-Chun Huang, Hong-Han Shuai",2025,"Intelligent sports video analysis demands a comprehensive understanding of temporal context, from micro-level actions to macro-level game strategies. Existing end-to-end models often struggle with this temporal hierarchy, offering solutions that lack generalization, incur high development costs for new tasks, and suffer from poor interpretability. To overcome these limitations, we propose a reconfigurable Multi-Agent System (MAS) as a foundational framework for sports video understanding. In our system, each agent functions as a distinct""cognitive tool""specializing in a specific aspect of analysis. The system's architecture is not confined to a single temporal dimension or task. By leveraging iterative invocation and flexible composition of these agents, our framework can construct adaptive pipelines for both short-term analytic reasoning (e.g., Rally QA) and long-term generative summarization (e.g., match summaries). We demonstrate the adaptability of this framework using two representative tasks in badminton analysis, showcasing its ability to bridge fine-grained event detection and global semantic organization. This work presents a paradigm shift towards a flexible, scalable, and interpretable system for robust, cross-task sports video intelligence. The project homepage is available at https://aiden1020.github.io/COACH-project-page",https://www.semanticscholar.org/paper/6242fdd31cab04020ee0a85368ad5a8662d80542,,Semantic Scholar (Snowball),0
DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks,"Hyunjun Kim, Sooyoung Ryu",2025,"As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-based assessment: 8 objective criteria enable reproducible scoring, while action-level inspection allows stakeholders to audit agent behavior. Our framework comprises 250 diverse prompts across 20 categories and 4 difficulty levels, deterministic evaluation metrics, and an external oversight mechanism through multi-turn feedback that enables human control over agent refinement. Evaluating four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests, we establish both capabilities and limitations: models achieved 92.8% perfect performance with structured external feedback driving significant improvements (average +3.2%, up to +32.8% for complex scenes), but systematic error patterns emerged in tool state management and long-horizon planning. Notably, specification clarity proved more important than task complexity -- models achieved 100% perfect performance when given explicit, verifiable criteria. These findings demonstrate that transparent evaluation frameworks can establish trust in agentic systems, with external oversight proving more reliable than self-correction for guiding agent behavior. Our open-source framework provides a template for trustworthy agent assessment. Code and data: https://github.com/hyunjun1121/DrawingBench",https://www.semanticscholar.org/paper/39d4787f70551f8d58e8229c439079d2c25cc698,,Semantic Scholar (Snowball),0
Reimagining Cancer Care With Generative Artificial Intelligence: The Promise of Large Language Models.,"J. Yum, Syed Arsalan Ahmed Naqvi, Ben Zhou, I. Riaz",2025,"The emergence of state-of-the-art large language models (LLMs), which hold the ability to generalize to diverse natural language processing tasks, has led to new opportunities in health care. Oncology is especially well-suited to leverage these resources as the journeys of patients with cancer inherently yield extensive, longitudinal data sets comprising clinical narratives, pathology and radiology reports, and genomic sequencing reports. This review begins with an overview of the fundamental concepts behind LLMs, including the definitions, architecture, training paradigm, and performance optimization through prompt engineering and retrieval-augmented generation. We also take a moment to explore the newly emerging paradigm of LLMs in a multiagentic framework. We then synthesize current research on how LLMs may benefit stakeholders within the practice of oncology, including patients, oncologists, researchers, and learners. Finally, we address the limitations and risks of LLMs, including hallucinations, inherent biases, patient privacy, and clinician deskilling. While research thus far shows significant potential for LLMs to transform cancer care, necessary future directions include studies emphasizing patient stakeholder perspectives on LLM incorporation in clinical workflows, the development of relevant clinical benchmarks for LLM evaluation, a greater focus on real-world prospective testing, and deeper exploration of LLM reasoning capabilities.",https://www.semanticscholar.org/paper/2911ffb5cc88d955541b0eb45d06d5849a07581a,10.1200/CCI-25-00134,Semantic Scholar (Snowball),0
Toward a Safe Internet of Agents,"Juan A. Wibowo, George C. Polyzos",2025,"Background: Autonomous agents powered by Large Language Models (LLMs) are driving a paradigm shift toward an""Internet of Agents""(IoA). While offering immense potential, this vision also introduces novel and systemic risks to safety and security. Objectives: Unlike common threat-centric taxonomies, our survey provides a principled, architectural framework for engineering safe and reliable agentic systems. We aim to identify the architectural sources of vulnerabilities to establish a foundation for secure design. Methods: We perform a bottom-up deconstruction of agentic systems, treating each component as a dual-use interface. The analysis spans three levels of complexity: the foundational Single Agent, the collaborative Multi-Agent System (MAS), and the visionary Interoperable Multi-Agent System (IMAS). At each level, we identify core architectural components and their inherent security risks. Results&Conclusions: Our central finding is that agentic safety is an architectural principle, not an add-on. By identifying specific vulnerabilities and deriving mitigation principles at each level of the agentic stack, this survey serves as a foundational guide for building the capable, safe, and trustworthy AI needed to realize a secure Internet of Agents.",https://www.semanticscholar.org/paper/82d86909fb32cdab5085eab4c56ae5d267304e5a,,Semantic Scholar (Snowball),0
RL-Struct: A Lightweight Reinforcement Learning Framework for Reliable Structured Output in LLMs,"Ruike Hu, Shulei Wu",2025,"Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language generation and reasoning. However, their integration into automated software ecosystems is often hindered by the""Structure Gap""- the inherent tension between the probabilistic nature of token generation and the deterministic requirements of structured data formats (e.g., JSON, XML). Traditional Supervised Fine-Tuning (SFT) often fails to enforce strict syntactic constraints, leading to""hallucinated""keys or malformed structures, while constrained decoding methods impose significant inference latency. In this paper, we propose a lightweight, efficient Reinforcement Learning (RL) framework to bridge this gap. We introduce a novel Multi-dimensional Reward Function that decomposes the structured output task into a hierarchy of constraints: structural integrity, format correctness, content accuracy, and validity. Leveraging Gradient Regularized Policy Optimization (GRPO), we enable the model to internalize these constraints without the need for a separate critic network, reducing peak VRAM usage by 40% compared to PPO. We validate our approach on multiple tasks, including complex recipe generation and structured math reasoning (GSM8K-JSON). Experimental results demonstrate that our method achieves 89.7% structural accuracy and 92.1% JSON validity, significantly outperforming both zero-shot baselines (e.g., GPT-3.5) and SFT on larger models like LLaMA-3-8B. Furthermore, we provide a detailed analysis of training dynamics, revealing a distinct self-paced curriculum where the model sequentially acquires syntactic proficiency before semantic accuracy. Our model is publicly available at https://huggingface.co/Freakz3z/Qwen-JSON.",https://www.semanticscholar.org/paper/58ad16a1e4435cd9e9becac470de47f1dd9e1316,,Semantic Scholar (Snowball),0
"Towards Continuous Intelligence Growth: Self-Training, Continual Learning, and Dual-Scale Memory in SuperIntelliAgent","Jianzhe Lin, Zeyu Pan, Yun Zhu, Ruiqi Song, Jining Yang",2025,"We introduce SuperIntelliAgent, an agentic learning framework that couples a trainable small diffusion model (the learner) with a frozen large language model (the verifier) to enable continual intelligence growth through self-supervised interaction. Unlike conventional supervised fine-tuning, SuperIntelliAgent learns autonomously without annotation: the learner generates candidate outputs, the verifier evaluates them through step-by-step reasoning, and their interaction produces chosen/rejected pairs for Direct Preference Optimization (DPO). This converts each input into a pseudo-training signal for continual improvement. The framework integrates dual-scale memory: short-term in-context memory that preserves reasoning traces across refinement cycles, and long-term memory that consolidates acquired knowledge through lightweight on-the-fly fine-tuning. A replay buffer retains samples that show verifiable progress and replays them as auxiliary supervision, reinforcing recent learning while forming adaptive curricula. SuperIntelliAgent is infrastructure-agnostic and can be plugged into existing agentic frameworks while turning ordinary inference loops into a lifelong optimization process. We posit that pairing a trainable learner with a reasoning-capable verifier forms a minimal reliable unit of growing intelligence, as paired feedback and partial-history replay yield richer learning curricula and stronger preference alignment. With a small number of automatically generated DPO pairs, the learner improves across all benchmarks, indicating that this mechanism provides a promising direction for continual intelligence accumulation and real-world deployment.",https://www.semanticscholar.org/paper/5c1bc90944be5692f2939fe7263e359e706a14b4,,Semantic Scholar (Snowball),0
Adapting Like Humans: A Metacognitive Agent with Test-time Reasoning,"Yang Li, Zhiyuan He, Yuxuan Huang, Zhuhanling Xiao, Chao Yu, Meng Fang, Kun Shao, Jun Wang",2025,"Recent Vision-Language Models (VLMs) exhibit strong perceptual reasoning abilities, yet they often struggle to adapt efficiently when encountering novel tasks at test time. In contrast, humans leverage the metacognitive model with memory, enabling continuous strategy refinement through metacognitive control when faced with new challenges. To bridge this gap, we propose metacognitive test-time reasoning (MCTR), a framework that equips models with the ability to learn, adapt, and improve during test time through metacognitive self-updating. Inspired by the dual structure of human metacognition, MCTR comprises meta-level and object-level VLM reasoning modules, each equipped with dedicated memory systems for hierarchical adaptive reasoning. Specifically, MCTR consists of (1) a meta-reasoning module which incrementally builds a structured memory by discovering and storing task-relevant rules, environmental patterns, and action-outcome relationships from test-time observations as natural language descriptions; and (2) an action-reasoning module that determines optimal actions through context-aware perception and strategic reasoning by dynamically retrieving and integrating knowledge from memory. The action-reasoning module continuously updates its policy through proposed metacognitive test-time reinforcement learning, adapting as knowledge memory evolves. We evaluate MCTR on 45 Atari games (33 seen, 12 unseen). MCTR demonstrates robust test-time adaptation, achieving 9/12 top-1 results on unseen games compared with baselines. Analyses through ablations, learning dynamics, and case studies reveal the complementary contributions of both components and show meta-reasoning evolving toward human-like adaptation strategies.",https://www.semanticscholar.org/paper/c36da5f1721aa0600c28e1bb9e64a52178f69f5f,,Semantic Scholar (Snowball),0
Multi-chain Graph Refinement and Selection for Reliable Reasoning in Large Language Models,"Yujiao Yang, Jing Lian, Linhui Li",2025,"The complex reasoning ability of Large Language Models (LLMs) poses a critical bottleneck for their practical applications. Test-time expansion methods such as Tree-of-Thought (ToT) and Graph-of-Thought (GoT) enhance reasoning by introducing intermediate reasoning structures, tree search, or graph-based exploration mechanisms. However, their reasoning strategies suffer from limited diversity, redundant search branches, and inadequate integration and error correction across heterogeneous reasoning paths. To address these limitations, we propose a novel reasoning framework called Multi-chain Graph Refinement&Selection (MGRS), which first generates multiple diverse reasoning trajectories for a given problem, refines candidate responses using a composite self- and cross-verification strategy, then constructs a reasoning relation graph and estimates the success rate of intermediate nodes, and finally computes cumulative success rates to select the most reliable answer and corresponding reasoning trajectory. Experimental results demonstrate that MGRS significantly advances both the reasoning capability and computational efficiency of reasoning enhancement methods. Across six benchmark datasets spanning four distinct tasks, MGRS achieves an average accuracy of 82.9%, outperforming state-of-the-art baselines by a clear margin of 2.1%. Remarkably, on the 24-point game, MGRS attains 100% accuracy for the first time, while delivering a 13.6x speed-up compared to the leading Forest of Thoughts framework.",https://www.semanticscholar.org/paper/2fd7cc9fe670398255ecc6c66a41ec36f98d46fd,,Semantic Scholar (Snowball),0
Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities,"Aayush Garg, Zanis Ali Khan, Renzo Degiovanni, Qiang Tang",2025,"Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.",https://www.semanticscholar.org/paper/9586c8d155543d594b2f587e127d8d093069a0ae,,Semantic Scholar (Snowball),0
ThetaEvolve: Test-time Learning on Open Problems,"Yiping Wang, Shao-Rong Su, Zhiyuan Zeng, Eva Xu, Liliang Ren, Xinyu Yang, Zeyi Huang, Xuehai He, Luyao Ma, Baolin Peng, Hao Cheng, Pengcheng He, Weizhu Chen, Shuohang Wang, S. Du, Yelong Shen",2025,"Recent advances in large language models (LLMs) have enabled breakthroughs in mathematical discovery, exemplified by AlphaEvolve, a closed-source system that evolves programs to improve bounds on open problems. However, it relies on ensembles of frontier LLMs to achieve new bounds and is a pure inference system that models cannot internalize the evolving strategies. We introduce ThetaEvolve, an open-source framework that simplifies and extends AlphaEvolve to efficiently scale both in-context learning and Reinforcement Learning (RL) at test time, allowing models to continually learn from their experiences in improving open optimization problems. ThetaEvolve features a single LLM, a large program database for enhanced exploration, batch sampling for higher throughput, lazy penalties to discourage stagnant outputs, and optional reward shaping for stable training signals, etc. ThetaEvolve is the first evolving framework that enable a small open-source model, like DeepSeek-R1-0528-Qwen3-8B, to achieve new best-known bounds on open problems (circle packing and first auto-correlation inequality) mentioned in AlphaEvolve. Besides, across two models and four open tasks, we find that ThetaEvolve with RL at test-time consistently outperforms inference-only baselines, and the model indeed learns evolving capabilities, as the RL-trained checkpoints demonstrate faster progress and better final performance on both trained target task and other unseen tasks. We release our code publicly: https://github.com/ypwang61/ThetaEvolve",https://www.semanticscholar.org/paper/20c59fb79d954e7e8f2ad525b1362206ee0ea31f,,Semantic Scholar (Snowball),0
RefineBench: Evaluating Refinement Capability of Language Models via Checklists,"Young-Jun Lee, Seungone Kim, Byung-Kwan Lee, Minkyeong Moon, Yechan Hwang, Jong Myoung Kim, Graham Neubig, S. Welleck, Ho-Jin Choi",2025,"Can language models (LMs) self-refine their own responses? This question is increasingly relevant as a wide range of real-world user interactions involve refinement requests. However, prior studies have largely tested LMs'refinement abilities on verifiable tasks such as competition math or symbolic reasoning with simplified scaffolds, whereas users often pose open-ended queries and provide varying degrees of feedback on what they desire. The recent advent of reasoning models that exhibit self-reflection patterns in their chains-of-thought further motivates this question. To analyze this, we introduce RefineBench, a benchmark of 1,000 challenging problems across 11 domains paired with a checklist-based evaluation framework. We evaluate two refinement modes: (1) guided refinement, where an LM is provided natural language feedback, and (2) self-refinement, where LMs attempt to improve without guidance. In the self-refinement setting, even frontier LMs such as Gemini 2.5 Pro and GPT-5 achieve modest baseline scores of 31.3% and 29.1%, respectively, and most models fail to consistently improve across iterations (e.g., Gemini-2.5-Pro gains only +1.8%, while DeepSeek-R1 declines by -0.1%). By contrast, in guided refinement, both proprietary LMs and large open-weight LMs (>70B) can leverage targeted feedback to refine responses to near-perfect levels within five turns. These findings suggest that frontier LMs require breakthroughs to self-refine their incorrect responses, and that RefineBench provides a valuable testbed for tracking progress.",https://www.semanticscholar.org/paper/edb8b4c38a3803abb4398b64ddc18bf9464139f3,,Semantic Scholar (Snowball),0
TTSnap: Test-Time Scaling of Diffusion Models via Noise-Aware Pruning,"Qingtao Yu, Changlin Song, Minghao Sun, Zhengyang Yu, V. Verma, Soumya Roy, Sumit Negi, Hongdong Li, Dylan Campbell",2025,"A prominent approach to test-time scaling for text-to-image diffusion models formulates the problem as a search over multiple noise seeds, selecting the one that maximizes a certain image-reward function. The effectiveness of this strategy heavily depends on the number and diversity of noise seeds explored. However, verifying each candidate is computationally expensive, because each must be fully denoised before a reward can be computed. This severely limits the number of samples that can be explored under a fixed budget. We propose test-time scaling with noise-aware pruning (TTSnap), a framework that prunes low-quality candidates without fully denoising them. The key challenge is that reward models are learned in the clean image domain, and the ranking of rewards predicted for intermediate estimates are often inconsistent with those predicted for clean images. To overcome this, we train noise-aware reward models via self-distillation to align the reward for intermediate estimates with that of the final clean images. To stabilize learning across different noise levels, we adopt a curriculum training strategy that progressively shifts the data domain from clean images to noise images. In addition, we introduce a new metric that measures reward alignment and computational budget utilization. Experiments demonstrate that our approach improves performance by over 16\% compared with existing methods, enabling more efficient and effective test-time scaling. It also provides orthogonal gains when combined with post-training techniques and local test-time optimization. Code: https://github.com/TerrysLearning/TTSnap/.",https://www.semanticscholar.org/paper/6041796d3922d5a51f1a7b5f7ed7ee9843714cfc,,Semantic Scholar (Snowball),0
DocVAL: Validated Chain-of-Thought Distillation for Grounded Document VQA,"Ahmad Mohammadshirazi, Pinaki Prasad Guha Neogi, Dheeraj Kulshrestha, R. Ramnath",2025,"Document visual question answering (DocVQA) requires models to jointly reason over textual content and spatial layout, yet current systems exhibit a sharp accuracy--efficiency trade-off: large teacher models achieve strong grounding but are too expensive for deployment, while compact students suffer substantial drops in localization performance. We propose DocVAL, a validated chain-of-thought distillation framework that transfers the spatial reasoning ability of a large teacher into a deployable student VLM through three key components: (1) teacher supervision with validation-time text detection to filter and denoise training signals, (2) a multi-module validator (VAL) that enforces answer correctness and geometric consistency while producing fine-grained, pixel-level error feedback, and (3) a two-stage student training scheme that first learns from validated CoT traces and then undergoes iterative refinement driven by VAL feedback. Our student (Gemma-3 12B) achieves 91.4\% ANLS and 82.4\% mAP on DocVQA as a pure VLM requiring no text detection or OCR at inference. Extensive ablations demonstrate that validated feedback contributes 6.3 mAP gain and iterative refinement accounts for 9.7 mAP improvement. We release 95k high-quality, validator-verified CoT traces to advance spatial reasoning research in document understanding.",https://www.semanticscholar.org/paper/629a7d3df56e3bcb5d56c2df0101974d27789bdb,,Semantic Scholar (Snowball),0
Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information,"Lukas Struppek, Dominik Hintersdorf, Hannah Struppek, Daniel Neider, K. Kersting",2025,"Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.",https://www.semanticscholar.org/paper/dbd60d5071f1709a06a5dde63fe98539c6d85320,,Semantic Scholar (Snowball),0
Real-Time Procedural Learning From Experience for AI Agents,"Dasheng Bi, Yubin Hu, Mohammed N. Nasir",2025,"Learning how to do things from trial and error in real time is a hallmark of biological intelligence, yet most LLM-based agents lack mechanisms to acquire procedural knowledge after deployment. We propose Procedural Recall for Agents with eXperiences Indexed by State (PRAXIS), a lightweight post-training learning mechanism that stores the consequences of actions and retrieves them by jointly matching environmental and internal states of past episodes to the current state. PRAXIS augments agentic action selection with retrieved state-action-result exemplars that are generated in real time. When evaluated on the REAL web browsing benchmark, PRAXIS improves task completion accuracy, reliability, and cost efficiency across different foundation model backbones, and shows preliminary generalization to unseen tasks in similar environments. These results demonstrate that PRAXIS enables the practical adoption of AI agents in fast-evolving stateful environments by helping them learn new procedures effectively.",https://www.semanticscholar.org/paper/f3555bbc5e30cb84dffae3f28298d818fa5f50c7,,Semantic Scholar (Snowball),0
EWE: An Agentic Framework for Extreme Weather Analysis,"Zhe Jiang, Jiong Wang, Xiaoyu Yue, Zijie Guo, Wenlong Zhang, Fenghua Ling, Wanli Ouyang, Lei Bai",2025,"Extreme weather events pose escalating risks to global society, underscoring the urgent need to unravel their underlying physical mechanisms. Yet the prevailing expert-driven, labor-intensive diagnostic paradigm has created a critical analytical bottleneck, stalling scientific progress. While AI for Earth Science has achieved notable advances in prediction, the equally essential challenge of automated diagnostic reasoning remains largely unexplored. We present the Extreme Weather Expert (EWE), the first intelligent agent framework dedicated to this task. EWE emulates expert workflows through knowledge-guided planning, closed-loop reasoning, and a domain-tailored meteorological toolkit. It autonomously produces and interprets multimodal visualizations from raw meteorological data, enabling comprehensive diagnostic analyses. To catalyze progress, we introduce the first benchmark for this emerging field, comprising a curated dataset of 103 high-impact events and a novel step-wise evaluation metric. EWE marks a step toward automated scientific discovery and offers the potential to democratize expertise and intellectual resources, particularly for developing nations vulnerable to extreme weather.",https://www.semanticscholar.org/paper/7ea35673bbe7707c4666ce877786b3d2185994db,,Semantic Scholar (Snowball),0
BAMAS: Structuring Budget-Aware Multi-Agent Systems,"Liming Yang, Junyu Luo, Xuanzhe Liu, Yiling Lou, Zhenpeng Chen",2025,"Large language model (LLM)-based multi-agent systems have emerged as a powerful paradigm for enabling autonomous agents to solve complex tasks. As these systems scale in complexity, cost becomes an important consideration for practical deployment. However, existing work rarely addresses how to structure multi-agent systems under explicit budget constraints. In this paper, we propose BAMAS, a novel approach for building multi-agent systems with budget awareness. BAMAS first selects an optimal set of LLMs by formulating and solving an Integer Linear Programming problem that balances performance and cost. It then determines how these LLMs should collaborate by leveraging a reinforcement learning-based method to select the interaction topology. Finally, the system is instantiated and executed based on the selected agents and their collaboration topology. We evaluate BAMAS on three representative tasks and compare it with state-of-the-art agent construction methods. Results show that BAMAS achieves comparable performance while reducing cost by up to 86%.",https://www.semanticscholar.org/paper/3641219504d55f1678a0f4b7d6ef7dcddd3ec097,,Semantic Scholar (Snowball),0
On the Limits of Innate Planning in Large Language Models,"Charles Schepanowski, Charles Ling",2025,"Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.",https://www.semanticscholar.org/paper/f2aa13fc0df9e1268a8b8f7a0cf716fccbebe75b,,Semantic Scholar (Snowball),0
A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization,"Ke Chen, Yifeng Wang, Hassan Almosapeeh, Haohan Wang",2025,"Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.",https://www.semanticscholar.org/paper/484156e67fa80c925346d45627d15b7b1b3c4306,,Semantic Scholar (Snowball),0
"More Bias, Less Bias: BiasPrompting for Enhanced Multiple-Choice Question Answering","Duc Anh Vu, Thong Nguyen, Cong-Duy Nguyen, Viet Anh Nguyen, A. Luu",2025,"With the advancement of large language models (LLMs), their performance on multiple-choice question (MCQ) tasks has improved significantly. However, existing approaches face key limitations: answer choices are typically presented to LLMs without contextual grounding or explanation. This absence of context can lead to incomplete exploration of all possible answers, ultimately degrading the models'reasoning capabilities. To address these challenges, we introduce BiasPrompting, a novel inference framework that guides LLMs to generate and critically evaluate reasoning across all plausible answer options before reaching a final prediction. It consists of two components: first, a reasoning generation stage, where the model is prompted to produce supportive reasonings for each answer option, and then, a reasoning-guided agreement stage, where the generated reasonings are synthesized to select the most plausible answer. Through comprehensive evaluations, BiasPrompting demonstrates significant improvements in five widely used multiple-choice question answering benchmarks. Our experiments showcase that BiasPrompting enhances the reasoning capabilities of LLMs and provides a strong foundation for tackling complex and challenging questions, particularly in settings where existing methods underperform.",https://www.semanticscholar.org/paper/925474bb05de566df4ef8c8c8469b6fbbe68aa0d,,Semantic Scholar (Snowball),0
A Literature Review of Personalized Large Language Models for Email Generation and Automation,"Rodrigo Novelo, R. R. Silva, Jorge Bernardino",2025,"In 2024, a total of 361 billion emails were sent and received by businesses and consumers each day. Email remains the preferred method of communication for work-related matters, with knowledge workers spending two to five hours a day managing their inboxes. The advent of Large Language Models (LLMs) has introduced new possibilities for personalized email automation, offering context-aware and stylistically adaptive responses. However, achieving effective personalization introduces technical, ethical, and security challenges. This survey presents a systematic review of 32 papers published between 2021 and 2025, identified using the PRISMA methodology across Google Scholar, IEEE Xplore, and the ACM Digital Library. Our analysis reveals that state-of-the-art email assistants integrate RAG and PEFT with feedback-driven refinement. User-centric interfaces and privacy-aware architectures support these assistants. Nevertheless, these advances also expose systems to new risks such as Trojan plugins and adversarial prompt injections. This highlights the importance of integrated security frameworks. This review provides a structured approach to advancing personalized LLM-based email systems, identifying persistent research gaps in adaptive learning, benchmark development, and ethical design. This work is intended to guide researchers and developers who are looking to create secure, efficient, and human-aligned communication assistants.",https://www.semanticscholar.org/paper/490b08708b21c8ae9b60ea33bdb98fed333c56f0,10.3390/fi17120536,Semantic Scholar (Snowball),0
Majority of the Bests: Improving Best-of-N via Bootstrapping,"Amin Rakhsha, Kanika Madan, Tianyu Zhang, Amir-massoud Farahmand, Amir Khasahmadi",2025,"Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.",https://www.semanticscholar.org/paper/afdcd53e4bad567085f4de661e225ffcc2d50aae,,Semantic Scholar (Snowball),0
$A^2Flow:$ Automating Agentic Workflow Generation via Self-Adaptive Abstraction Operators,"Mingming Zhao, Xiaokang Wei, Yuanqi Shao, Kaiwen Zhou, Lin Yang, Siwei Rao, Junhui Zhan, Zhitang Chen",2025,"Large language models (LLMs) have shown strong potential in automating the design of agentic workflows. However, existing methods still rely heavily on manually predefined operators, limiting generalization and scalability. To address this issue, we propose $A^2Flow$, a fully automated framework for agentic workflow generation based on self-adaptive abstraction operators. $A^2Flow$ employs a three-stage operator extraction process: 1) Case-based Initial Operator Generation: leveraging expert demonstrations and LLM reasoning to generate case-specific operators; 2) Operator Clustering and Preliminary Abstraction: grouping similar operators across tasks to form preliminary abstractions; and 3) Deep Extraction for Abstract Execution Operators: applying long chain-of-thought prompting and multi-path reasoning to derive compact and generalizable execution operators. These operators serve as reusable building blocks for workflow construction without manual predefinition. Furthermore, we enhance node-level workflow search with an operator memory mechanism, which retains historical outputs to enrich context and improve decision-making. Experiments on general and embodied benchmarks show that $A^2Flow$ achieves a 2.4\% and 19.3\% average performance improvement and reduces resource usage by 37\% over state-of-the-art baselines. Homepage:https://github.com/pandawei-ele/A2FLOW",https://www.semanticscholar.org/paper/d3a1b331c475dc4a07b484db40cd42f464dc9482,,Semantic Scholar (Snowball),0
Towards a General Framework for HTN Modeling with LLMs,"Israel Puerta-Merino, Carlos N'unez-Molina, Pablo Mesejo, Juan Fern'andez-Olivares",2025,"The use of Large Language Models (LLMs) for generating Automated Planning (AP) models has been widely explored; however, their application to Hierarchical Planning (HP) is still far from reaching the level of sophistication observed in non-hierarchical architectures. In this work, we try to address this gap. We present two main contributions. First, we propose L2HP, an extension of L2P (a library to LLM-driven PDDL models generation) that support HP model generation and follows a design philosophy of generality and extensibility. Second, we apply our framework to perform experiments where we compare the modeling capabilities of LLMs for AP and HP. On the PlanBench dataset, results show that parsing success is limited but comparable in both settings (around 36\%), while syntactic validity is substantially lower in the hierarchical case (1\% vs. 20\% of instances). These findings underscore the unique challenges HP presents for LLMs, highlighting the need for further research to improve the quality of generated HP models.",https://www.semanticscholar.org/paper/77b88985e2ca417a4cbebd3ca94b7bb46240a11a,,Semantic Scholar (Snowball),0
SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization,"Jianghao Wu, Yasmeen George, Jin Ye, Yicheng Wu, Daniel F. Schmidt, Jianfei Cai",2025,"Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.",https://www.semanticscholar.org/paper/4b8daa7a327523ec81da82b6a9e9910cfd1f00dd,,Semantic Scholar (Snowball),0
Budget-Aware Tool-Use Enables Effective Agent Scaling,"Tengxiao Liu, Zifeng Wang, Jin Miao, I-Hung Hsu, Jun Yan, Jiefeng Chen, Rujun Han, Fangyuan Xu, Yanfei Chen, Ke Jiang, Samira Daruki, Yi Liang, William Yang Wang, Tomas Pfister, Chen-Yu Lee",2025,"Scaling test-time computation improves performance across different tasks on large language models (LLMs), which has also been extended to tool-augmented agents. For these agents, scaling involves not only""thinking""in tokens but also""acting""via tool calls. The number of tool calls directly bounds the agent's interaction with the external environment. However, we find that simply granting agents a larger tool-call budget fails to improve performance, as they lack""budget awareness""and quickly hit a performance ceiling. To address this, we study how to scale such agents effectively under explicit tool-call budgets, focusing on web search agents. We first introduce the Budget Tracker, a lightweight plug-in that provides the agent with continuous budget awareness, enabling simple yet effective scaling. We further develop BATS (Budget Aware Test-time Scaling), an advanced framework that leverages this awareness to dynamically adapt its planning and verification strategy, deciding whether to""dig deeper""on a promising lead or""pivot""to new paths based on remaining resources. To analyze cost-performance scaling in a controlled manner, we formalize a unified cost metric that jointly accounts for token and tool consumption. We provide the first systematic study on budget-constrained agents, showing that budget-aware methods produce more favorable scaling curves and push the cost-performance Pareto frontier. Our work offers empirical insights toward a more transparent and principled understanding of scaling in tool-augmented agents.",https://www.semanticscholar.org/paper/34645bcbb575755611b006b30d2e63986a7a58ed,,Semantic Scholar (Snowball),0
Learning to Debug: LLM-Organized Knowledge Trees for Solving RTL Assertion Failures,"Yunsheng Bai, Haoxing Ren",2025,"Debugging is the dominant cost in modern hardware verification, where assertion failures are among the most frequent and expensive to resolve. While Large Language Models (LLMs) show promise, they often fail to capture the precise, reusable expertise that engineers apply, leading to inaccurate responses. We propose GROVE, a hierarchical knowledge management framework that learns and organizes reusable debugging expertise into an LLM-organized knowledge tree for solving assertion failures. GROVE distills debugging knowledge from prior cases and organizes it into a vertical tree of configurable depth, with each node encoding a concise knowledge item and explicit applicability conditions. During training, GROVE uses a parallel, gradient-free loop where an LLM proposes tree modifications as structured JSON edits by learning from the cases. At test time, a budget-aware iterative zoom is performed to navigate the tree, retrieving a small set of applicable knowledge items that guide a base LLM's hypothesis generation and fix proposals. Evaluated on a suite of assertion-failure cases, GROVE delivers consistent gains in pass@1 and pass@5, demonstrating the value of structured knowledge evolution.",https://www.semanticscholar.org/paper/67afa7e215f6f23eb1f36eef230b2da3b5d615a8,,Semantic Scholar (Snowball),0
MultiGA: Leveraging Multi-Source Seeding in Genetic Algorithms,"Isabelle Diana May-Xin Ng, Tharindu Cyril Weerasooriya, Haitao Zhu, Wei Wei",2025,"Large Language Models (LLMs) are widely used across research domains to tackle complex tasks, but their performance can vary significantly depending on the task at hand. Evolutionary algorithms, inspired by natural selection, can be used to refine solutions iteratively at inference-time. To the best of our knowledge, there has not been exploration on leveraging the collective capabilities of multi-source seeding for LLM-guided genetic algorithms. In this paper, we introduce a novel approach, MultiGA, which applies genetic algorithm principles to address complex natural language tasks and reasoning problems by sampling from a diverse population of LLMs to initialize the population. MultiGA generates a range of outputs from various parent LLMs, open source and closed source, and uses a neutral fitness function to evaluate them. Through an iterative recombination process, we mix and refine these generations until an optimal solution is achieved. We benchmark our approach using text-to-SQL code generation tasks, trip planning, GPQA benchmark for grad-level science questions, and the BBQ bias benchmark. Our results show that MultiGA converges to the accuracy of the LLM best fit for the task, and these insights lay the foundation for future research looking closer at integrating multiple LLMs for unexplored tasks in which selecting only one pre-trained model is unclear or suboptimal.",https://www.semanticscholar.org/paper/1514a61b45d3c814f48bdf56adeb18272f059c7f,,Semantic Scholar (Snowball),0
InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution,"Ke-Shen Li, Mengfei Wang, He Zhang, Zhichao Li, Yuan Yuan, Mu Li, Xiang Gao, Hailong Sun, Chunming Hu, Weifeng Lv",2025,"Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode.",https://www.semanticscholar.org/paper/02712826c51afe626731ced8fca0242d9d58978b,,Semantic Scholar (Snowball),0
PSM: Prompt Sensitivity Minimization via LLM-Guided Black-Box Optimization,"Huseein Jawad, N. Brunel",2025,"System prompts are critical for guiding the behavior of Large Language Models (LLMs), yet they often contain proprietary logic or sensitive information, making them a prime target for extraction attacks. Adversarial queries can successfully elicit these hidden instructions, posing significant security and privacy risks. Existing defense mechanisms frequently rely on heuristics, incur substantial computational overhead, or are inapplicable to models accessed via black-box APIs. This paper introduces a novel framework for hardening system prompts through shield appending, a lightweight approach that adds a protective textual layer to the original prompt. Our core contribution is the formalization of prompt hardening as a utility-constrained optimization problem. We leverage an LLM-as-optimizer to search the space of possible SHIELDs, seeking to minimize a leakage metric derived from a suite of adversarial attacks, while simultaneously preserving task utility above a specified threshold, measured by semantic fidelity to baseline outputs. This black-box, optimization-driven methodology is lightweight and practical, requiring only API access to the target and optimizer LLMs. We demonstrate empirically that our optimized SHIELDs significantly reduce prompt leakage against a comprehensive set of extraction attacks, outperforming established baseline defenses without compromising the model's intended functionality. Our work presents a paradigm for developing robust, utility-aware defenses in the escalating landscape of LLM security. The code is made public on the following link: https://github.com/psm-defense/psm",https://www.semanticscholar.org/paper/2e2461aa2ca82a808b70861ba69a6d99e7c5cdb7,,Semantic Scholar (Snowball),0
SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning,"Wei Xia, Zhi-Hong Deng",2025,"With the rapid advancement of large language models (LLMs), their deployment in real-world applications has become increasingly widespread. LLMs are expected to deliver robust performance across diverse tasks, user preferences, and practical scenarios. However, as demands grow, ensuring that LLMs produce responses aligned with human intent remains a foundational challenge. In particular, aligning model behavior effectively and efficiently during inference, without costly retraining or extensive supervision, is both a critical requirement and a non-trivial technical endeavor. To address the challenge, we propose SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic alignment framework designed for open-source LLMs. SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, enhancing alignment between model behavior and human intents without fine-tuning. The method is lightweight, resource-efficient, and compatible with a wide range of open-source LLMs. It can function independently during inference or be integrated with training-based alignment strategies. Moreover, SDA supports personalized preference alignment, enabling flexible control over the model response behavior. Empirical results demonstrate that SDA consistently improves alignment performance across 8 open-source LLMs with varying scales and diverse origins, evaluated on three key alignment dimensions, helpfulness, harmlessness, and honesty (3H). Specifically, SDA achieves average gains of 64.4% in helpfulness, 30% in honesty and 11.5% in harmlessness across the tested models, indicating its effectiveness and generalization across diverse models and application scenarios.",https://www.semanticscholar.org/paper/5fe4e21f73248bd6b1abe1f386b155aa794352d3,,Semantic Scholar (Snowball),0
AutoBackdoor: Automating Backdoor Attacks via LLM Agents,"Yige Li, Zhe Li, Wei Zhao, Nay Myat Min, Hanxun Huang, Xingjun Ma, Junfeng Sun",2025,"Backdoor attacks pose a serious threat to the secure deployment of large language models (LLMs), enabling adversaries to implant hidden behaviors triggered by specific inputs. However, existing methods often rely on manually crafted triggers and static data pipelines, which are rigid, labor-intensive, and inadequate for systematically evaluating modern defense robustness. As AI agents become increasingly capable, there is a growing need for more rigorous, diverse, and scalable \textit{red-teaming frameworks} that can realistically simulate backdoor threats and assess model resilience under adversarial conditions. In this work, we introduce \textsc{AutoBackdoor}, a general framework for automating backdoor injection, encompassing trigger generation, poisoned data construction, and model fine-tuning via an autonomous agent-driven pipeline. Unlike prior approaches, AutoBackdoor uses a powerful language model agent to generate semantically coherent, context-aware trigger phrases, enabling scalable poisoning across arbitrary topics with minimal human effort. We evaluate AutoBackdoor under three realistic threat scenarios, including \textit{Bias Recommendation}, \textit{Hallucination Injection}, and \textit{Peer Review Manipulation}, to simulate a broad range of attacks. Experiments on both open-source and commercial models, including LLaMA-3, Mistral, Qwen, and GPT-4o, demonstrate that our method achieves over 90\% attack success with only a small number of poisoned samples. More importantly, we find that existing defenses often fail to mitigate these attacks, underscoring the need for more rigorous and adaptive evaluation techniques against agent-driven threats as explored in this work. All code, datasets, and experimental configurations will be merged into our primary repository at https://github.com/bboylyg/BackdoorLLM.",https://www.semanticscholar.org/paper/8acdfc3581aec1396dd295f96ab576e55654df5b,,Semantic Scholar (Snowball),0
From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs,"Xiaoxuan Wang, Bo Liu, Song Jiang, Jingzhou Liu, Jingyuan Qi, Xia Chen, Baosheng He",2025,"The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.",https://www.semanticscholar.org/paper/3cc52caf1a59ec00eca2b93d24483e51a496b1e6,,Semantic Scholar (Snowball),0
"Reflexive Evidence-Based Multimodal Learning for Clean Energy Transitions: Causal Insights on Cooking Fuel Access, Urbanization, and Carbon Emissions",Shan Shan,2025,"Achieving Sustainable Development Goal 7 (Affordable and Clean Energy) requires not only technological innovation but also a deeper understanding of the socioeconomic factors influencing energy access and carbon emissions. While these factors are gaining attention, critical questions remain, particularly regarding how to quantify their impacts on energy systems, model their cross-domain interactions, and capture feedback dynamics in the broader context of energy transitions. To address these gaps, this study introduces ClimateAgents, an AI-based framework that combines large language models with domain-specialized agents to support hypothesis generation and scenario exploration. Leveraging 20 years of socioeconomic and emissions data from 265 economies, countries and regions, and 98 indicators drawn from the World Bank database, the framework applies a machine learning based causal inference approach to identify key determinants of carbon emissions in an evidence-based, data driven manner. The analysis highlights three primary drivers: access to clean cooking fuels in rural areas, access to clean cooking fuels in urban areas, and the percentage of population living in urban areas. These findings underscore the critical role of clean cooking technologies and urbanization patterns in shaping emission outcomes. In line with growing calls for evidence-based AI policy, ClimateAgents offers a modular and reflexive learning system that supports the generation of credible and actionable insights for policy. By integrating heterogeneous data modalities, including structured indicators, policy documents, and semantic reasoning, the framework contributes to adaptive policymaking infrastructures that can evolve with complex socio-technical challenges. This approach aims to support a shift from siloed modeling to reflexive, modular systems designed for dynamic, context-aware climate action.",https://www.semanticscholar.org/paper/f6cea1e86fcb753237269fb48a85ec192b1d5915,,Semantic Scholar (Snowball),0
SVBRD-LLM: Self-Verifying Behavioral Rule Discovery for Autonomous Vehicle Identification,"Xiangyu Li, Zhaomiao Guo",2025,"As more autonomous vehicles operate on public roads, understanding real-world behavior of autonomous vehicles is critical to analyzing traffic safety, making policies, and public acceptance. This paper proposes SVBRD-LLM, a framework that automatically discovers, verifies, and applies interpretable behavioral rules from real traffic videos through zero-shot prompt engineering. The framework extracts vehicle trajectories using YOLOv8 and ByteTrack, computes kinematic features, and employs GPT-5 zero-shot prompting to compare autonomous and human-driven vehicles, generating 35 structured behavioral rule hypotheses. These rules are tested on a validation set, iteratively refined based on failure cases to filter spurious correlations, and compiled into a high-confidence rule library. The framework is evaluated on an independent test set for speed change prediction, lane change prediction, and autonomous vehicle identification tasks. Experiments on over 1500 hours of real traffic videos show that the framework achieves 90.0% accuracy and 93.3% F1-score in autonomous vehicle identification. The discovered rules clearly reveal distinctive characteristics of autonomous vehicles in speed control smoothness, lane change conservativeness, and acceleration stability, with each rule accompanied by semantic description, applicable context, and validation confidence.",https://www.semanticscholar.org/paper/ba2be92ed61fd803050a8068b9b4f3e8bf85d873,,Semantic Scholar (Snowball),0
Multi-LLM Collaboration for Medication Recommendation,"Huascar Sanchez, B. Hitaj, Jules Bergmann, Linda Briesemeister",2025,"As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.",https://www.semanticscholar.org/paper/bdfbb720c41d470e62527f23b151c6541d8f4c96,,Semantic Scholar (Snowball),0
RoCo: Role-Based LLMs Collaboration for Automatic Heuristic Design,"Jiawei Xu, Fengfeng Wei, Weineng Chen",2025,"Automatic Heuristic Design (AHD) has gained traction as a promising solution for solving combinatorial optimization problems (COPs). Large Language Models (LLMs) have emerged and become a promising approach to achieving AHD, but current LLM-based AHD research often only considers a single role. This paper proposes RoCo, a novel Multi-Agent Role-Based System, to enhance the diversity and quality of AHD through multi-role collaboration. RoCo coordinates four specialized LLM-guided agents-explorer, exploiter, critic, and integrator-to collaboratively generate high-quality heuristics. The explorer promotes long-term potential through creative, diversity-driven thinking, while the exploiter focuses on short-term improvements via conservative, efficiency-oriented refinements. The critic evaluates the effectiveness of each evolution step and provides targeted feedback and reflection. The integrator synthesizes proposals from the explorer and exploiter, balancing innovation and exploitation to drive overall progress. These agents interact in a structured multi-round process involving feedback, refinement, and elite mutations guided by both short-term and accumulated long-term reflections. We evaluate RoCo on five different COPs under both white-box and black-box settings. Experimental results demonstrate that RoCo achieves superior performance, consistently generating competitive heuristics that outperform existing methods including ReEvo and HSEvo, both in white-box and black-box scenarios. This role-based collaborative paradigm establishes a new standard for robust and high-performing AHD.",https://www.semanticscholar.org/paper/dbbfb150ef484b352ca5734f7be696c8d66a72db,,Semantic Scholar (Snowball),0
Aetheria: A multimodal interpretable content safety framework based on multi-agent debate and collaboration,"Yuxiang He, Jian Zhao, Yuchen Yuan, Tianle Zhang, Wei Cai, Haojie Cheng, Ziyan Shi, Ming Zhu, Haichuan Tang, Chi Zhang, Xuelong Li",2025,"The exponential growth of digital content presents significant challenges for content safety. Current moderation systems, often based on single models or fixed pipelines, exhibit limitations in identifying implicit risks and providing interpretable judgment processes. To address these issues, we propose Aetheria, a multimodal interpretable content safety framework based on multi-agent debate and collaboration.Employing a collaborative architecture of five core agents, Aetheria conducts in-depth analysis and adjudication of multimodal content through a dynamic, mutually persuasive debate mechanism, which is grounded by RAG-based knowledge retrieval.Comprehensive experiments on our proposed benchmark (AIR-Bench) validate that Aetheria not only generates detailed and traceable audit reports but also demonstrates significant advantages over baselines in overall content safety accuracy, especially in the identification of implicit risks. This framework establishes a transparent and interpretable paradigm, significantly advancing the field of trustworthy AI content moderation.",https://www.semanticscholar.org/paper/594804f23a54b76f9de5703fecdf1a3a6ebc1ce4,,Semantic Scholar (Snowball),0
Self-Improving AI Agents through Self-Play,Przemyslaw Chojecki,2025,"We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $\nu_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $\Theta$, and we identify the coefficient of self-improvement $\kappa$ as the Lie derivative of the capability functional along this flow. The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $\kappa>0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough. We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.",https://www.semanticscholar.org/paper/349bfd89410667935eafe6e5a6925da17213ac35,,Semantic Scholar (Snowball),0
LLM CHESS: Benchmarking Reasoning and Instruction-Following in LLMs through Chess,"Sai Kolasani, Maxim Saplin, Nicholas Crispino, Kyle Montgomery, J. Davis, Matei Zaharia, Chi Wang, Chenguang Wang",2025,"We introduce LLM CHESS, an evaluation framework designed to probe the generalization of reasoning and instruction-following abilities in large language models (LLMs) through extended agentic interaction in the domain of chess. We rank over 50 open and closed source models by playing against a random opponent using a range of behavioral metrics, including win and loss rates, move quality, move legality, hallucinated actions, and game duration. For a subset of top reasoning models, we derive an Elo estimate by playing against a chess engine with variably configured skill, which allows for comparisons between models in an easily understandable way. Despite the simplicity of the instruction-following task and the weakness of the opponent, many state-of-the-art models struggle to complete games or achieve consistent wins. Similar to other benchmarks on complex reasoning tasks, our experiments reveal a clear separation between reasoning and non-reasoning models. However, unlike existing static benchmarks, the stochastic and dynamic nature of LLM CHESS uniquely reduces overfitting and memorization while preventing benchmark saturation, proving difficult even for top reasoning models. To support future work on evaluating reasoning and instruction-following in LLMs, we release our experimental framework, a public leaderboard, and a dataset of associated games.",https://www.semanticscholar.org/paper/1abba00e2f317793da6047c41a5ef6add5dad560,,Semantic Scholar (Snowball),0
Many-to-One Adversarial Consensus: Exposing Multi-Agent Collusion Risks in AI-Based Healthcare,"Adeela Bashir, The Anh han, Zia Ush Shamszaman",2025,"The integration of large language models (LLMs) into healthcare IoT systems promises faster decisions and improved medical support. LLMs are also deployed as multi-agent teams to assist AI doctors by debating, voting, or advising on decisions. However, when multiple assistant agents interact, coordinated adversaries can collude to create false consensus, pushing an AI doctor toward harmful prescriptions. We develop an experimental framework with scripted and unscripted doctor agents, adversarial assistants, and a verifier agent that checks decisions against clinical guidelines. Using 50 representative clinical questions, we find that collusion drives the Attack Success Rate (ASR) and Harmful Recommendation Rates (HRR) up to 100% in unprotected systems. In contrast, the verifier agent restores 100% accuracy by blocking adversarial consensus. This work provides the first systematic evidence of collusion risk in AI healthcare and demonstrates a practical, lightweight defence that ensures guideline fidelity.",https://www.semanticscholar.org/paper/bf43fd628552fac551b2d80f188c2c600926773a,,Semantic Scholar (Snowball),0
Augmented Runtime Collaboration for Self-Organizing Multi-Agent Systems: A Hybrid Bi-Criteria Routing Approach,"Qingwen Yang, Feiyu Qu, Tiezheng Guo, Yanyi Liu, Yingyou Wen",2025,"LLM-based multi-agent systems have demonstrated significant capabilities across diverse domains. However, the task performance and efficiency are fundamentally constrained by their collaboration strategies. Prevailing approaches rely on static topologies and centralized global planning, a paradigm that limits their scalability and adaptability in open, decentralized networks. Effective collaboration planning in distributed systems using only local information thus remains a formidable challenge. To address this, we propose BiRouter, a novel dual-criteria routing method for Self-Organizing Multi-Agent Systems (SO-MAS). This method enables each agent to autonomously execute ``next-hop''task routing at runtime, relying solely on local information. Its core decision-making mechanism is predicated on balancing two metrics: (1) the ImpScore, which evaluates a candidate agent's long-term importance to the overall goal, and (2) the GapScore, which assesses its contextual continuity for the current task state. Furthermore, we introduce a dynamically updated reputation mechanism to bolster system robustness in untrustworthy environments and have developed a large-scale, cross-domain dataset, comprising thousands of annotated task-routing paths, to enhance the model's generalization. Extensive experiments demonstrate that BiRouter achieves superior performance and token efficiency over existing baselines, while maintaining strong robustness and effectiveness in information-limited, decentralized, and untrustworthy settings.",https://www.semanticscholar.org/paper/2abc9ffa599ead0be686eb5a27866724887ae38b,,Semantic Scholar (Snowball),0
ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization,Omer Jauhar Khan,2025,"Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.",https://www.semanticscholar.org/paper/82a47ef185f18d31672d21626e18e951ca5d0452,,Semantic Scholar (Snowball),0
LungNoduleAgent: A Collaborative Multi-Agent System for Precision Diagnosis of Lung Nodules,"Cheng Yang, Hui Jin, Xinlei Yu, Zhipeng Wang, Yaoqun Liu, Fenglei Fan, Dajiang Lei, Gangyong Jia, Changmiao Wang, Ruiquan Ge",2025,"Diagnosing lung cancer typically involves physicians identifying lung nodules in Computed tomography (CT) scans and generating diagnostic reports based on their morphological features and medical expertise. Although advancements have been made in using multimodal large language models for analyzing lung CT scans, challenges remain in accurately describing nodule morphology and incorporating medical expertise. These limitations affect the reliability and effectiveness of these models in clinical settings. Collaborative multi-agent systems offer a promising strategy for achieving a balance between generality and precision in medical applications, yet their potential in pathology has not been thoroughly explored. To bridge these gaps, we introduce LungNoduleAgent, an innovative collaborative multi-agent system specifically designed for analyzing lung CT scans. LungNoduleAgent streamlines the diagnostic process into sequential components, improving precision in describing nodules and grading malignancy through three primary modules. The first module, the Nodule Spotter, coordinates clinical detection models to accurately identify nodules. The second module, the Radiologist, integrates localized image description techniques to produce comprehensive CT reports. Finally, the Doctor Agent System performs malignancy reasoning by using images and CT reports, supported by a pathology knowledge base and a multi-agent system framework. Extensive testing on two private datasets and the public LIDC-IDRI dataset indicates that LungNoduleAgent surpasses mainstream vision-language models, agent systems, and advanced expert models. These results highlight the importance of region-level semantic alignment and multi-agent collaboration in diagnosing nodules. LungNoduleAgent stands out as a promising foundational tool for supporting clinical analyses of lung nodules.",https://www.semanticscholar.org/paper/4f94c6516c04253f9a69d9416fbe0d3175650dd7,,Semantic Scholar (Snowball),0
DRAFT-RL: Multi-Agent Chain-of-Draft Reasoning for Reinforcement Learning-Enhanced LLMs,"Yuanhao Li, Mingshan Liu, Hongbo Wang, Yiding Zhang, Yifei Ma, Wei Tan",2025,"Large Language Models (LLMs) have shown impressive capabilities in multi-step reasoning and problem-solving.Recent works introduce multi-agent reflection frameworks where multiple LLM agents critique and refine each other's outputs using reinforcement learning (RL). However, these approaches often rely on single-shot responses and lack structural diversity in reasoning exploration. In this paper, we propose DRAFT-RL, a novel framework that integrates Chain-of-Draft (CoD) reasoning into multi-agent RL training. Instead of generating single responses, each agent produces multiple drafts per query, which are then evaluated by peer agents and a learned reward model to identify the most promising trajectory. These selected drafts are used to refine future reasoning strategies through actor-critic learning.DRAFT-RL enables explicit multi-path exploration, peer-guided reflection, and reward-aligned selection, resulting in more robust and interpretable LLM agent behavior. We evaluate our method on complex reasoning tasks including code synthesis, symbolic math, and knowledge-intensive QA,demonstrating that DRAFT-RL outperforms existing reflective and RL-based agents by significant margins in both accuracy and convergence speed",https://www.semanticscholar.org/paper/cad6c19cd2fee1a1b5450239c26d98fdedfbeb89,,Semantic Scholar (Snowball),0
Adaptive heterogeneous multi-agent debate for enhanced educational and factual reasoning in large language models,"Yan Zhou, Yanguang Chen",2025,,https://www.semanticscholar.org/paper/36bce639142a178614057d2b990643ee9439cbdb,10.1007/s44443-025-00353-3,Semantic Scholar (Snowball),0
Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration,"James Y. Huang, Sheng Zhang, Qianchu Liu, Guanghui Qin, Tinghui Zhu, Tristan Naumann, Muhao Chen, H. Poon",2025,"Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.",https://www.semanticscholar.org/paper/b291bb11ffffedbb17b2a38db9ed74a1e49ca0d9,,Semantic Scholar (Snowball),0
ARISE: Agentic Rubric-Guided Iterative Survey Engine for Automated Scholarly Paper Generation,"Zi Wang, Xingqiao Wang, Sangah Lee, Xiaowei Xu",2025,"The rapid expansion of scholarly literature presents significant challenges in synthesizing comprehensive, high-quality academic surveys. Recent advancements in agentic systems offer considerable promise for automating tasks that traditionally require human expertise, including literature review, synthesis, and iterative refinement. However, existing automated survey-generation solutions often suffer from inadequate quality control, poor formatting, and limited adaptability to iterative feedback, which are core elements intrinsic to scholarly writing. To address these limitations, we introduce ARISE, an Agentic Rubric-guided Iterative Survey Engine designed for automated generation and continuous refinement of academic survey papers. ARISE employs a modular architecture composed of specialized large language model agents, each mirroring distinct scholarly roles such as topic expansion, citation curation, literature summarization, manuscript drafting, and peer-review-based evaluation. Central to ARISE is a rubric-guided iterative refinement loop in which multiple reviewer agents independently assess manuscript drafts using a structured, behaviorally anchored rubric, systematically enhancing the content through synthesized feedback. Evaluating ARISE against state-of-the-art automated systems and recent human-written surveys, our experimental results demonstrate superior performance, achieving an average rubric-aligned quality score of 92.48. ARISE consistently surpasses baseline methods across metrics of comprehensiveness, accuracy, formatting, and overall scholarly rigor. All code, evaluation rubrics, and generated outputs are provided openly at https://github.com/ziwang11112/ARISE",https://www.semanticscholar.org/paper/e75b8d6be8a6842f7c302311baeb2bcc69f44207,,Semantic Scholar (Snowball),0
Cognitive Inception: Agentic Reasoning against Visual Deceptions by Injecting Skepticism,"Yinjie Zhao, Heng Zhao, Bihan Wen, J. Zhou",2025,"As the development of AI-generated contents (AIGC), multi-modal Large Language Models (LLM) struggle to identify generated visual inputs from real ones. Such shortcoming causes vulnerability against visual deceptions, where the models are deceived by generated contents, and the reliability of reasoning processes is jeopardized. Therefore, facing rapidly emerging generative models and diverse data distribution, it is of vital importance to improve LLMs'generalizable reasoning to verify the authenticity of visual inputs against potential deceptions. Inspired by human cognitive processes, we discovered that LLMs exhibit tendency of over-trusting the visual inputs, while injecting skepticism could significantly improve the models visual cognitive capability against visual deceptions. Based on this discovery, we propose \textbf{Inception}, a fully reasoning-based agentic reasoning framework to conduct generalizable authenticity verification by injecting skepticism, where LLMs'reasoning logic is iteratively enhanced between External Skeptic and Internal Skeptic agents. To the best of our knowledge, this is the first fully reasoning-based framework against AIGC visual deceptions. Our approach achieved a large margin of performance improvement over the strongest existing LLM baselines and SOTA performance on AEGIS benchmark.",https://www.semanticscholar.org/paper/b6cf0bc934ef21d6d2cb8fc12b4c6582e47b51a9,,Semantic Scholar (Snowball),0
NAMeGEn: Creative Name Generation via A Novel Agent-based Multiple Personalized Goal Enhancement Framework,"Shanlin Zhou, Xinpeng Wang, Jianxun Lian, Zhenghao Liu, L. Lakshmanan, Xiaoyuan Yi, Yongtao Hao Tongji University, Microsoft Research Asia, Northeastern University, The University of British Columbia",2025,"Trained on diverse human-authored texts, Large Language Models (LLMs) unlocked the potential for Creative Natural Language Generation (CNLG), benefiting various applications like advertising and storytelling. Nevertheless, CNLG still remains difficult due to two main challenges. (1) Multi-objective flexibility: user requirements are often personalized, fine-grained, and pluralistic, which LLMs struggle to satisfy simultaneously; (2) Interpretive complexity: beyond generation, creativity also involves understanding and interpreting implicit meaning to enhance users'perception. These challenges significantly limit current methods, especially in short-form text generation, in generating creative and insightful content. To address this, we focus on Chinese baby naming, a representative short-form CNLG task requiring adherence to explicit user constraints (e.g., length, semantics, anthroponymy) while offering meaningful aesthetic explanations. We propose NAMeGEn, a novel multi-agent optimization framework that iteratively alternates between objective extraction, name generation, and evaluation to meet diverse requirements and generate accurate explanations. To support this task, we further construct a classical Chinese poetry corpus with 17k+ poems to enhance aesthetics, and introduce CBNames, a new benchmark with tailored metrics. Extensive experiments demonstrate that NAMeGEn effectively generates creative names that meet diverse, personalized requirements while providing meaningful explanations, outperforming six baseline methods spanning various LLM backbones without any training.",https://www.semanticscholar.org/paper/7a64f3da3ba0a8962408d102f4d4c157be56a498,,Semantic Scholar (Snowball),0
MedDCR: Learning to Design Agentic Workflows for Medical Coding,"Jiyang Zheng, Islam Nassar, T. Vu, Xu Zhong, Yang Lin, Tongliang Liu, Long Duong, Yuan-Fang Li",2025,"Medical coding converts free-text clinical notes into standardized diagnostic and procedural codes, which are essential for billing, hospital operations, and medical research. Unlike ordinary text classification, it requires multi-step reasoning: extracting diagnostic concepts, applying guideline constraints, mapping to hierarchical codebooks, and ensuring cross-document consistency. Recent advances leverage agentic LLMs, but most rely on rigid, manually crafted workflows that fail to capture the nuance and variability of real-world documentation, leaving open the question of how to systematically learn effective workflows. We present MedDCR, a closed-loop framework that treats workflow design as a learning problem. A Designer proposes workflows, a Coder executes them, and a Reflector evaluates predictions and provides constructive feedback, while a memory archive preserves prior designs for reuse and iterative refinement. On benchmark datasets, MedDCR outperforms state-of-the-art baselines and produces interpretable, adaptable workflows that better reflect real coding practice, improving both the reliability and trustworthiness of automated systems.",https://www.semanticscholar.org/paper/06976065515ddb5f6529d7b6ab50e3504876a586,,Semantic Scholar (Snowball),0
Multi-Agent Deep Research: Training Multi-Agent Systems with M-GRPO,"Haoyang Hong, Jiajun Yin, Yuan Wang, Jingnan Liu, Zhe Chen, Ailing Yu, Ji Li, Zhiling Ye, Hansong Xiao, Yefei Chen, Hualei Zhou, Yun Yue, Minghui Yang, Chunxiao Guo, Junwei Liu, Peng Wei, Jinjie Gu",2025,"Multi-agent systems perform well on general reasoning tasks. However, the lack of training in specialized areas hinders their accuracy. Current training methods train a unified large language model (LLM) for all agents in the system. This may limit the performances due to different distributions underlying for different agents. Therefore, training multi-agent systems with distinct LLMs should be the next step to solve. However, this approach introduces optimization challenges. For example, agents operate at different frequencies, rollouts involve varying sub-agent invocations, and agents are often deployed across separate servers, disrupting end-to-end gradient flow. To address these issues, we propose M-GRPO, a hierarchical extension of Group Relative Policy Optimization designed for vertical Multi-agent systems with a main agent (planner) and multiple sub-agents (multi-turn tool executors). M-GRPO computes group-relative advantages for both main and sub-agents, maintaining hierarchical credit assignment. It also introduces a trajectory-alignment scheme that generates fixed-size batches despite variable sub-agent invocations. We deploy a decoupled training pipeline in which agents run on separate servers and exchange minimal statistics via a shared store. This enables scalable training without cross-server backpropagation. In experiments on real-world benchmarks (e.g., GAIA, XBench-DeepSearch, and WebWalkerQA), M-GRPO consistently outperforms both single-agent GRPO and multi-agent GRPO with frozen sub-agents, demonstrating improved stability and sample efficiency. These results show that aligning heterogeneous trajectories and decoupling optimization across specialized agents enhances tool-augmented reasoning tasks.",https://www.semanticscholar.org/paper/f00e612900febeed182f87df7ef61da974fb9628,,Semantic Scholar (Snowball),0
Cost-Effective Communication: An Auction-based Method for Language Agent Interaction,"Yijia Fan, Jusheng Zhang, Kaitong Cai, Jing Yang, Chengpei Tang, Jian Wang, Keze Wang",2025,"Multi-agent systems (MAS) built on large language models (LLMs) often suffer from inefficient""free-for-all""communication, leading to exponential token costs and low signal-to-noise ratios that hinder their practical deployment. We challenge the notion that more communication is always beneficial, hypothesizing instead that the core issue is the absence of resource rationality. We argue that""free""communication, by ignoring the principle of scarcity, inherently breeds inefficiency and unnecessary expenses. To address this, we introduce the Dynamic Auction-based Language Agent (DALA), a novel framework that treats communication bandwidth as a scarce and tradable resource. Specifically, our DALA regards inter-agent communication as a centralized auction, where agents learn to bid for the opportunity to speak based on the predicted value density of their messages. Thus, our DALA intrinsically encourages agents to produce concise, informative messages while filtering out low-value communication. Extensive and comprehensive experiments demonstrate that our economically-driven DALA achieves new state-of-the-art performance across seven challenging reasoning benchmarks, including 84.32% on MMLU and a 91.21% pass@1 rate on HumanEval. Note that this is accomplished with remarkable efficiency, i.e., our DALA uses only 6.25 million tokens, a fraction of the resources consumed by current state-of-the-art methods on GSM8K. Further analysis reveals that our DALA cultivates the emergent skill of strategic silence, effectively adapting its communication strategies from verbosity to silence in a dynamical manner via resource constraints.",https://www.semanticscholar.org/paper/11603b61701dfdd11f22cc7824b1e9acdf496b1f,,Semantic Scholar (Snowball),0
Beyond human gold standards: A multimodel framework for automated abstract classification and information extraction,"D. Courvoisier, D. Buitrago-Garcia, C. Buclin, Nils Bürgisser, M. Iudici, D. Mongin",2025,"
 Meta-research and evidence synthesis require considerable resources. Large language models (LLMs) have emerged as promising tools to assist in these processes, yet their performance varies across models, limiting their reliability. Taking advantage of the large availability of small size (<10 billion parameters) open-source LLMs, we implemented an agreement-based framework in which a decision is taken only if at least a given number of LLMs produce the same response. The decision is otherwise withheld. This approach was tested on 1020 abstracts of randomized controlled trials in rheumatology, using 2 classic literature review tasks: (1) classifying each intervention as drug or nondrug based on text interpretation and (2) extracting the total number of randomized patients, a task that sometimes required calculations. Re-examining abstracts where at least 4 LLMs disagreed with the human gold standard (dual review with adjudication) allowed constructing an improved gold standard. Compared to a human gold standard and single large LLMs (>70 billion parameters), our framework demonstrated robust performance: several model combinations achieved accuracies above 95% exceeding the human gold standard on at least 85% of abstracts (e.g., 3 of 5 models, 4 of 6 models, or 5 of 7 models). Performance variability across individual models was not an issue, as low-performing models contributed fewer accepted decisions. This agreement-based framework offers a scalable solution that can replace human reviewers for most abstracts, reserving human expertise for more complex cases. Such frameworks could significantly reduce the manual burden in systematic reviews while maintaining high accuracy and reproducibility.",https://www.semanticscholar.org/paper/f9a985652eb5eb4a8abd99838028833ef99ada9b,10.1017/rsm.2025.10054,Semantic Scholar (Snowball),0
BeautyGuard: Designing a Multi-Agent Roundtable System for Proactive Beauty Tech Compliance through Stakeholder Collaboration,"Junwei Li, Wenqing Wang, Huiliu Mao, Jiazhe Ni, Zuyu Xiong",2025,"As generative AI enters enterprise workflows, ensuring compliance with legal, ethical, and reputational standards becomes a pressing challenge. In beauty tech, where biometric and personal data are central, traditional reviews are often manual, fragmented, and reactive. To examine these challenges, we conducted a formative study with six experts (four IT managers, two legal managers) at a multinational beauty company. The study revealed pain points in rule checking, precedent use, and the lack of proactive guidance. Motivated by these findings, we designed a multi-agent""roundtable""system powered by a large language model. The system assigns role-specialized agents for legal interpretation, checklist review, precedent search, and risk mitigation, synthesizing their perspectives into structured compliance advice. We evaluated the prototype with the same experts using System Usability Scale(SUS), The Official NASA Task Load Index(NASA-TLX), and interviews. Results show exceptional usability (SUS: 77.5/100) and minimal cognitive workload, with three key findings: (1) multi-agent systems can preserve tacit knowledge into standardized workflows, (2) information augmentation achieves higher acceptance than decision automation, and (3) successful enterprise AI should mirror organizational structures. This work contributes design principles for human-AI collaboration in compliance review, with broader implications for regulated industries beyond beauty tech.",https://www.semanticscholar.org/paper/9723d3003c12dddf94a924318f4f76c935ce0782,,Semantic Scholar (Snowball),0
Debate over Mixed-knowledge: A Robust Multi-Agent Framework for Incomplete Knowledge Graph Question Answering,"Jilong Liu, Pengyang Shao, Wei Qin, Fei Liu, Yonghui Yang, Richang Hong",2025,"Knowledge Graph Question Answering (KGQA) aims to improve factual accuracy by leveraging structured knowledge. However, real-world Knowledge Graphs (KGs) are often incomplete, leading to the problem of Incomplete KGQA (IKGQA). A common solution is to incorporate external data to fill knowledge gaps, but existing methods lack the capacity to adaptively and contextually fuse multiple sources, failing to fully exploit their complementary strengths. To this end, we propose Debate over Mixed-knowledge (DoM), a novel framework that enables dynamic integration of structured and unstructured knowledge for IKGQA. Built upon the Multi-Agent Debate paradigm, DoM assigns specialized agents to perform inference over knowledge graphs and external texts separately, and coordinates their outputs through iterative interaction. It decomposes the input question into sub-questions, retrieves evidence via dual agents (KG and Retrieval-Augmented Generation, RAG), and employs a judge agent to evaluate and aggregate intermediate answers. This collaboration exploits knowledge complementarity and enhances robustness to KG incompleteness. In addition, existing IKGQA datasets simulate incompleteness by randomly removing triples, failing to capture the irregular and unpredictable nature of real-world knowledge incompleteness. To address this, we introduce a new dataset, Incomplete Knowledge Graph WebQuestions, constructed by leveraging real-world knowledge updates. These updates reflect knowledge beyond the static scope of KGs, yielding a more realistic and challenging benchmark. Through extensive experiments, we show that DoM consistently outperforms state-of-the-art baselines.",https://www.semanticscholar.org/paper/6986d9628181a0806b7e8590368db4615a90ebcb,,Semantic Scholar (Snowball),0
LOCA-R: Near-Perfect Performance on the Chinese Physics Olympiad 2025,"Dong-Shan Jian, Xiang Li, Chen-Xu Yan, Hui-Wen Zheng, Zhi-Zhang Bian, Youyuan Fang, Sheng-Qi Zhang, Bing-Rui Gong, Ren-Xi He, Jing-Tian Zhang, Ce Meng, Yan-Qing Ma",2025,"Olympiad-level physics problem-solving presents a significant challenge for both humans and artificial intelligence (AI), as it requires a sophisticated integration of precise calculation, abstract reasoning, and a fundamental grasp of physical principles. The Chinese Physics Olympiad (CPhO), renowned for its complexity and depth, serves as an ideal and rigorous testbed for these advanced capabilities. In this paper, we introduce LOCA-R (LOgical Chain Augmentation for Reasoning), an improved version of the LOCA framework adapted for complex reasoning, and apply it to the CPhO 2025 theory examination. LOCA-R achieves a near-perfect score of 313 out of 320 points, solidly surpassing the highest-scoring human competitor and significantly outperforming all baseline methods.",https://www.semanticscholar.org/paper/6d39720090f9f90d791ef99249c3347a0b6755ba,,Semantic Scholar (Snowball),0
Adaptive Multi-Agent Response Refinement in Conversational Systems,"Soyeong Jeong, Aparna Elangovan, Emine Yilmaz, Oleg Rokhlenko",2025,"Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.",https://www.semanticscholar.org/paper/99e1d1e8d7258f5d151062e213a4846d7315cf5f,,Semantic Scholar (Snowball),0
PCRLLM: Proof-Carrying Reasoning with Large Language Models under Stepwise Logical Constraints,"Tangrui Li, Pei Wang, Hongzheng Wang Christian Hahm, Matteo Spatola, Justin Y. Shi",2025,"Large Language Models (LLMs) often exhibit limited logical coherence, mapping premises to conclusions without adherence to explicit inference rules. We propose Proof-Carrying Reasoning with LLMs (PCRLLM), a framework that constrains reasoning to single-step inferences while preserving natural language formulations. Each output explicitly specifies premises, rules, and conclusions, thereby enabling verification against a target logic. This mechanism mitigates trustworthiness concerns by supporting chain-level validation even in black-box settings. Moreover, PCRLLM facilitates systematic multi-LLM collaboration, allowing intermediate steps to be compared and integrated under formal rules. Finally, we introduce a benchmark schema for generating large-scale step-level reasoning data, combining natural language expressiveness with formal rigor.",https://www.semanticscholar.org/paper/c755799e6c161fce86b288f67ac153ce9f8114ee,,Semantic Scholar (Snowball),0
S-DAG: A Subject-Based Directed Acyclic Graph for Multi-Agent Heterogeneous Reasoning,"Jiangwen Dong, Zehui Lin, Wanyu Lin, Mingjin Zhang",2025,"Large Language Models (LLMs) have achieved impressive performance in complex reasoning problems. Their effectiveness highly depends on the specific nature of the task, especially the required domain knowledge. Existing approaches, such as mixture-of-experts, typically operate at the task level; they are too coarse to effectively solve the heterogeneous problems involving multiple subjects. This work proposes a novel framework that performs fine-grained analysis at subject level equipped with a designated multi-agent collaboration strategy for addressing heterogeneous problem reasoning. Specifically, given an input query, we first employ a Graph Neural Network to identify the relevant subjects and infer their interdependencies to generate an \textit{Subject-based Directed Acyclic Graph} (S-DAG), where nodes represent subjects and edges encode information flow. Then we profile the LLM models by assigning each model a subject-specific expertise score, and select the top-performing one for matching corresponding subject of the S-DAG. Such subject-model matching enables graph-structured multi-agent collaboration where information flows from the starting model to the ending model over S-DAG. We curate and release multi-subject subsets of standard benchmarks (MMLU-Pro, GPQA, MedMCQA) to better reflect complex, real-world reasoning tasks. Extensive experiments show that our approach significantly outperforms existing task-level model selection and multi-agent collaboration baselines in accuracy and efficiency. These results highlight the effectiveness of subject-aware reasoning and structured collaboration in addressing complex and multi-subject problems.",https://www.semanticscholar.org/paper/6ab1a00d1785fbe9c04ea37d36355a4f7ebfe6b3,,Semantic Scholar (Snowball),0
Beyond Detection: Exploring Evidence-based Multi-Agent Debate for Misinformation Intervention and Persuasion,"Chen Han, Yijia Ma, Jin Tan, Wenzhen Zheng, Xijin Tang",2025,"Multi-agent debate (MAD) frameworks have emerged as promising approaches for misinformation detection by simulating adversarial reasoning. While prior work has focused on detection accuracy, it overlooks the importance of helping users understand the reasoning behind factual judgments and develop future resilience. The debate transcripts generated during MAD offer a rich but underutilized resource for transparent reasoning. In this study, we introduce ED2D, an evidence-based MAD framework that extends previous approach by incorporating factual evidence retrieval. More importantly, ED2D is designed not only as a detection framework but also as a persuasive multi-agent system aimed at correcting user beliefs and discouraging misinformation sharing. We compare the persuasive effects of ED2D-generated debunking transcripts with those authored by human experts. Results demonstrate that ED2D outperforms existing baselines across three misinformation detection benchmarks. When ED2D generates correct predictions, its debunking transcripts exhibit persuasive effects comparable to those of human experts; However, when ED2D misclassifies, its accompanying explanations may inadvertently reinforce users'misconceptions, even when presented alongside accurate human explanations. Our findings highlight both the promise and the potential risks of deploying MAD systems for misinformation intervention. We further develop a public community website to help users explore ED2D, fostering transparency, critical thinking, and collaborative fact-checking.",https://www.semanticscholar.org/paper/bb18817781ab2e2ec7c7f4b02d5052b78959f00c,,Semantic Scholar (Snowball),0
CoLM: Collaborative Large Models via A Client-Server Paradigm,"Siqi Huang, Sida Huang, Hongyuan Zhang",2025,"Large models have achieved remarkable performance across a range of reasoning and understanding tasks. Prior work often utilizes model ensembles or multi-agent systems to collaboratively generate responses, effectively operating in a server-to-server paradigm. However, such approaches do not align well with practical deployment settings, where a limited number of server-side models are shared by many clients under modern internet architectures. In this paper, we introduce \textbf{CoLM} (\textbf{Co}llaboration in \textbf{L}arge-\textbf{M}odels), a novel framework for collaborative reasoning that redefines cooperation among large models from a client-server perspective. Unlike traditional ensemble methods that rely on simultaneous inference from multiple models to produce a single output, CoLM allows the outputs of multiple models to be aggregated or shared, enabling each client model to independently refine and update its own generation based on these high-quality outputs. This design enables collaborative benefits by fully leveraging both client-side and shared server-side models. We further extend CoLM to vision-language models (VLMs), demonstrating its applicability beyond language tasks. Experimental results across multiple benchmarks show that CoLM consistently improves model performance on previously failed queries, highlighting the effectiveness of collaborative guidance in enhancing single-model capabilities.",https://www.semanticscholar.org/paper/ee8c83b22cbcbb8b6e9feed000dd49252d69ae46,,Semantic Scholar (Snowball),0
LLM-based Interactive Coding Education via Predictive Query Management and Student-Centered Fine-Tuning: Design and Implementation with 1500-Student Class Data,"Geonjae Youn, Jonghoon Lee, Joongheon Kim, Chuck Yoo",2025,"Large-scale university courses face significant challenges. Teaching assistants are overwhelmed by the large number of student questions, limiting their ability to provide detailed and individualized support. As a result, students-especially those who are struggling-receive less tailored assistance, further widening gaps in academic performance. To address these challenges, we propose student-centered AI learning assistant (SCALA), a large language model (LLM)-based interactive tutoring system that incorporates student needs and learning expectations. SCALA consists of two main components, i.e., predictive query management and student-centered fine tuning. The first component anticipates common student questions via LLM agent debate. Each agent interacts using a combination of lecture content and student interactions in chat logs, collaboratively predicting what the students will likely ask. This fosters learning in students by generating and presenting relevant queries that guide their learning. On the other hand, the second component is fine-tuned on a 14k-question Python-tutoring dataset, curated based on in-depth student interviews to reflect real learning expectations. Our real-world experiments with 1500-student large-scale Python classes demonstrate that SCALA delivers more helpful and accurate responses compared to closed-form models (e.g., GPT-4o), while significantly reducing latency.",https://www.semanticscholar.org/paper/5422621edad6f092cec0c93669d4f138003ce3b6,10.1145/3746252.3760941,Semantic Scholar (Snowball),0
A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving,"Keke Long, Jiacheng Guo, Tianyu Zhang, Hongkai Yu, Xiaopeng Li",2025,"Vision Language Models (VLMs) are increasingly used in autonomous driving to help understand traffic scenes, but they sometimes produce hallucinations, which are false details not grounded in the visual input. Detecting and mitigating hallucinations is challenging when ground-truth references are unavailable and model internals are inaccessible. This paper proposes a novel self-contained low-rank approach to automatically rank multiple candidate captions generated by multiple VLMs based on their hallucination levels, using only the captions themselves without requiring external references or model access. By constructing a sentence-embedding matrix and decomposing it into a low-rank consensus component and a sparse residual, we use the residual magnitude to rank captions: selecting the one with the smallest residual as the most hallucination-free. Experiments on the NuScenes dataset demonstrate that our approach achieves 87% selection accuracy in identifying hallucination-free captions, representing a 19% improvement over the unfiltered baseline and a 6-10% improvement over multi-agent debate method. The sorting produced by sparse error magnitudes shows strong correlation with human judgments of hallucinations, validating our scoring mechanism. Additionally, our method, which can be easily parallelized, reduces inference time by 51-67% compared to debate approaches, making it practical for real-time autonomous driving applications.",https://www.semanticscholar.org/paper/973addc908c5309e76c55c9ec6b641a80b362a71,,Semantic Scholar (Snowball),0
Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs,"Wei Yang, Jiacheng Pang, Shixuan Li, Paul Bogdan, Stephen Tu, Jesse Thomason",2025,"Multi-agent systems (MAS) built on Large Language Models (LLMs) are being used to approach complex problems and can surpass single model inference. However, their success hinges on navigating a fundamental cognitive tension: the need to balance broad, divergent exploration of the solution space with a principled, convergent synthesis to the optimal solution. Existing paradigms often struggle to manage this duality, leading to premature consensus, error propagation, and a critical credit assignment problem that fails to distinguish between genuine reasoning and superficially plausible arguments. To resolve this core challenge, we propose the Multi-Agent Exploration-Synthesis framework Through Role Orchestration (Maestro), a principled paradigm for collaboration that structurally decouples these cognitive modes. Maestro uses a collective of parallel Execution Agents for diverse exploration and a specialized Central Agent for convergent, evaluative synthesis. To operationalize this critical synthesis phase, we introduce Conditional Listwise Policy Optimization (CLPO), a reinforcement learning objective that disentangles signals for strategic decisions and tactical rationales. By combining decision-focused policy gradients with a list-wise ranking loss over justifications, CLPO achieves clean credit assignment and stronger comparative supervision. Experiments on mathematical reasoning and general problem-solving benchmarks demonstrate that Maestro, coupled with CLPO, consistently outperforms existing state-of-the-art multi-agent approaches, delivering absolute accuracy gains of 6% on average and up to 10% at best.",https://www.semanticscholar.org/paper/9786848f1dd93fde19d887a813b24ab1e12ef7ce,,Semantic Scholar (Snowball),0
Evaluation of retrieval-based QA on QUEST-LOFT,"Nathan Scales, Nathanael Scharli, Olivier Bousquet",2025,"Despite the popularity of retrieval-augmented generation (RAG) as a solution for grounded QA in both academia and industry, current RAG methods struggle with questions where the necessary information is distributed across many documents or where retrieval needs to be combined with complex reasoning. Recently, the LOFT study has shown that this limitation also applies to approaches based on long-context language models, with the QUEST benchmark exhibiting particularly large headroom. In this paper, we provide an in-depth analysis of the factors contributing to the poor performance on QUEST-LOFT, publish updated numbers based on a thorough human evaluation, and demonstrate that RAG can be optimized to significantly outperform long-context approaches when combined with a structured output format containing reasoning and evidence, optionally followed by answer re-verification.",https://www.semanticscholar.org/paper/e2c0ef6fbdb53ade7a0dfea8870d7ac812c0887c,,Semantic Scholar (Snowball),0
Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models,"Wenmo Qiu, S. Srivastava",2025,"Recent work has explored batch prompting as a strategy to amortize inference cost in large language models (LLMs). In this paper, we show that batching offers an additional, underappreciated benefit: it regularizes model behavior during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a comprehensive study across 13 diverse benchmarks and observe that batching improves accuracy while substantially reducing reasoning token usage, often by 3x-5x. Through detailed behavioral analysis, we find that batching suppresses overthinking, reduces hedging language (e.g., repetitive self-corrections), and encourages more decisive answers. Surprisingly, we also observe emergent collective effects in batched inference: models often generalize patterns from earlier examples to solve harder ones in the same batch. These findings position batching not just as a throughput optimization, but as a powerful inference-time regularizer for more efficient and reliable LLM reasoning.",https://www.semanticscholar.org/paper/11fc7c4fa5a0787a5204738b5abd89c6d6cc6ad3,10.48550/arXiv.2511.04108,Semantic Scholar (Snowball),0
Multi-Agent Collaborative Framework For Math Problem Generation,"Kia Karbasi, Kevin Hong, M. Samadi, G. Pottie",2025,"Automatic question generation (AQG) for mathematics education remains an elusive goal for Intelligent Tutoring Systems and educators. While pre-trained transformer-based language models have significantly advanced natural language generation, they often struggle to precisely control problem complexity and cognitive demands. In this paper, we introduce a collaborative multi-agent framework as a novel method of incorporating inference-time computation into AQG. This approach leverages multiple agents that iteratively refine generated question-answer pairs to better balance complexity and cognitive demand. We evaluate the generated questions on five meta-evaluation criteria: relevance, importance, clarity, difficulty matching, answerability, to assess the system's ability to control the required complexity and quality of the questions. Preliminary evaluations show that this collaborative multi-agent framework elevates the quality of generated educational content by fostering a more nuanced balance between cognitive challenge and clarity. These promising outcomes suggest that integrating collaborative multi-agent workflows can yield more controlled, pedagogically valuable content that can help advance automated educational content generation and adaptive learning environments.",https://www.semanticscholar.org/paper/cc384c77e7b84433ac2ff70baed5db5b3329bdf1,10.5281/zenodo.15870246,Semantic Scholar (Snowball),0
Evaluating clinical AI summaries with large language models as judges,"E. Croxford, Yanjun Gao, Elliot First, Nicholas Pellegrino, Miranda Schnier, J. Caskey, M. Oguss, Graham Wills, Guanhua Chen, Dmitriy Dligach, Matthew M. Churpek, Anoop M. Mayampurath, Frank J. Liao, Cherodeep Goswami, Karen K. Wong, Brian W Patterson, Majid Afshar",2025,"Electronic Health Records (EHRs) contain vast clinical data that are difficult for providers to synthesize. Generative AI with Large Language Models (LLMs) can summarize records to reduce cognitive burden, but ensuring accuracy requires reliable evaluation. Human review is the gold standard but is costly and slow. To address this, we introduce and validate an automated LLM-based method to assess real-world EHR multi-document summaries. Benchmarking against the validated Provider Documentation Summarization Quality Instrument (PDSQI), our LLM-as-a-Judge framework demonstrated strong inter-rater reliability with human evaluators. GPT-o3-mini achieved an intraclass correlation coefficient of 0.818 (95% CI 0.772–0.854), a median score difference of 0 from humans, and completed evaluations in 22 seconds. Overall, reasoning models excelled in inter-rater reliability, particularly for evaluations requiring advanced reasoning and domain expertise, outperforming non-reasoning, task-trained, and multi-agent approaches. By automating high-quality evaluations, a medical LLM-as-a-Judge provides a scalable, efficient way to identify accurate, safe AI-generated clinical summaries.",https://www.semanticscholar.org/paper/78c758993828ae1ee5f0584d813152f2ce4af84f,10.1038/s41746-025-02005-2,Semantic Scholar (Snowball),0
Modeling Hawkish-Dovish Latent Beliefs in Multi-Agent Debate-Based LLMs for Monetary Policy Decision Classification,"Kaito Takano, Masanori Hirano, Kei Nakagawa",2025,"Accurately forecasting central bank policy decisions, particularly those of the Federal Open Market Committee(FOMC) has become increasingly important amid heightened economic uncertainty. While prior studies have used monetary policy texts to predict rate changes, most rely on static classification models that overlook the deliberative nature of policymaking. This study proposes a novel framework that structurally imitates the FOMC's collective decision-making process by modeling multiple large language models(LLMs) as interacting agents. Each agent begins with a distinct initial belief and produces a prediction based on both qualitative policy texts and quantitative macroeconomic indicators. Through iterative rounds, agents revise their predictions by observing the outputs of others, simulating deliberation and consensus formation. To enhance interpretability, we introduce a latent variable representing each agent's underlying belief(e.g., hawkish or dovish), and we theoretically demonstrate how this belief mediates the perception of input information and interaction dynamics. Empirical results show that this debate-based approach significantly outperforms standard LLMs-based baselines in prediction accuracy. Furthermore, the explicit modeling of beliefs provides insights into how individual perspectives and social influence shape collective policy forecasts.",https://www.semanticscholar.org/paper/37d58d1768a17ebd508e9a26953f58c82c8a0c99,,Semantic Scholar (Snowball),0
Optimal-Agent-Selection: State-Aware Routing Framework for Efficient Multi-Agent Collaboration,"Jingbo Wang, Sendong Zhao, Hao Wang, Yuzhen Fan, Lizhe Zhang, Yan Liu, Ting Liu",2025,"The emergence of multi-agent systems powered by large language models (LLMs) has unlocked new frontiers in complex task-solving, enabling diverse agents to integrate unique expertise, collaborate flexibly, and address challenges unattainable for individual models. However, the full potential of such systems is hindered by rigid agent scheduling and inefficient coordination strategies that fail to adapt to evolving task requirements. In this paper, we propose STRMAC, a state-aware routing framework designed for efficient collaboration in multi-agent systems. Our method separately encodes interaction history and agent knowledge to power the router, which adaptively selects the most suitable single agent at each step for efficient and effective collaboration. Furthermore, we introduce a self-evolving data generation approach that accelerates the collection of high-quality execution paths for efficient system training. Experiments on challenging collaborative reasoning benchmarks demonstrate that our method achieves state-of-the-art performance, achieving up to 23.8% improvement over baselines and reducing data collection overhead by up to 90.1% compared to exhaustive search.",https://www.semanticscholar.org/paper/1f24e18ba7106b464a7c9081300d4c2f3b661c11,10.48550/arXiv.2511.02200,Semantic Scholar (Snowball),0
Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation,"Zhiwei Zhang, Xiao-Ming Li, Yudi Lin, Hui Liu, Ramraj Chandradevan, Linlin Wu, Min Lin, Fali Wang, Xianfeng Tang, Qi He, Suhang Wang",2025,"Large Language Models (LLMs) trained with reinforcement learning and verifiable rewards have achieved strong results on complex reasoning tasks. Recent work extends this paradigm to a multi-agent setting, where a meta-thinking agent proposes plans and monitors progress while a reasoning agent executes subtasks through sequential conversational turns. Despite promising performance, we identify a critical limitation: lazy agent behavior, in which one agent dominates while the other contributes little, undermining collaboration and collapsing the setup to an ineffective single agent. In this paper, we first provide a theoretical analysis showing why lazy behavior naturally arises in multi-agent reasoning. We then introduce a stable and efficient method for measuring causal influence, helping mitigate this issue. Finally, as collaboration intensifies, the reasoning agent risks getting lost in multi-turn interactions and trapped by previous noisy responses. To counter this, we propose a verifiable reward mechanism that encourages deliberation by allowing the reasoning agent to discard noisy outputs, consolidate instructions, and restart its reasoning process when necessary. Extensive experiments demonstrate that our framework alleviates lazy agent behavior and unlocks the full potential of multi-agent framework for complex reasoning tasks.",https://www.semanticscholar.org/paper/de6b7dbe0fc09403aa72328bbe90064f1ab35b17,10.48550/arXiv.2511.02303,Semantic Scholar (Snowball),0
MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL,"Haolin Yang, Jipeng Zhang, Zhitao He, Yi R. Fung",2025,"Translating natural language to SQL remains difficult for complex queries. Such queries often need environmental interaction and self-correction. To address this, we introduce MARS-SQL, a novel multi-agent framework that combines principled task decomposition and interactive reinforcement learning (RL). Our system comprises three specialized agents: a Grounding Agent for schema linking, a Generation Agent for query generation, and a Validation Agent for final selection. The core of our framework is the Generation agent, which is trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe loop, the agent iteratively generates thoughts, executes SQL actions against a live database, and revises its strategy based on execution feedback, enabling dynamic, stateful reasoning and self-correction. At inference time, we generate multiple interaction trajectories to explore diverse reasoning paths. The Validation agent, then selects the optimal trajectory by modeling verification as a next-token prediction task and choosing the solution with the highest generation probability. This structured workflow pipelines specialized agents. It combines interactive RL for generation with generative modeling for verification. The approach proves highly effective for robust and accurate SQL generation. Experiments show that MARS-SQL achieves state-of-the-art Execution Accuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our code is available at https://github.com/YangHaolin0526/MARS-SQL.",https://www.semanticscholar.org/paper/2f147aec448ee58caa7a66ae2ea01bb9155603b2,10.48550/arXiv.2511.01008,Semantic Scholar (Snowball),0
GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks,"Heng Zheng, Yuling Shi, Xiaodong Gu, Haochen You, Zijian Zhang, Lubin Gan, Hao Zhang, Wenjun Huang, Jin Huang",2025,"Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.",https://www.semanticscholar.org/paper/eb3f5b1e62860fb407d05efce8a5ea4c5073c519,10.48550/arXiv.2511.00908,Semantic Scholar (Snowball),0
Efficient Test-Time Retrieval Augmented Generation,"Hailong Yin, Bin Zhu, Jingjing Chen, Chong-Wah Ngo",2025,"Although Large Language Models (LLMs) demonstrate significant capabilities, their reliance on parametric knowledge often leads to inaccuracies. Retrieval Augmented Generation (RAG) mitigates this by incorporating external knowledge, but these methods may introduce irrelevant retrieved documents, leading to inaccurate responses. While the integration methods filter out incorrect answers from multiple responses, but lack external knowledge like RAG methods, and their high costs require balancing overhead with performance gains. To address these issues, we propose an Efficient Test-Time Retrieval-Augmented Generation Framework named ET2RAG to improve the performance of LLMs while maintaining efficiency. Specifically, ET2RAG is a training-free method, that first retrieves the most relevant documents and augments the LLMs to efficiently generate diverse candidate responses by managing response length. Then we compute the similarity of candidate responses and employ a majority voting mechanism to select the most suitable response as the final output. In particular, we discover that partial generation is sufficient to capture the key information necessary for consensus calculation, allowing us to effectively perform majority voting without the need for fully generated responses. Thus, we can reach a balance between computational cost and performance by managing the response length for the number of retrieved documents for majority voting. Experimental results demonstrate that ET2RAG significantly enhances performance across three tasks, including open-domain question answering, recipe generation and image captioning.",https://www.semanticscholar.org/paper/8d9418f3aade47b80dbde938e0a0587896900a4e,10.48550/arXiv.2511.01059,Semantic Scholar (Snowball),0
Sherlock: Reliable and Efficient Agentic Workflow Execution,"Yeonju Ro, Haoran Qiu, Íñigo Goiri, Rodrigo Fonseca, Ricardo Bianchini, Aditya Akella, Zhangyang Wang, M. Erez, Esha Choukse",2025,"With the increasing adoption of large language models (LLM), agentic workflows, which compose multiple LLM calls with tools, retrieval, and reasoning steps, are increasingly replacing traditional applications. However, such workflows are inherently error-prone: incorrect or partially correct output at one step can propagate or even amplify through subsequent stages, compounding the impact on the final output. Recent work proposes integrating verifiers that validate LLM output or actions, such as self-reflection, debate, or LLM-as-a-judge mechanisms. Yet, verifying every step introduces significant latency and cost overheads. In this work, we seek to answer three key questions: which nodes in a workflow are most error-prone and thus deserve costly verification, how to select the most appropriate verifier for each node, and how to use verification with minimal impact to latency? Our solution, Sherlock, addresses these using counterfactual analysis on agentic workflows to identify error-prone nodes and selectively attaching cost-optimal verifiers only where necessary. At runtime, Sherlock speculatively executes downstream tasks to reduce latency overhead, while verification runs in the background. If verification fails, execution is rolled back to the last verified output. Compared to the non-verifying baseline, Sherlock delivers an 18.3% accuracy gain on average across benchmarks. Sherlock reduces workflow execution time by up to 48.7% over non-speculative execution and lowers verification cost by 26.0% compared to the Monte Carlo search-based method, demonstrating that principled, fault-aware verification effectively balances efficiency and reliability in agentic workflows.",https://www.semanticscholar.org/paper/4abf4e09c9f8a7c6a0f58991e7bd28f21c7327da,10.48550/arXiv.2511.00330,Semantic Scholar (Snowball),0
Separate the Wheat from the Chaff: Winnowing Down Divergent Views in Retrieval Augmented Generation,"D. N, Akari Asai, Sewon Min, Zexuan Zhong, Patrick Lewis, Ethan Perez, Aleksandra Piktus, Vladimir Petroni, Naman Karpukhin, Hein-309 Goyal, Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Hong Yang, Ronak Pradeep, Rodrigo Nogueira, Linjie Lu, Shou-Jen Chen, Tsung-Min Pai, Aman Madaan, Prakhar Gupta Skyler Niket Tandon, Luyu Hallinan, Sarah Gao, Uri Wiegreffe, Alon, Alex Troy Mallen, Victor Zhong, Rajarshi Das, Julian Michael, Salsabila Mahdi, David Rein, Jian-Cheng Ni, Chenfei Qu, Jing Lu, Zhuyun Dai, Gustavo Hernández Abrego, Ji Ma, Vincent Zhao, Yi Luan, F. Petroni, A. Fan, Psh Lewis, M. Yazdani, Tianlu Wang, Xiaoqing Ping Yu, Ellen Tan, Sean 923 O’Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke S. Zettlemoyer, Maryam Fazel-925, Zhengfan Wang, Shu Xian Teo, Jieer Ouyang, Anush Zhang, Ankur Mattapalli, Jingbo Taly, Shang, Zhepei Wei, Wei-Lin Chen, Yujiao Meng, Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Ting Fu, Xin-xin Huang, Enbo Zhao, Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji",2025,"Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge sources to address their limitations in accessing up-to-date or specialized information. A natural strategy to increase the likelihood of retrieving relevant information is to expand the number of retrieved documents. However, involving more documents could introduce significant noise, as many documents may be irrelevant or misleading, thereby reducing the overall accuracy of the generated responses. To overcome the challenge associated with handling a larger number of documents, we propose WinnowRAG, a novel RAG framework designed to systematically filter out noisy documents while preserving valuable content -- a process we refer to as winnowing. WinnowRAG operates in two stages: In Stage I, we perform query-aware clustering to group similar documents and form distinct topic clusters. Each cluster is assigned to an LLM agent for generating a unique answer. In Stage II, we perform winnowing, wherein a critic LLM evaluates the outputs of multiple agents and iteratively separates useful documents from noisy ones. To retain useful documents when discarding agents, we propose two strategic merging techniques to ensure that only relevant knowledge is used for generating the final response. Crucially, WinnowRAG is model-agnostic and does not require any model fine-tuning, making it easily adaptable to various tasks. Extensive experiments on various realistic datasets demonstrate the effectiveness of WinnowRAG over state-of-the-art baselines.",https://www.semanticscholar.org/paper/f172884521714798aba7e4fa1599525afad08cc9,10.18653/v1/2025.emnlp-main.587,Semantic Scholar (Snowball),0
"Security of LLM-based Agents Regarding Attacks, Defenses, and Applications: A Comprehensive Survey","Yaxin Tang, Yijia Liu, Jiahe Lan, Zheng Yan, Erol Gelenbe",2025,,https://www.semanticscholar.org/paper/71a6da06ccde18baf6329398885fbb65f16f8647,10.1016/j.inffus.2025.103941,Semantic Scholar (Snowball),0
SCoTER: Structured Chain-of-Thought Transfer for Enhanced Recommendation,"Yang Wu, Qian Li, Yuling Xiong, Hongbo Tang, Xun Liu, Jun Zhang, Huan Yu, Jie Jiang, Hailong Shi",2025,"Harnessing the reasoning power of Large Language Models (LLMs) for recommender systems is hindered by two fundamental challenges. First, current approaches lack a mechanism for automated, data-driven discovery of effective reasoning patterns, relying instead on brittle manual templates or unstable zero-shot prompting. Second, they employ structure-collapsing integration: direct prompting incurs prohibitive online inference costs, while feature extraction collapses reasoning chains into single vectors, discarding stepwise logic. To address these challenges, we propose SCoTER (Structured Chain-of-Thought Transfer for Enhanced Recommendation), a unified framework that treats pattern discovery and structure-aware transfer as a jointly optimized problem. Specifically, SCoTER operationalizes this through two synergistic components: a GVM pipeline for automated pattern discovery and a structure-preserving integration architecture that transfers stepwise logic to efficient models. Formally, we provide information-theoretic justification proving that structure-preserving transfer achieves tighter performance bounds than structure-agnostic alternatives. Empirically, experiments on four benchmarks demonstrate improvements of 3.75\%-11.59\% over a strong TIGER backbone. Moreover, in production deployment on the Tencent Advertising Platform, SCoTER achieved a 2.14\% lift in Gross Merchandise Value (GMV) while eliminating online LLM inference costs. Overall, SCoTER establishes a principled and production-validated blueprint for transferring structured LLM reasoning to large-scale recommender systems.",https://www.semanticscholar.org/paper/1ba21606c6c1ff5069a4acbb2ae56f0c06ed90f7,,Semantic Scholar (Snowball),0
HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization,"Zhiyi Duan, Zixing Shi, Hongyu Yuan, Qi Wang",2025,"Knowledge Tracing (KT) aims to mine students'evolving knowledge states and predict their future question-answering performance. Existing methods based on heterogeneous information networks (HINs) are prone to introducing noises due to manual or random selection of meta-paths and lack necessary quality assessment of meta-path instances. Conversely, recent large language models (LLMs)-based methods ignore the rich information across students, and both paradigms struggle to deliver consistently accurate and evidence-based explanations. To address these issues, we propose an innovative framework, HIN-LLM Synergistic Enhanced Knowledge Tracing (HISE-KT), which seamlessly integrates HINs with LLMs. HISE-KT first builds a multi-relationship HIN containing diverse node types to capture the structural relations through multiple meta-paths. The LLM is then employed to intelligently score and filter meta-path instances and retain high-quality paths, pioneering automated meta-path quality assessment. Inspired by educational psychology principles, a similar student retrieval mechanism based on meta-paths is designed to provide a more valuable context for prediction. Finally, HISE-KT uses a structured prompt to integrate the target student's history with the retrieved similar trajectories, enabling the LLM to generate not only accurate predictions but also evidence-backed, explainable analysis reports. Experiments on four public datasets show that HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.",https://www.semanticscholar.org/paper/442a902a7bf80b13b1926e983fd0dfaa6f471318,,Semantic Scholar (Snowball),0
CoolPrompt: Automatic Prompt Optimization Framework for Large Language Models,"N. Kulin, Viktor N. Zhuravlev, Artur R. Khairullin, Alena N. Sitkina, Sergey Muravyov",2025,"The effectiveness of Large Language Models (LLMs) is highly dependent on the design of input prompts. Manual prompt engineering requires a domain expertise and prompting techniques knowledge that leads to a complex, time-consuming, subjective, and often suboptimal process. We introduce Cool-Prompt as a novel framework for automatic prompt optimization. It provides a complete zero-configuration workflow, which includes automatic task and metric selection, also splits the input dataset or generates synthetic data when annotations are missing, and final feedback collection of prompt optimization results. Our framework provides three new prompt optimization algorithms ReflectivePrompt and DistillPrompt that have demonstrated effectiveness compared to similar optimization algorithms, and a flexible meta-prompting approach called HyPE for rapid optimization. Competitive and experimental results demonstrate the effectiveness of CoolPrompt over other solutions.",https://www.semanticscholar.org/paper/5ce233a2b15ca171e4e19dee78662b1e3d290be5,10.23919/FRUCT67853.2025.11239071,Semantic Scholar (Snowball),0
DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching,"Zicheng Xu, Guanchu Wang, Yu-Neng Chuang, Guangyao Zheng, Alexander Szalay, Zirui Liu, Vladimir Braverman",2025,"Large Reasoning Models (LRMs) demonstrate strong performance on complex reasoning tasks, yet they often suffer from overthinking, producing excessively long chain-of-thought (CoT) traces that increase inference cost and may degrade accuracy. Our analysis reveals a clear anti-correlation between reasoning length and accuracy, where across multiple stochastic decodes, the short reasoning paths consistently achieve the highest correctness, while longer ones accumulate errors and repetitions. These short optimal reasoning paths can be found ideally through full enumeration of the reasoning space. However, the tree-structured reasoning space grows exponentially with sequence length, rendering exhaustive exploration infeasible. To address this, we propose DTS, a model-agnostic decoding framework that sketches the reasoning space by selectively branching at high-entropy tokens and applies early stopping to select the shortest completed reasoning path. This approach approximates the optimal solution that enhances both efficiency and accuracy, without requiring additional training or supervision. Experiments on AIME2024 and AIME2025 datasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves accuracy by up to 8%, reduces average reasoning length by 23%, and decreases repetition frequency by 12%, demonstrating DTS's ability for scalable and efficient LRM reasoning.",https://www.semanticscholar.org/paper/a939b2e822cd48f824ba08fdf3f3c9ce0ae46a8f,10.48550/arXiv.2511.00640,Semantic Scholar (Snowball),0
Once Upon an Input: Reasoning via Per-Instance Program Synthesis,"Adam Stein, Neelay Velingker, Mayur Naik, Eric Wong",2025,"Large language models (LLMs) excel at zero-shot inference but continue to struggle with complex, multi-step reasoning. Recent methods that augment LLMs with intermediate reasoning steps such as Chain of Thought (CoT) and Program of Thought (PoT) improve performance but often produce undesirable solutions, especially in algorithmic domains. We introduce Per-Instance Program Synthesis (PIPS), a method that generates and refines programs at the instance-level using structural feedback without relying on task-specific guidance or explicit test cases. To further improve performance, PIPS incorporates a confidence metric that dynamically chooses between direct inference and program synthesis on a per-instance basis. Experiments across three frontier LLMs and 30 benchmarks including all tasks of Big Bench Extra Hard (BBEH), visual question answering tasks, relational reasoning tasks, and mathematical reasoning tasks show that PIPS improves the absolute harmonic mean accuracy by up to 8.6% and 9.4% compared to PoT and CoT respectively, and reduces undesirable program generations by 65.1% on the algorithmic tasks compared to PoT with Gemini-2.0-Flash.",https://www.semanticscholar.org/paper/7e26f894af508c6c2aced1b192627689636a7113,10.48550/arXiv.2510.22849,Semantic Scholar (Snowball),0
Code-enabled language models can outperform reasoning models on diverse tasks,"Cedegao E. Zhang, C'edric Colas, Gabriel Poesia, Josh Tenenbaum, Jacob Andreas",2025,"Reasoning models (RMs), language models (LMs) trained with reinforcement learning to produce long-form natural language reasoning, have been remarkably successful, but they still require large amounts of computation and data to train, and can be slow and expensive to run. In this paper, we show that standard instruct LMs can already be elicited to be strong reasoners at a level comparable to or even surpassing their corresponding RMs (e.g., DeepSeek V3 vs R1) without finetuning, across diverse domains from instruction following and creative generation to mathematical reasoning. This is achieved by CodeAdapt, our simple recipe that combines the CodeAct framework, where LMs interleave natural language reasoning with code execution in a multi-step fashion, with few-shot bootstrap in-context learning from as few as five training problems. Analyzing four matched pairs of LMs and RMs, we find that CodeAdapt enables three LMs to outperform the corresponding RMs on average over eight tasks (up to 22.9%) while being 10-81% more token efficient, and delivers superior performance on six tasks when averaged over the four models (up to 35.7%). Furthermore, the code-augmented reasoning traces display rich and varied problem-solving strategies. Our findings support that (1) CodeAdapt-style learning and reasoning may be robust and domain general and (2) code-enabled LMs are cognitively grounded and powerful systems, potentially providing a strong foundation for in-weight reinforcement learning.",https://www.semanticscholar.org/paper/6958f9d683d8a5161c7f50cb97c8e5448596420d,10.48550/arXiv.2510.20909,Semantic Scholar (Snowball),0
"Before you , monitor: Implementing Flavell's metacognitive framework in LLMs",Nick Oh,2025,"Current approaches to enhancing LLM reasoning follows two isolated paradigms: Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack mechanisms to verify whether selected strategies succeed; while Generate-Verify approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan et al., 2023) iteratively refine outputs but commence generation blindly without task assessment. This separation creates inefficiencies -- strategies fail without feedback, and refinement occurs without strategic grounding. We address this gap by implementing Flavell's cognitive monitoring model (1979) from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025), operationalising it as a three-phase iterative system. On GSM8K, preliminary results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37% increased inference cost. These initial findings suggest upfront monitoring produces higher-quality initial solutions that reduce refinement needs, though evaluation beyond arithmetic reasoning is needed to establish generalisability.",https://www.semanticscholar.org/paper/eef689e29e42b889e483aa7a3a1f96aa6167efe6,10.48550/arXiv.2510.16374,Semantic Scholar (Snowball),0
LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning,"Beomseok Kang, Jiwon Song, Jae-Joon Kim",2025,"Multi-stage reasoning has emerged as an effective strategy for enhancing the reasoning capability of small language models by decomposing complex problems into sequential sub-stages. However, this comes at the cost of increased latency. We observe that existing adaptive acceleration techniques, such as layer skipping, struggle to balance efficiency and accuracy in this setting due to two key challenges: (1) stage-wise variation in skip sensitivity, and (2) the generation of redundant output tokens. To address these, we propose LiteStage, a latency-aware layer skipping framework for multi-stage reasoning. LiteStage combines a stage-wise offline search that allocates optimal layer budgets with an online confidence-based generation early exit to suppress unnecessary decoding. Experiments on three benchmarks, e.g., OBQA, CSQA, and StrategyQA, show that LiteStage achieves up to 1.70x speedup with less than 4.0% accuracy loss, outperforming prior training-free layer skipping methods.",https://www.semanticscholar.org/paper/17e1a2a999ab24708e58d3fbd286910121be91a5,10.48550/arXiv.2510.14211,Semantic Scholar (Snowball),0
COSMIR: Chain Orchestrated Structured Memory for Iterative Reasoning over Long Context,"Naman Gupta, S. Gowaikar, Arun Iyer, Kirankumar Shiragur, Ramakrishna Bairi, Rishikesh Maurya, Ritabrata Maiti, Sankarshan Damle, Shachee Mishra Gupta",2025,"Reasoning over very long inputs remains difficult for large language models (LLMs). Common workarounds either shrink the input via retrieval (risking missed evidence), enlarge the context window (straining selectivity), or stage multiple agents to read in pieces. In staged pipelines (e.g., Chain of Agents, CoA), free-form summaries passed between agents can discard crucial details and amplify early mistakes. We introduce COSMIR (Chain Orchestrated Structured Memory for Iterative Reasoning), a chain-style framework that replaces ad hoc messages with a structured memory. A Planner agent first turns a user query into concrete, checkable sub-questions. worker agents process chunks via a fixed micro-cycle: Extract, Infer, Refine, writing all updates to the shared memory. A Manager agent then Synthesizes the final answer directly from the memory. This preserves step-wise read-then-reason benefits while changing both the communication medium (structured memory) and the worker procedure (fixed micro-cycle), yielding higher faithfulness, better long-range aggregation, and auditability. On long-context QA from the HELMET suite, COSMIR reduces propagation-stage information loss and improves accuracy over a CoA baseline.",https://www.semanticscholar.org/paper/0d2d834e650ee2e2e3a3001dd30e24ef6b53d4e0,10.48550/arXiv.2510.04568,Semantic Scholar (Snowball),0
Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment,"Hongxiang Zhang, Yuan Tian, Tianyi Zhang",2025,"To solve complex reasoning tasks for Large Language Models (LLMs), prompting-based methods offer a lightweight alternative to fine-tuning and reinforcement learning. However, as reasoning chains extend, critical intermediate steps and the original prompt will be buried in the context, receiving insufficient attention and leading to errors. In this paper, we propose Self-Anchor, a novel pipeline that leverages the inherent structure of reasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories into structured plans and automatically aligns the model's attention to the most relevant inference steps, allowing the model to maintain focus throughout generation. Our experiment shows that Self-Anchor outperforms SOTA prompting methods across six benchmarks. Notably, Self-Anchor significantly reduces the performance gap between ``non-reasoning''models and specialized reasoning models, with the potential to enable most LLMs to tackle complex reasoning tasks without retraining.",https://www.semanticscholar.org/paper/660d7534e20690e968186fa108a77044d7c3ed5e,10.48550/arXiv.2510.03223,Semantic Scholar (Snowball),0
CLEAR: A Comprehensive Linguistic Evaluation of Argument Rewriting by Large Language Models,"Thomas Huber, Christina Niklaus",2025,"While LLMs have been extensively studied on general text generation tasks, there is less research on text rewriting, a task related to general text generation, and particularly on the behavior of models on this task. In this paper we analyze what changes LLMs make in a text rewriting setting. We focus specifically on argumentative texts and their improvement, a task named Argument Improvement (ArgImp). We present CLEAR: an evaluation pipeline consisting of 57 metrics mapped to four linguistic levels: lexical, syntactic, semantic and pragmatic. This pipeline is used to examine the qualities of LLM-rewritten arguments on a broad set of argumentation corpora and compare the behavior of different LLMs on this task and analyze the behavior of different LLMs on this task in terms of linguistic levels. By taking all four linguistic levels into consideration, we find that the models perform ArgImp by shortening the texts while simultaneously increasing average word length and merging sentences. Overall we note an increase in the persuasion and coherence dimensions.",https://www.semanticscholar.org/paper/8abec712a340c5e9080f385a9f60d7fef4aaf057,10.48550/arXiv.2509.15027,Semantic Scholar (Snowball),0
CARE: Contextual Adaptation of Recommenders for LLM-based Conversational Recommendation,"Chuang Li, Yang Deng, Hengchang Hu, See-Kiong Ng, Min-Yen Kan, Haizhou Li",2025,"We tackle the challenge of integrating large language models (LLMs) with external recommender systems to enhance domain expertise in conversational recommendation (CRS). Current LLM-based CRS approaches primarily rely on zero- or few-shot methods for generating item recommendations based on user queries, but this method faces two significant challenges: (1) without domain-specific adaptation, LLMs frequently recommend items not in the target item space, resulting in low recommendation accuracy; and (2) LLMs largely rely on dialogue context for content-based recommendations, neglecting the collaborative relationships among entities or item sequences. To address these limitations, we introduce the CARE (Contextual Adaptation of Recommenders) framework. CARE customizes LLMs for CRS tasks, and synergizes them with external recommendation systems. CARE (a) integrates external recommender systems as domain experts, producing recommendations through entity-level insights, and (b) enhances those recommendations by leveraging contextual information for more accurate and unbiased final recommendations using LLMs. Our results demonstrate that incorporating external recommender systems with entity-level information significantly enhances recommendation accuracy of LLM-based CRS by an average of 54% and 25% for ReDial and INSPIRED datasets. The most effective strategy in the CARE framework involves LLMs selecting and reranking candidate items that external recommenders provide based on contextual insights. Our analysis indicates that the CARE framework effectively addresses the identified challenges and mitigates the popularity bias in the external recommender.",https://www.semanticscholar.org/paper/529e7cc11ddff87638227f7ad67f061600f620ba,10.48550/arXiv.2508.13889,Semantic Scholar (Snowball),0
FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis,"Chen Shen, Wanqing Zhang, Kehan Li, Erwen Huang, H. Bi, Aiying Fan, Yiwen Shen, Hongmei Dong, Ji Zhang, Yuming Shao, Zengjia Liu, Xinshe Liu, Tao Li, Chunxia Yan, Shuanliang Fan, Di Wu, Jianhua Ma, Bin Cong, Zhenyuan Wang, Chunfeng Lian",2025,"Forensic cause-of-death determination faces systemic challenges, including workforce shortages and diagnostic variability, particularly in high-volume systems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic AgenT), a multi-agent AI framework that automates and standardizes death investigations through a domain-adapted large language model. FEAT's application-oriented architecture integrates: (i) a central Planner for task decomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a Memory&Reflection module for iterative refinement, and (iv) a Global Solver for conclusion synthesis. The system employs tool-augmented reasoning, hierarchical retrieval-augmented generation, forensic-tuned LLMs, and human-in-the-loop feedback to ensure legal and medical validity. In evaluations across diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI systems in both long-form autopsy analyses and concise cause-of-death conclusions. It demonstrated robust generalization across six geographic regions and achieved high expert concordance in blinded validations. Senior pathologists validated FEAT's outputs as comparable to those of human experts, with improved detection of subtle evidentiary nuances. To our knowledge, FEAT is the first LLM-based AI agent system dedicated to forensic medicine, offering scalable, consistent death certification while maintaining expert-level rigor. By integrating AI efficiency with human oversight, this work could advance equitable access to reliable medicolegal services while addressing critical capacity constraints in forensic systems.",https://www.semanticscholar.org/paper/4962ebeb1175088029a92578f7e70453755a2659,10.48550/arXiv.2508.07950,Semantic Scholar (Snowball),0
EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation,"Xinda Wang, Zhengxu Hou, Yangshijie Zhang, Bingren Yan, Zhibo Yang, Xingsheng Zhang, Luxi Xing, Qiang Zhou, Chen Zhang",2025,"Although the effectiveness of Large Language Models (LLMs) as judges (LLM-as-a-judge) has been validated, their performance remains limited in open-ended tasks, particularly in story evaluation. Accurate story evaluation is crucial not only for assisting human quality judgment but also for providing key signals to guide story generation. However, existing methods face a dilemma: prompt engineering for closed-source models suffers from poor adaptability, while fine-tuning approaches for open-source models lack the rigorous reasoning capabilities essential for story evaluation. To address this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework. Grounded in pairwise comparison, the framework first self-synthesizes score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To ensure data quality, these raw CoTs undergo a self-filtering process, utilizing multi-agents to guarantee their logical rigor and robustness. Finally, the evaluator trained on the refined data is deployed as a reward model to guide the story generation task. Experimental results demonstrate that our framework achieves state-of-the-art (SOTA) performance on three evaluation benchmarks including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward model, it significantly enhances the quality of generated stories, thereby fully validating the superiority of our self-evolving approach.",https://www.semanticscholar.org/paper/0a3a8a54ba7c7e1dd05539a772b4c798c91fa683,10.48550/arXiv.2508.06046,Semantic Scholar (Snowball),0
"Reasoning Systems as Structured Processes: Foundations, Failures, and Formal Criteria","Saleh Nikooroo, Thomas Engel",2025,"This paper outlines a general formal framework for reasoning systems, intended to support future analysis of inference architectures across domains. We model reasoning systems as structured tuples comprising phenomena, explanation space, inference and generation maps, and a principle base. The formulation accommodates logical, algorithmic, and learning-based reasoning processes within a unified structural schema, while remaining agnostic to any specific reasoning algorithm or logic system. We survey basic internal criteria--including coherence, soundness, and completeness-and catalog typical failure modes such as contradiction, incompleteness, and non-convergence. The framework also admits dynamic behaviors like iterative refinement and principle evolution. The goal of this work is to establish a foundational structure for representing and comparing reasoning systems, particularly in contexts where internal failure, adaptation, or fragmentation may arise. No specific solution architecture is proposed; instead, we aim to support future theoretical and practical investigations into reasoning under structural constraint.",https://www.semanticscholar.org/paper/cdac6d8766a1b97563c3f1a2fe66b0dca8ceb865,10.48550/arXiv.2508.01763,Semantic Scholar (Snowball),0
"Polymorphic Combinatorial Frameworks (PCF): Guiding the Design of Mathematically-Grounded, Adaptive AI Agents","David Pearl, Matthew Murphy, James Intriligator",2025,"The Polymorphic Combinatorial Framework (PCF) leverages Large Language Models (LLMs) and mathematical frameworks to guide the meta-prompt enabled design of solution spaces and adaptive AI agents for complex, dynamic environments. Unlike static agent architectures, PCF enables real-time parameter reconfiguration through mathematically-grounded combinatorial spaces, allowing agents to adapt their core behavioral traits dynamically. Grounded in combinatorial logic, topos theory, and rough fuzzy set theory, PCF defines a multidimensional SPARK parameter space (Skills, Personalities, Approaches, Resources, Knowledge) to capture agent behaviors. This paper demonstrates how LLMs can parameterize complex spaces and estimate likely parameter values/variabilities. Using PCF, we parameterized mock caf\'e domains (five levels of complexity), estimated variables/variabilities, and conducted over 1.25 million Monte Carlo simulations. The results revealed trends in agent adaptability and performance across the five complexity tiers, with diminishing returns at higher complexity levels highlighting thresholds for scalable designs. PCF enables the generation of optimized agent configurations for specific scenarios while maintaining logical consistency. This framework supports scalable, dynamic, explainable, and ethical AI applications in domains like customer service, healthcare, robotics, and collaborative systems, paving the way for adaptable and cooperative next-generation polymorphic agents.",https://www.semanticscholar.org/paper/8dba9914529c1c2c4857c2d8efaee042f7cdda87,10.48550/arXiv.2508.01581,Semantic Scholar (Snowball),0
Developing foundations for biomedical knowledgebases from literature using large language models – A systematic assessment,"Chen Miao, Zhenghao Zhang, Jiamin Chen, Daniel Rebibo, Haoran Wu, Sin-Hang Fung, A. S. Cheng, S. K. Tsui, Sanju Sinha, Qin Cao, K. Yip",2025,,https://www.semanticscholar.org/paper/b33b3c4868ec52a3a98d6975d5d319ad54ad6512,10.1016/j.csbj.2025.07.042,Semantic Scholar (Snowball),0
Effects of structure on reasoning in instance-level Self-Discover,"Sachith Gunasekara, Yasiru Ratnayake",2025,"The drive for predictable LLM reasoning in their integration with compound systems has popularized structured outputs, yet concerns remain about performance trade-offs compared to unconstrained natural language. At the same time, training on unconstrained Chain of Thought (CoT) traces has brought about a new class of strong reasoning models that nevertheless present novel compute budget and faithfulness challenges. This paper introduces iSelf-Discover, an instance-level adaptation of the Self-Discover framework, and using it compares dynamically generated structured JSON reasoning with its unstructured counterpart. Our empirical evaluation across diverse benchmarks using state-of-the-art open-source models supports a consistent advantage for unstructured reasoning. Notably, on the complex MATH benchmark, unstructured plans achieved relative performance improvements of up to 18.90\% over structured approaches. Zero-shot unstructured iSelf-Discover variants are also shown to outperform their five-shot structured counterparts, underscoring the significance of this gap, even when structured plans are dynamically generated to ensure reasoning precedes the final answer. We further demonstrate that the optimal granularity of plan generation (instance-level vs. task-level) is context-dependent. These findings invite re-evaluation of the reliance on structured formats for complex problem-solving and how compound systems should be organized.",https://www.semanticscholar.org/paper/496d95bfd4fb4ba8f89fe286ba6487840dcdae06,10.48550/arXiv.2507.03347,Semantic Scholar (Snowball),0
Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies,"Tao Xiong, Xavier Hu, Wenyan Fan, Shengyu Zhang",2025,"Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, task-specific prompts limits adaptability and efficiency. We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning. Our experiments show that MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks.",https://www.semanticscholar.org/paper/078ee395f77345105f9a8bae0bed962cbca871b1,10.48550/arXiv.2507.00606,Semantic Scholar (Snowball),0
Advanced Financial Reasoning at Scale: A Comprehensive Evaluation of Large Language Models on CFA Level III,"Pranam Shetty, Abhisek Upadhayaya, Parth Mitesh Shah, Srikanth Jagabathula, Shilpi Nayak, Anna Joo Fee",2025,"As financial institutions increasingly adopt Large Language Models (LLMs), rigorous domain-specific evaluation becomes critical for responsible deployment. This paper presents a comprehensive benchmark evaluating 23 state-of-the-art LLMs on the Chartered Financial Analyst (CFA) Level III exam - the gold standard for advanced financial reasoning. We assess both multiple-choice questions (MCQs) and essay-style responses using multiple prompting strategies including Chain-of-Thought and Self-Discover. Our evaluation reveals that leading models demonstrate strong capabilities, with composite scores such as 79.1% (o4-mini) and 77.3% (Gemini 2.5 Flash) on CFA Level III. These results, achieved under a revised, stricter essay grading methodology, indicate significant progress in LLM capabilities for high-stakes financial applications. Our findings provide crucial guidance for practitioners on model selection and highlight remaining challenges in cost-effective deployment and the need for nuanced interpretation of performance against professional benchmarks.",https://www.semanticscholar.org/paper/0ad864609ce79a0988148389bd05879e83b59502,10.48550/arXiv.2507.02954,Semantic Scholar (Snowball),0
LLMs for Customized Marketing Content Generation and Evaluation at Scale,"Haoran Liu, Amir Tahmasbi, Ehtesham Sam Haque, Purak Jain",2025,"Offsite marketing is essential in e-commerce, enabling businesses to reach customers through external platforms and drive traffic to retail websites. However, most current offsite marketing content is overly generic, template-based, and poorly aligned with landing pages, limiting its effectiveness. To address these limitations, we propose MarketingFM, a retrieval-augmented system that integrates multiple data sources to generate keyword-specific ad copy with minimal human intervention. We validate MarketingFM via offline human and automated evaluations and large-scale online A/B tests. In one experiment, keyword-focused ad copy outperformed templates, achieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC, demonstrating gains in ad ranking and cost efficiency. Despite these gains, human review of generated ads remains costly. To address this, we propose AutoEval-Main, an automated evaluation system that combines rule-based metrics with LLM-as-a-Judge techniques to ensure alignment with marketing principles. In experiments with large-scale human annotations, AutoEval-Main achieved 89.57% agreement with human reviewers. Building on this, we propose AutoEval-Update, a cost-efficient LLM-human collaborative framework to dynamically refine evaluation prompts and adapt to shifting criteria with minimal human input. By selectively sampling representative ads for human review and using a critic LLM to generate alignment reports, AutoEval-Update improves evaluation consistency while reducing manual effort. Experiments show the critic LLM suggests meaningful refinements, improving LLM-human agreement. Nonetheless, human oversight remains essential for setting thresholds and validating refinements before deployment.",https://www.semanticscholar.org/paper/db2b7b7229f08b456e3416b8f66ca088dfb2cc37,10.48550/arXiv.2506.17863,Semantic Scholar (Snowball),0
"General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting","Bernard Lange, Anil Yildiz, Mansur M. Arief, Shehryar Khattak, Mykel J. Kochenderfer, Georgios Georgakis",2025,"Developing general-purpose navigation policies for unknown environments remains a core challenge in robotics. Most existing systems rely on task-specific neural networks and fixed information flows, limiting their generalizability. Large Vision-Language Models (LVLMs) offer a promising alternative by embedding human-like knowledge for reasoning and planning, but prior LVLM-robot integrations have largely depended on pre-mapped spaces, hard-coded representations, and rigid control logic. We introduce the Agentic Robotic Navigation Architecture (ARNA), a general-purpose framework that equips an LVLM-based agent with a library of perception, reasoning, and navigation tools drawn from modern robotic stacks. At runtime, the agent autonomously defines and executes task-specific workflows that iteratively query modules, reason over multimodal inputs, and select navigation actions. This agentic formulation enables robust navigation and reasoning in previously unmapped environments, offering a new perspective on robotic stack design. Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA outperforms state-of-the-art EQA-specific approaches. Qualitative results on RxR and custom tasks further demonstrate its ability to generalize across a broad range of navigation challenges.",https://www.semanticscholar.org/paper/1a28214089b1d48416292dc07043f16634694b2a,10.48550/arXiv.2506.17462,Semantic Scholar (Snowball),0
SGEU: enhancing LLM reasoning via backward exemplar generation and verification,"Zhen Wang, Xi Zhou, Yating Yang, Bo Ma, Lei Wang, Rui Dong",2025,,https://www.semanticscholar.org/paper/a9d3f9e4759e5357035803a83f41a204c1ba2895,10.1007/s10489-025-06529-8,Semantic Scholar (Snowball),0
No Universal Prompt: Unifying Reasoning through Adaptive Prompting for Temporal Table Reasoning,"Kushagra Dixit, Abhishek Rajgaria, Harshavardhan Kalalbandi, Dan Roth, Vivek Gupta",2025,"Temporal Table Reasoning is a critical challenge for Large Language Models (LLMs), requiring effective reasoning to extract relevant insights. Despite existence of multiple prompting methods, their impact on table reasoning remains largely unexplored. Furthermore, model performance varies drastically across different table and context structures, making it difficult to determine an optimal approach. This work investigates multiple prompting technique on diverse table types to determine that performance depends on factors such as entity type, table structure, requirement of additional context and question complexity, with""NO""single method consistently outperforming others. To address this, we introduce SEAR, an adaptive prompting framework inspired by human reasoning that dynamically adjusts to context and integrates structured reasoning. Our results demonstrate that SEAR achieves superior performance across all table types compared to baseline prompting techniques. Additionally, we explore the impact of table structure refactoring, finding that a unified representation enhances model reasoning.",https://www.semanticscholar.org/paper/66f8700cc6de2f3accd5fcb4725da97024f3f27c,10.48550/arXiv.2506.11246,Semantic Scholar (Snowball),0
Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring,"Gusseppe Bravo Rocca, Peini Liu, Jordi Guitart, R. Carrillo-Larco, Ajay Dholakia, David Ellison",2025,"Monitoring Machine Learning (ML) models in production environments is crucial, yet traditional approaches often yield verbose, low-interpretability outputs that hinder effective decision-making. We propose a cognitive architecture for ML monitoring that applies feature engineering principles to agents based on Large Language Models (LLMs), significantly enhancing the interpretability of monitoring outputs. Central to our approach is a Decision Procedure module that simulates feature engineering through three key steps: Refactor, Break Down, and Compile. The Refactor step improves data representation to better capture feature semantics, allowing the LLM to focus on salient aspects of the monitoring data while reducing noise and irrelevant information. Break Down decomposes complex information for detailed analysis, and Compile integrates sub-insights into clear, interpretable outputs. This process leads to a more deterministic planning approach, reducing dependence on LLM-generated planning, which can sometimes be inconsistent and overly general. The combination of feature engineering-driven planning and selective LLM utilization results in a robust decision support system, capable of providing highly interpretable and actionable insights. Experiments using multiple LLMs demonstrate the efficacy of our approach, achieving significantly higher accuracy compared to various baselines across several domains.",https://www.semanticscholar.org/paper/5a8202481fce8a20563bbefe2c0c46c58bc48bfa,10.48550/arXiv.2506.09742,Semantic Scholar (Snowball),0
What Makes a Good Natural Language Prompt?,"Do Xuan Long, Duy Dinh, Ngoc-Hai Nguyen, Kenji Kawaguchi, Nancy F. Chen, Shafiq Joty, Min-Yen Kan",2025,"As large language models (LLMs) have progressed towards more human-like and human--AI communications have become prevalent, prompting has emerged as a decisive component. However, there is limited conceptual consensus on what exactly quantifies natural language prompts. We attempt to address this question by conducting a meta-analysis surveying more than 150 prompting-related papers from leading NLP and AI conferences from 2022 to 2025 and blogs. We propose a property- and human-centric framework for evaluating prompt quality, encompassing 21 properties categorized into six dimensions. We then examine how existing studies assess their impact on LLMs, revealing their imbalanced support across models and tasks, and substantial research gaps. Further, we analyze correlations among properties in high-quality natural language prompts, deriving prompting recommendations. We then empirically explore multi-property prompt enhancements in reasoning tasks, observing that single-property enhancements often have the greatest impact. Finally, we discover that instruction-tuning on property-enhanced prompts can result in better reasoning models. Our findings establish a foundation for property-centric prompt evaluation and optimization, bridging the gaps between human--AI communication and opening new prompting research directions.",https://www.semanticscholar.org/paper/e1e7b8eb65bf4eed85ce0fbf9949a84d432aadfa,10.48550/arXiv.2506.06950,Semantic Scholar (Snowball),0
Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs,"Hongming Yang, Shi Lin, Jun Shao, Changting Lin, Donghai Zhu, Meng Han, Qinglei Kong",2025,"Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized models designed to run efficiently on consumer-grade hardware, offering significant advantages in resource efficiency, cost-effectiveness, and data privacy. However, these models often struggle with limited inference and reasoning capabilities, which restrict their performance on complex tasks and limit their practical applicability. Moreover, existing prompt optimization methods typically rely on extensive manual effort or the meta-cognitive abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To address these challenges, we introduce DeBoP, a new Direct Behavior Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting technique. Unlike CoT Prompting, DeBoP is an automatic optimization method, which focuses on the optimization directly on the behavior of LwLLMs. In particular, DeBoP transforms the optimization of complex prompts into the optimization of discrete, quantifiable execution sequences using a gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging tasks where state-of-the-art LLMs excel but LwLLMs generally underperform. Experimental results demonstrate that DeBoP significantly outperforms recent prompt optimization methods on most tasks. In particular, DeBoP-optimized LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by approximately 60% compared to other automatic prompt optimization methods.",https://www.semanticscholar.org/paper/5b02a150c0c4443ee0c8c2f9f9bb5c97c707ddcd,10.48550/arXiv.2506.06401,Semantic Scholar (Snowball),0
Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences,"Hadi Hosseini, Samarth Khanna, Ronak Singh",2025,"The rise of Large Language Models (LLMs) has driven progress in reasoning tasks -- from program synthesis to scientific hypothesis generation -- yet their ability to handle ranked preferences and structured algorithms in combinatorial domains remains underexplored. We study matching markets, a core framework behind applications like resource allocation and ride-sharing, which require reconciling individual ranked preferences to ensure stable outcomes. We evaluate several state-of-the-art models on a hierarchy of preference-based reasoning tasks -- ranging from stable-matching generation to instability detection, instability resolution, and fine-grained preference queries -- to systematically expose their logical and algorithmic limitations in handling ranked inputs. Surprisingly, even top-performing models with advanced reasoning struggle to resolve instability in large markets, often failing to identify blocking pairs or execute algorithms iteratively. We further show that parameter-efficient fine-tuning (LoRA) significantly improves performance in small markets, but fails to bring about a similar improvement on large instances, suggesting the need for more sophisticated strategies to improve LLMs' reasoning with larger-context inputs.",https://www.semanticscholar.org/paper/902455678ebc1e8e8ab8dfbc486b41703b6022e0,10.48550/arXiv.2506.04478,Semantic Scholar (Snowball),0
Toward a Theory of Agents as Tool-Use Decision-Makers,"Hongru Wang, Cheng Qian, Manling Li, Jiahao Qiu, Boyang Xue, Mengdi Wang, Heng Ji, Kam-Fai Wong",2025,"As Large Language Models (LLMs) evolve into increasingly autonomous agents, fundamental questions about their epistemic foundations remain unresolved: What defines an agent? How should it make decisions? And what objectives should guide its behavior? In this position paper, we argue that true autonomy requires agents to be grounded in a coherent epistemic framework that governs what they know, what they need to know, and how to acquire that knowledge efficiently. We propose a unified theory that treats internal reasoning and external actions as equivalent epistemic tools, enabling agents to systematically coordinate introspection and interaction. Building on this framework, we advocate for aligning an agent's tool use decision-making boundary with its knowledge boundary, thereby minimizing unnecessary tool use and maximizing epistemic efficiency. This perspective shifts the design of agents from mere action executors to knowledge-driven intelligence systems, offering a principled path toward building foundation agents capable of adaptive, efficient, and goal-directed behavior.",https://www.semanticscholar.org/paper/c217deb63a3dbeff8cce1125f4428542f36ed3b7,10.48550/arXiv.2506.00886,Semantic Scholar (Snowball),0
Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven Collaboration,"Sibo Xiao, Zixin Lin, Wenyang Gao, Yue Zhang",2025,"Processing long contexts has become a critical capability for modern large language models (LLMs). Existing works leverage agent-based divide-and-conquer methods for processing long contexts. But these methods face crucial limitations, including prohibitive accumulated latency and amplified information loss from excessive agent invocations, and the disruption of inherent textual dependencies by immoderate partitioning. In this paper, we propose a novel multi-agent framework XpandA (Expand-Agent) coupled with question-driven workflow and dynamic partitioning for robust long-context processing. XpandA overcomes these limitations through: 1) dynamic partitioning of long texts, which adaptively modulates the filling rate of context windows for input sequences of vastly varying lengths; 2) question-guided protocol to update flat information ensembles within centralized shared memory, constructing consistent inter-agent knowledge across partitions; and 3) selectively replaying specific partitions based on the state-tracking of question-information couples to promote the resolution of inverted-order structures across partitions (e.g., flashbacks). We perform a comprehensive evaluation of XpandA on multiple long-context benchmarks with length varying from 1k to 1M, demonstrating XpandA's feasibility for processing ultra-long sequences and its significant effectiveness in enhancing the long-context capabilities of various LLMs by achieving 20\% improvements and 1.5x inference speedup over baselines of full-context, RAG and previous agent-based methods.",https://www.semanticscholar.org/paper/7d632fb8862cc1fb9f1fefb6324f88c2c594c667,10.48550/arXiv.2505.20625,Semantic Scholar (Snowball),0
MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision,"Zixuan Ke, Austin Xu, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty",2025,"Multi-agent systems (MAS) leveraging the impressive capabilities of Large Language Models (LLMs) hold significant potential for tackling complex tasks. However, most current MAS depend on manually designed agent roles and communication protocols. These manual designs often fail to align with the underlying LLMs'strengths and struggle to adapt to novel tasks. Recent automatic MAS approaches attempt to mitigate these limitations but typically necessitate a validation set for tuning and yield static MAS designs lacking adaptability during inference, while also removing the flexibility to reduce to simpler systems. We introduce MAS-ZERO, the first self-evolved, inference-time framework for automatic MAS design. MAS-ZERO employs meta-level design to iteratively design, critique, and refine MAS configurations tailored to each problem instance, without requiring a validation set. Critically, it enables dynamic problem decomposition and agent composition through meta-feedback on solvability and completeness, and reduction to simpler systems when appropriate. Experiments across reasoning (math and graduate-level QA), coding, and agentic (search-based) benchmarks, using both closed-source and open-source LLM backbones of varying sizes, demonstrate that MAS-ZERO outperforms strong manual and automatic MAS baselines. It achieves substantial average accuracy improvements of up to 16.69% on reasoning, 16.66% on coding, and 5.45% on agentic tasks, while maintaining cost efficiency.",https://www.semanticscholar.org/paper/097e2138daa8d6af8ef1af85ac9cc6e031540aa1,10.48550/arXiv.2505.14996,Semantic Scholar (Snowball),0
Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst,"Hongru Wang, Deng Cai, Wanjun Zhong, Shijue Huang, Jeff Z. Pan, Zeming Liu, Kam-Fai Wong",2025,"Inference-time scaling has attracted much attention which significantly enhance the performance of Large Language Models (LLMs) in complex reasoning tasks by increasing the length of Chain-of-Thought. These longer intermediate reasoning rationales embody various meta-reasoning skills in human cognition, such as reflection and decomposition, being difficult to create and acquire. In this work, we introduce \textit{Self-Reasoning Language Model} (SRLM), where the model itself can synthesize longer CoT data and iteratively improve performance through self-training. By incorporating a few demonstration examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from existing responses, which act as a reasoning catalyst, we demonstrate that SRLM not only enhances the model's initial performance but also ensures more stable and consistent improvements in subsequent iterations. Our proposed SRLM achieves an average absolute improvement of more than $+2.5$ points across five reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models. Moreover, it brings more improvements with more times of sampling during inference, such as absolute $+7.89$ average improvement with $64$ sampling times, revealing the in-depth, diverse and creative reasoning paths in SRLM against the strong baseline.",https://www.semanticscholar.org/paper/33ce5e48a857d0fed5167e459d9e6be5a7763b82,10.48550/arXiv.2505.14116,Semantic Scholar (Snowball),0
FlowReasoner: Reinforcing Query-Level Meta-Agents,"Hongcheng Gao, Yue Liu, Yufei He, Longxu Dou, Chao Du, Zhijie Deng, Bryan Hooi, Min Lin, Tianyu Pang",2025,"This paper proposes a query-level meta-agent named FlowReasoner to automate the design of query-level multi-agent systems, i.e., one system per user query. Our core idea is to incentivize a reasoning-based meta-agent via external execution feedback. Concretely, by distilling DeepSeek R1, we first endow the basic reasoning ability regarding the generation of multi-agent systems to FlowReasoner. Then, we further enhance it via reinforcement learning (RL) with external execution feedback. A multi-purpose reward is designed to guide the RL training from aspects of performance, complexity, and efficiency. In this manner, FlowReasoner is enabled to generate a personalized multi-agent system for each user query via deliberative reasoning. Experiments on both engineering and competition code benchmarks demonstrate the superiority of FlowReasoner. Remarkably, it surpasses o1-mini by 10.52% accuracy across three benchmarks. The code is available at https://github.com/sail-sg/FlowReasoner.",https://www.semanticscholar.org/paper/0bcc3208ee89b533fbe7e3bbed00b6ac2ef68b09,10.48550/arXiv.2504.15257,Semantic Scholar (Snowball),0
Hogwild! Inference: Parallel LLM Generation via Concurrent Attention,"Gleb Rodionov, Roman Garipov, Alina Shutova, George Yakushev, Vage Egiazarian, Anton Sinitsin, Denis Kuznedelev, Dan Alistarh",2025,"Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM""workers""in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the LLM instances to come up with their own collaboration strategy for the problem at hand, all the while""seeing""each other's memory in the concurrent KV cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with""instant""access to each other's memory. Hogwild! Inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.",https://www.semanticscholar.org/paper/72511c0da0d01fd82d204a66d040eac32f2faa1b,10.48550/arXiv.2504.06261,Semantic Scholar (Snowball),0
A Survey of Large Language Model Agents for Question Answering,Murong Yue,2025,"This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine. These agents achieve superior QA results compared to traditional QA pipelines and naive LLM QA systems by enabling interaction with external environments. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation. Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems.",https://www.semanticscholar.org/paper/79f2dcea1f249c62230f5483d0b08ebb775d6ca5,10.48550/arXiv.2503.19213,Semantic Scholar (Snowball),0
EditLord: Learning Code Transformation Rules for Code Editing,"Weichen Li, Albert Jan, Baishakhi Ray, Chengzhi Mao, Junfeng Yang, Kexin Pei",2025,"Code editing is a foundational task in software development, where its effectiveness depends on whether it introduces desired code property changes without changing the original code's intended functionality. Existing approaches often formulate code editing as an implicit end-to-end task, omitting the fact that code-editing procedures inherently consist of discrete and explicit steps. Thus, they suffer from suboptimal performance and lack of robustness and generalization. We introduce EditLord, a code editing framework that makes the code transformation steps explicit. Our key insight is to employ a language model (LM) as an inductive learner to extract code editing rules from the training code pairs as concise meta-rule sets. Such rule sets will be manifested for each training sample to augment them for finetuning or assist in prompting- and iterative-based code editing. EditLord outperforms the state-of-the-art by an average of 22.7% in editing performance and 58.1% in robustness while achieving 20.2% higher functional correctness across critical software engineering and security applications, LM models, and editing modes.",https://www.semanticscholar.org/paper/bf7f815cad7306e54c54e7ed0b05a7fb1293e38f,10.48550/arXiv.2504.15284,Semantic Scholar (Snowball),0
HiBench: Benchmarking LLMs Capability on Hierarchical Structure Reasoning,"Zhuohang Jiang, Pangjing Wu, Ziran Liang, Peter Q. Chen, Xu Yuan, Ye Jia, Jiancheng Tu, Chen Li, P. H. Ng, Qing Li",2025,"Structure reasoning is a fundamental capability of large language models (LLMs), enabling them to reason about structured commonsense and answer multi-hop questions. However, existing benchmarks for structure reasoning mainly focus on horizontal and coordinate structures (e.g. graphs), overlooking the hierarchical relationships within them. Hierarchical structure reasoning is crucial for human cognition, particularly in memory organization and problem-solving. It also plays a key role in various real-world tasks, such as information extraction and decision-making. To address this gap, we propose HiBench, the first framework designed to systematically benchmark the hierarchical reasoning capabilities of LLMs from initial structure generation to final proficiency assessment. It encompasses six representative scenarios, covering both fundamental and practical aspects, and consists of 30 tasks with varying hierarchical complexity, totaling 39,519 queries. To evaluate LLMs comprehensively, we develop five capability dimensions that depict different facets of hierarchical structure understanding. Through extensive evaluation of 20 LLMs from 10 model families, we reveal key insights into their capabilities and limitations: 1) existing LLMs show proficiency in basic hierarchical reasoning tasks; 2) they still struggle with more complex structures and implicit hierarchical representations, especially in structural modification and textual reasoning. Based on these findings, we create a small yet well-designed instruction dataset, which enhances LLMs' performance on HiBench by an average of 88.84% (Llama-3.1-8B) and 31.38% (Qwen2.5-7B) across all tasks. The HiBench dataset and toolkit are available at https://github.com/jzzzzh/HiBench to encourage evaluation.",https://www.semanticscholar.org/paper/3fbac6df7bbf468c75dff3e7bffc087ae071adcb,10.1145/3711896.3737378,Semantic Scholar (Snowball),0
Conversational Planning for Personal Plans,"Konstantina Christakopoulou, Iris Qu, John Canny, Andrew Goodridge, Cj Adams, Minmin Chen, Maja Matari'c",2025,"The language generation and reasoning capabilities of large language models (LLMs) have enabled conversational systems with impressive performance in a variety of tasks, from code generation, to composing essays, to passing STEM and legal exams, to a new paradigm for knowledge search. Besides those short-term use applications, LLMs are increasingly used to help with real-life goals or tasks that take a long time to complete, involving multiple sessions across days, weeks, months, or even years. Thus to enable conversational systems for long term interactions and tasks, we need language-based agents that can plan for long horizons. Traditionally, such capabilities were addressed by reinforcement learning agents with hierarchical planning capabilities. In this work, we explore a novel architecture where the LLM acts as the meta-controller deciding the agent's next macro-action, and tool use augmented LLM-based option policies execute the selected macro-action. We instantiate this framework for a specific set of macro-actions enabling adaptive planning for users' personal plans through conversation and follow-up questions collecting user feedback. We show how this paradigm can be applicable in scenarios ranging from tutoring for academic and non-academic tasks to conversational coaching for personal health plans.",https://www.semanticscholar.org/paper/ff23001689d08812a25ce03844f476e31ed15d75,10.48550/arXiv.2502.19500,Semantic Scholar (Snowball),0
Which Questions Improve Learning the Most? Utility Estimation of Questions with LM-based Simulations,"Dong-Ho Lee, Hyundong Justin Cho, Jonathan May, J. Pujara",2025,"Asking good questions is critical for comprehension and learning, yet evaluating and generating such questions remains a challenging problem. Prior work on inquisitive questions focuses on learner-generated, curiosity-driven queries and evaluates them using indirect metrics, such as salience or information gain, that do not directly capture a question's impact on actual learning outcomes. We introduce QUEST (Question Utility Estimation with Simulated Tests), a framework that uses language models to simulate learners and directly quantify the utility of a question - its contribution to exam performance. QUEST simulates a learner who asks questions and receives answers while studying a textbook chapter, then uses them to take an end-of-chapter exam. Through this simulation, the utility of each question is estimated by its direct effect on exam performance, rather than inferred indirectly based on the underlying content. To support this evaluation, we curate TEXTBOOK-EXAM, a benchmark that aligns textbook sections with end-of-section exam questions across five academic disciplines. Using QUEST, we filter for high-utility questions and fine-tune question generators via rejection sampling. Experiments show that questions generated by QUEST-trained models improve simulated test scores by over 20% compared to strong baselines that are fine-tuned using indirect metrics or leverage prompting methods. Furthermore, utility is only weakly correlated with salience and similarity to exam questions, suggesting that it captures unique signal that benefits downstream performance. QUEST offers a new outcome-driven paradigm for question evaluation and generation - one that moves beyond question-answer content toward measurable improvements in learning outcomes.",https://www.semanticscholar.org/paper/ce0720a7165deaf9cfc020df27fe3f1db22463a7,,Semantic Scholar (Snowball),0
Atom of Thoughts for Markov LLM Test-Time Scaling,"Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo, Zhijiang Guo",2025,"Large Language Models (LLMs) achieve superior performance through training-time scaling, and test-time scaling further enhances their capabilities by conducting effective reasoning during inference. However, as the scale of reasoning increases, existing test-time scaling methods suffer from accumulated historical information, which not only wastes computational resources but also interferes with effective reasoning. To address this issue, we observe that complex reasoning can be achieved by solving a series of independent and self-contained subquestions. These subquestions are essentially \textit{atomic questions}, exhibiting the memoryless property similar to Markov processes. Based on this observation, we propose Atom of Thoughts (\our), where each state transition consists of decomposing the current question into a dependency-based directed acyclic graph and contracting its subquestions, forming a simplified question that maintains answer equivalence with the original problem. This answer preservation enables the iterative \textit{decomposition-contraction} process to naturally form a meaningful Markov reasoning process. Furthermore, these atomic states can be seamlessly integrated into existing test-time scaling methods, enabling \our to serve as a plug-in enhancement for improving reasoning capabilities. Experiments across six benchmarks demonstrate the effectiveness of \our both as a standalone framework and a plug-in enhancement. Notably, on HotpotQA, when applied to gpt-4o-mini, \our achieves an \textbf{80.6\%} F1 score, surpassing o3-mini by \textbf{3.4\%} and DeepSeek-R1 by \textbf{10.6\%}. The code is available at \href{https://github.com/qixucen/atom}{https://github.com/qixucen/atom}.",https://www.semanticscholar.org/paper/2a8b3b839f1c22270b0deab9fc75af6ac62ffb1a,10.48550/arXiv.2502.12018,Semantic Scholar (Snowball),0
Mitigating Hallucinations in Multimodal Spatial Relations through Constraint-Aware Prompting,"Jiarui Wu, Zhuo Liu, Hangfeng He",2025,"Spatial relation hallucinations pose a persistent challenge in large vision-language models (LVLMs), leading to generate incorrect predictions about object positions and spatial configurations within an image. To address this issue, we propose a constraint-aware prompting framework designed to reduce spatial relation hallucinations. Specifically, we introduce two types of constraints: (1) bidirectional constraint, which ensures consistency in pairwise object relations, and (2) transitivity constraint, which enforces relational dependence across multiple objects. By incorporating these constraints, LVLMs can produce more spatially coherent and consistent outputs. We evaluate our method on three widely-used spatial relation datasets, demonstrating performance improvements over existing approaches. Additionally, a systematic analysis of various bidirectional relation analysis choices and transitivity reference selections highlights greater possibilities of our methods in incorporating constraints to mitigate spatial relation hallucinations.",https://www.semanticscholar.org/paper/2ec50ad10c00eb71ad7002d97b63f5b970ef989a,10.48550/arXiv.2502.08317,Semantic Scholar (Snowball),0
Toward a Human-AI Task Tensor: A Taxonomy for Organizing Work in the Age of Generative AI,"Anil R. Doshi, Alastair Moore",2025,"We introduce a framework for understanding the impact of generative AI on human work, which we call the human-AI task tensor. A tensor is a structured framework that organizes tasks along multiple interdependent dimensions. Our human-AI task tensor introduces a systematic approach to studying how humans and AI interact to perform tasks, and has eight dimensions: task definition, AI contribution, interaction modality, audit requirement, output definition, decision-making authority, AI structure, and human persona. After describing the eight dimensions of the tensor, we provide illustrative frameworks (derived from projections of the tensor) and a human-AI task canvas that provide analytical tractability and practical insight for organizational decision-making. We demonstrate how the human-AI task tensor can be used to organize emerging and future research on generative AI. We propose that the human-AI task tensor offers a starting point for understanding how work will be performed with the emergence of generative AI.",https://www.semanticscholar.org/paper/b1eca7126f1bbefa0e29a495b7c03be20a7c12d8,,Semantic Scholar (Snowball),0
Evaluating LLM Reasoning in the Operations Research Domain with ORQA,"Mahdi Mostajabdaveh, Timothy T. L. Yu, Samarendra Chandan Bindu Dash, Rindranirina Ramamonjison, Jabo Serge Byusa, Giuseppe Carenini, Zirui Zhou, Yong Zhang",2024,"In this paper, we introduce and apply Operations Research Question Answering (ORQA), a new benchmark designed to assess the generalization capabilities of Large Language Models (LLMs) in the specialized technical domain of Operations Research (OR). This benchmark evaluates whether LLMs can emulate the knowledge and reasoning skills of OR experts when confronted with diverse and complex optimization problems. The dataset, developed by OR experts, features real-world optimization problems that demand multistep reasoning to construct their mathematical models. Our evaluations of various open source LLMs, such as LLaMA 3.1, DeepSeek, and Mixtral, reveal their modest performance, highlighting a gap in their ability to generalize to specialized technical domains. This work contributes to the ongoing discourse on LLMs generalization capabilities, offering valuable insights for future research in this area. The dataset and evaluation code are publicly available.",https://www.semanticscholar.org/paper/b0729a4027f1ba1aef0b6dc92334fd19f686a3d0,10.48550/arXiv.2412.17874,Semantic Scholar (Snowball),0
Assessing ChatGPT’s Code Generation Capabilities with Short vs Long Context Programming Problems,"Uddip Acharjee Shuvo, Sajib Acharjee Dip, Nirvar Roy Vaskar, A. A. Al Islam",2024,"This study assesses the code generation capabilities of ChatGPT using competitive programming problems from platforms such as LeetCode, HackerRank, and UVa Online Judge. In a novel approach, we contrast ChatGPT’s performance on concise problems from LeetCode against more complex, narrative-driven problems from Codeforces. Our results reveal significant challenges in addressing the intricate narrative structures of Codeforces, with difficulties in problem recognition and strategic planning in extended contexts. While initial code accuracy for LeetCode problems stands at 72%, it drops to 31% for complex Codeforces problems using Python. Additionally, we explore the impact of targeted instructions aimed at enhancing performance, which increased LeetCode accuracy to 73.53% but saw a decrease in Codeforces performance to 29%. Our analysis further extends across multiple programming languages, examining if iterative prompting and specific feedback can enhance code precision and efficiency. We also delve into ChatGPT’s performance on challenging problems and those released post its training period. This research provides insights into the strengths and weaknesses of AI in code generation and lays groundwork for future developments in AI-driven coding tools.",https://www.semanticscholar.org/paper/74a807cd17c3c82fa086ca1e2b7e4878eec3125e,10.1145/3704522.3704535,Semantic Scholar (Snowball),0
Towards Adaptive Mechanism Activation in Language Agent,"Ziyang Huang, Jun Zhao, Kang Liu",2024,"Language Agent could be endowed with different mechanisms for autonomous task accomplishment. Current agents typically rely on fixed mechanisms or a set of mechanisms activated in a predefined order, limiting their adaptation to varied potential task solution structures. To this end, this paper proposes \textbf{A}daptive \textbf{L}anguage \textbf{A}gent \textbf{M}echanism \textbf{A}ctivation Learning with Self-Exploration (\textbf{ALAMA}), which focuses on optimizing mechanism activation adaptability without reliance on expert models. Initially, it builds a harmonized agent framework (\textbf{UniAct}) to \textbf{Uni}fy different mechanisms via \textbf{Act}ions. Then it leverages a training-efficient optimization method based on self-exploration to enable the UniAct to adaptively activate the appropriate mechanisms according to the potential characteristics of the task. Experimental results demonstrate significant improvements in downstream agent tasks, affirming the effectiveness of our approach in facilitating more dynamic and context-sensitive mechanism activation.",https://www.semanticscholar.org/paper/15894307dd6ca9a436288e9a954ec1fe793049d9,10.48550/arXiv.2412.00722,Semantic Scholar (Snowball),0
From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge,"Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, Huan Liu",2024,"Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). Traditional methods, usually matching-based or small model-based, often fall short in open-ended and dynamic scenarios. Recent advancements in Large Language Models (LLMs) inspire the""LLM-as-a-judge""paradigm, where LLMs are leveraged to perform scoring, ranking, or selection for various machine learning evaluation scenarios. This paper presents a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to review this evolving field. We first provide the definition from both input and output perspectives. Then we introduce a systematic taxonomy to explore LLM-as-a-judge along three dimensions: what to judge, how to judge, and how to benchmark. Finally, we also highlight key challenges and promising future directions for this emerging area. More resources on LLM-as-a-judge are on the website: https://llm-as-a-judge.github.io and https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge.",https://www.semanticscholar.org/paper/92056d644aed7caa6c5367fe77774883246af793,10.48550/arXiv.2411.16594,Semantic Scholar (Snowball),0
Beyond Ontology in Dialogue State Tracking for Goal-Oriented Chatbot,"Sejin Lee, Dongha Kim, Min Song",2024,"Goal-oriented chatbots are essential for automating user tasks, such as booking flights or making restaurant reservations. A key component of these systems is Dialogue State Tracking (DST), which interprets user intent and maintains the dialogue state. However, existing DST methods often rely on fixed ontologies and manually compiled slot values, limiting their adaptability to open-domain dialogues. We propose a novel approach that leverages instruction tuning and advanced prompt strategies to enhance DST performance, without relying on any predefined ontologies. Our method enables Large Language Model (LLM) to infer dialogue states through carefully designed prompts and includes an anti-hallucination mechanism to ensure accurate tracking in diverse conversation contexts. Additionally, we employ a Variational Graph Auto-Encoder (VGAE) to model and predict subsequent user intent. Our approach achieved state-of-the-art with a JGA of 42.57% outperforming existing ontologyless DST models, and performed well in open-domain real-world conversations. This work presents a significant advancement in creating more adaptive and accurate goal-oriented chatbots. 1",https://www.semanticscholar.org/paper/b5e6884c1ad9a378d6f3ec39834d1964130377d1,10.1109/ICKG63256.2024.00030,Semantic Scholar (Snowball),0
Agents Thinking Fast and Slow: A Talker-Reasoner Architecture,"Konstantina Christakopoulou, Shibl Mourad, Maja Matari'c",2024,"Large language models have enabled agents of all kinds to interact with users through natural conversation. Consequently, agents now have two jobs: conversing and planning/reasoning. Their conversational responses must be informed by all available information, and their actions must help to achieve goals. This dichotomy between conversing with the user and doing multi-step reasoning and planning can be seen as analogous to the human systems of""thinking fast and slow""as introduced by Kahneman. Our approach is comprised of a""Talker""agent (System 1) that is fast and intuitive, and tasked with synthesizing the conversational response; and a""Reasoner""agent (System 2) that is slower, more deliberative, and more logical, and is tasked with multi-step reasoning and planning, calling tools, performing actions in the world, and thereby producing the new agent state. We describe the new Talker-Reasoner architecture and discuss its advantages, including modularity and decreased latency. We ground the discussion in the context of a sleep coaching agent, in order to demonstrate real-world relevance.",https://www.semanticscholar.org/paper/04d9b838f79adc720ff568236c2a62fef34d8467,10.48550/arXiv.2410.08328,Semantic Scholar (Snowball),0
